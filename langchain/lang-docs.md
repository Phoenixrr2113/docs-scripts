

Page URL: https://js.langchain.com/docs/get_started

# Get started
Get started with LangChain
## 📄️ Introduction
## 📄️ Installation
## 📄️ Quickstart
Installation



Page URL: https://js.langchain.com/docs/get_started/introduction

# Introduction
LangChain is a framework for developing applications powered by language models. It enables applications that are:
The main value props of LangChain are:
Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.
## Get started​
Here’s how to install LangChain, set up your environment, and start building.
We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.
Note: These docs are for the LangChain JS/TS package. For documentation on the Python version, head here.
## Modules​
LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:
#### Model I/O​
Interface with language models
#### Data connection​
Interface with application-specific data
#### Chains​
Construct sequences of calls
#### Agents​
Let chains choose which tools to use given high-level directives
#### Memory​
Persist application state between runs of a chain
#### Callbacks​
Log and stream intermediate steps of any chain
## Examples, ecosystem, and resources​
### Use cases​
Walkthroughs and best-practices for common end-to-end use cases, like:
### Additional resources​
Our community is full of prolific developers, creative builders, and fantastic teachers. Check out the Gallery for a list of awesome LangChain projects, compiled by the folks at KyroLabs.
###  Support 
Join us on GitHub or Discord to ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLM’s.



Page URL: https://js.langchain.com/docs/get_started/installation

# Installation
Updating from <0.0.52? See this section for instructions.
## Supported Environments​
LangChain is written in TypeScript and can be used in:
## Installation​
To get started, install LangChain with the following command:
```typescript
npm install -S langchain
```
```typescript
yarn add langchain
```
```typescript
pnpm add langchain
```
### TypeScript​
LangChain is written in TypeScript and provides type definitions for all of its public APIs.
## Loading the library​
### ESM​
LangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:
```typescript
import { OpenAI } from "langchain/llms/openai";
```
If you are using TypeScript in an ESM project we suggest updating your tsconfig.json to include the following:
```typescript
{
  "compilerOptions": {
    ...
    "target": "ES2020", // or higher
    "module": "nodenext",
  }
}
```
### CommonJS​
LangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:
```typescript
const { OpenAI } = require("langchain/llms/openai");
```
### Cloudflare Workers​
LangChain can be used in Cloudflare Workers. You can import it using the following syntax:
```typescript
import { OpenAI } from "langchain/llms/openai";
```
### Vercel / Next.js​
LangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:
```typescript
import { OpenAI } from "langchain/llms/openai";
```
### Deno / Supabase Edge Functions​
LangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:
```typescript
import { OpenAI } from "https://esm.sh/langchain/llms/openai";
```
We recommend looking at our Supabase Template for an example of how to use LangChain in Supabase Edge Functions.
### Browser​
LangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:
```typescript
import { OpenAI } from "langchain/llms/openai";
```
## Updating from <0.0.52​
If you are updating from a version of LangChain prior to 0.0.52, you will need to update your imports to use the new path structure.
For example, if you were previously doing
```typescript
import { OpenAI } from "langchain/llms";
```
you will now need to do
```typescript
import { OpenAI } from "langchain/llms/openai";
```
This applies to all imports from the following 6 modules, which have been split into submodules for each integration. The combined modules are deprecated, do not work outside of Node.js, and will be removed in a future version.
Other modules are not affected by this change, and you can continue to import them from the same path.
Additionally, there are some breaking changes that were needed to support new environments:
## Unsupported: Node.js 16​
We do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.
You will have to make fetch available globally, either:
Additionally you'll have to polyfill structuredClone, eg. by installing core-js and following the instructions here.
If you are running this on Node.js 18+, you do not need to do anything.



Page URL: https://js.langchain.com/docs/get_started/quickstart

# Quickstart
## Installation​
To install LangChain run:
```typescript
npm install -S langchain
```
```typescript
yarn add langchain
```
```typescript
pnpm add langchain
```
For more details, see our Installation guide.
## Environment setup​
Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.
Accessing their API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:
```typescript
export OPENAI_API_KEY="..."
```
If you'd prefer not to set an environment variable you can pass the key in directly via the openAIApiKey parameter when initializing the OpenAI LLM class:
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  openAIApiKey: "YOUR_KEY_HERE",
});
```
## Building an application​
Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.
## LLMs​
#### Get predictions from a language model​
The basic building block of LangChain is the LLM, which takes in text and generates more text.
As an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature.
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  temperature: 0.9,
});
```
And now we can pass in text and get predictions!
```typescript
const result = await llm.predict("What would be a good company name for a company that makes colorful socks?");
// "Feetful of Fun"
```
## Chat models​
Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.
You can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, FunctionMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage, ChatMessage, SystemMessage } from "langchain/schema";

const chat = new ChatOpenAI({
  temperature: 0
});

const result = await chat.predictMessages([
  new HumanMessage("Translate this sentence from English to French. I love programming.")
]);

/*
  AIMessage {
    content: "J'adore la programmation."
  }
*/
```
It is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same.
LangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM.
You can access this through the predict interface.
```typescript
const result = await chat.predict("Translate this sentence from English to French. I love programming.")
// "J'adore la programmation."
```
## Prompt templates​
Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.
In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.
With PromptTemplates this is easy! In this case our template would be very simple:
```typescript
import { PromptTemplate } from "langchain/prompts";

const prompt = PromptTemplate.fromTemplate("What is a good name for a company that makes {product}?");

const formattedPrompt = await prompt.format({
  product: "colorful socks"
});
```
```typescript
"What is a good name for a company that makes colorful socks?"
```
Similar to LLMs, you can make use of templating by using a MessagePromptTemplate. You can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_messages method to generate the formatted messages.
Because this is generating a list of messages, it is slightly more complex than the normal prompt template which is generating only a string. Please see the detailed guides on prompts to understand more options available to you here.
```typescript
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate
} from "langchain/prompts";

const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate);

const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt]);

const formattedPrompt = await chatPrompt.formatMessages({
  input_language: "English",
  output_language: "French",
  text: "I love programming."
});
```
```typescript
/*
  [
    SystemMessage {
      content: 'You are a helpful assistant that translates English to French.'
    },
    HumanMessage {
      content: 'I love programming.'
    }
  ]
*/
```
## Chains​
Now that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.
The simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.
Using this we can replace
```typescript
const result = await llm.predict("What would be a good company name for a company that makes colorful socks?");
```
with
```typescript
import { OpenAI } from "langchain/llms/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

const llm = new OpenAI({});
const prompt = PromptTemplate.fromTemplate("What is a good name for a company that makes {product}?");

const chain = new LLMChain({
  llm,
  prompt
});

// Run is a convenience method for chains with prompts that require one input and one output.
const result = await chain.run("colorful socks");
```
```typescript
"Feetful of Fun"
```
There we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.
The LLMChain can be used with chat models as well:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate
} from "langchain/prompts";

const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate);

const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt]);

const chat = new ChatOpenAI({
  temperature: 0,
});

const chain = new LLMChain({
  llm: chat,
  prompt: chatPrompt,
});

const result = await chain.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming",
});
```
```typescript
// { text: "J'adore programmer" }
```
## Agents​
Our first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.
Agents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.
To load an agent, you need to choose a(n):
For this example, we'll be using SerpAPI to query a search engine.
You'll need to set the SERPAPI_API_KEY environment variable.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const input = "What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?";

const result = await executor.call({
  input,
});
```
```typescript
> Entering new AgentExecutor chain...

Thought: I need to find the temperature first, then use the calculator to raise it to the .023 power.
Action: Search
Action Input: "High temperature in SF yesterday"
Observation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 °F (at 1:56 pm) Minimum temperature yesterday: 49 °F (at 1:56 am) Average temperature ...

Thought: I now have the temperature, so I can use the calculator to raise it to the .023 power.
Action: Calculator
Action Input: 57^.023
Observation: Answer: 1.0974509573251117

Thought: I now know the final answer
Final Answer: 1.0974509573251117.

> Finished chain.
```
```typescript
// { output: "1.0974509573251117" }
```
Agents can also be used with chat models. There are a few varieties, but if using OpenAI and a functions-capable model, you can use openai-functions as the agent type.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const executor = await initializeAgentExecutorWithOptions(
  [new Calculator(), new SerpAPI()],
  new ChatOpenAI({ modelName: "gpt-4-0613", temperature: 0 }),
  {
    agentType: "openai-functions",
    verbose: true,
  }
);

const result = await executor.run("What is the temperature in New York?");
```
```typescript
/*
  {
    "output": "The current temperature in New York is 89°F, but it feels like 92°F. Please be cautious as the heat can lead to dehydration or heat stroke."
  }
*/
```
## Memory​
The chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.
The Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.
There are a number of built-in memory systems. The simplest of these is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
const chain = new ConversationChain({
  llm: model,
  memory,
  verbose: true,
});
const res1 = await chain.call({ input: "Hi! I'm Jim." });
```
here's what's going on under the hood
```typescript
> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:

> Finished chain.

>> 'Hello! How are you today?'
```
Now if we run the chain again
```typescript
const res2 = await chain.call({ input: "What's my name?" });
```
we'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input
```typescript
> Entering new chain...
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: Hi there!
AI:  Hello! How are you today?
Human: I'm doing well! Just having a conversation with an AI.
AI:

> Finished chain.

>> "Your name is Jim."
```
You can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object.
```typescript
import { ConversationChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
  MessagesPlaceholder,
} from "langchain/prompts";
import { BufferMemory } from "langchain/memory";

const chat = new ChatOpenAI({ temperature: 0 });

const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
  ),
  new MessagesPlaceholder("history"),
  HumanMessagePromptTemplate.fromTemplate("{input}"),
]);

// Return the current conversation directly as messages and insert them into the MessagesPlaceholder in the above prompt.
const memory = new BufferMemory({
  returnMessages: true,
  memoryKey: "history"
});

const chain = new ConversationChain({
  memory,
  prompt: chatPrompt,
  llm: chat,
  verbose: true,
});

const res = await chain.call({
  input: "My name is Jim.",
});
```
```typescript
Hello Jim! It's nice to meet you. How can I assist you today?
```
```typescript
const res2 = await chain.call({
  input: "What is my name?",
});
```
```typescript
Your name is Jim. You mentioned it at the beginning of our conversation. Is there anything specific you would like to know or discuss, Jim?
```



Page URL: https://js.langchain.com/docs/modules/

# Modules
LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:
#### Model I/O​
Interface with language models
#### Data connection​
Interface with application-specific data
#### Chains​
Construct sequences of calls
#### Agents​
Let chains choose which tools to use given high-level directives
#### Memory​
Persist application state between runs of a chain
#### Callbacks​
Log and stream intermediate steps of any chain



Page URL: https://js.langchain.com/docs/modules/model_io/

# Model I/O
The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.




Page URL: https://js.langchain.com/docs/modules/model_io/prompts/

# Prompts
The new way of programming models is through prompts.
A prompt refers to the input to the model.
This input is often constructed from multiple components.
LangChain provides several classes and functions to make constructing and working with prompts easy.



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_templates/

# Prompt templates
Language models take text as input - that text is commonly referred to as a prompt.
Typically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input.
LangChain provides several classes and functions to make constructing and working with prompts easy.
## What is a prompt template?​
A prompt template refers to a reproducible way to generate a prompt. It contains a text string ("the template"), that can take in a set of parameters from the end user and generates a prompt.
A prompt template can contain:
Here's a simple example:
```typescript
import { PromptTemplate } from "langchain/prompts";

const prompt = PromptTemplate.fromTemplate(`You are a naming consultant for new companies.
What is a good name for a company that makes {product}?`
);

const formattedPrompt = await prompt.format({
  product: "colorful socks",
});

/*
  You are a naming consultant for new companies.
  What is a good name for a company that makes colorful socks?
*/
```
## Create a prompt template​
You can create simple hardcoded prompts using the PromptTemplate class. Prompt templates can take any number of input variables, and can be formatted to generate a prompt.
```typescript
import { PromptTemplate } from "langchain/prompts";

// An example prompt with no input variables
const noInputPrompt = new PromptTemplate({
  inputVariables: [],
  template: "Tell me a joke.",
});
const formattedNoInputPrompt = await noInputPrompt.format();

console.log(formattedNoInputPrompt);
// "Tell me a joke."

// An example prompt with one input variable
const oneInputPrompt = new PromptTemplate({
  inputVariables: ["adjective"],
  template: "Tell me a {adjective} joke."
})
const formattedOneInputPrompt = await oneInputPrompt.format({
  adjective: "funny",
});

console.log(formattedOneInputPrompt);
// "Tell me a funny joke."

// An example prompt with multiple input variables
const multipleInputPrompt = new PromptTemplate({
  inputVariables: ["adjective", "content"],
  template: "Tell me a {adjective} joke about {content}.",
});
const formattedMultipleInputPrompt = await multipleInputPrompt.format({
  adjective: "funny",
  content: "chickens",
});

console.log(formattedMultipleInputPrompt);
// "Tell me a funny joke about chickens."
```
If you do not wish to specify inputVariables manually, you can also create a PromptTemplate using the fromTemplate class method. LangChain will automatically infer the inputVariables based on the template passed.
```typescript
import { PromptTemplate } from "langchain/prompts";

const template = "Tell me a {adjective} joke about {content}.";

const promptTemplate = PromptTemplate.fromTemplate(template);
console.log(promptTemplate.inputVariables);
// ['adjective', 'content']
const formattedPromptTemplate = await promptTemplate.format({
  adjective: "funny",
  content: "chickens",
});
console.log(formattedPromptTemplate);
// "Tell me a funny joke about chickens."
```
You can create custom prompt templates that format the prompt in any way you want. For more information, see Custom Prompt Templates.
## Chat prompt template​
Chat Models take a list of chat messages as input - this list commonly referred to as a prompt.
These chat messages differ from raw string (which you would pass into a LLM model) in that every message is associated with a role.
For example, in OpenAI Chat Completion API, a chat message can be associated with an AI, human or system role. The model is supposed to follow instruction from system chat message more closely.
LangChain provides several prompt templates to make constructing and working with prompts easily. You are encouraged to use these chat related prompt templates instead of PromptTemplate when querying chat models to fully explore the potential of underlying chat model.
```typescript
import {
  ChatPromptTemplate,
  PromptTemplate,
  SystemMessagePromptTemplate,
  AIMessagePromptTemplate,
  HumanMessagePromptTemplate,
} from "langchain/prompts";
import {
  AIMessage,
  HumanMessage,
  SystemMessage,
} from "langchain/schema";
```
To create a message template associated with a role, you use the corresponding <ROLE>MessagePromptTemplate.
For convenience, there is a fromTemplate method exposed on these classes. If you were to use this template, this is what it would look like:
```typescript
const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate);
```
If you wanted to construct the MessagePromptTemplate more directly, you could create a PromptTemplate externally and then pass it in, e.g.:
```typescript
const prompt = new PromptTemplate({
  template: "You are a helpful assistant that translates {input_language} to {output_language}.",
  inputVariables: ["input_language", "output_language"],
});
const systemMessagePrompt2 = new SystemMessagePromptTemplate({
  prompt,
});
```
After that, you can build a ChatPromptTemplate from one or more MessagePromptTemplates. You can use ChatPromptTemplate's format_prompt -- this returns a PromptValue, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.
```typescript
const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt]);

// Format the messages
const formattedChatPrompt = await chatPrompt.formatMessages({
  input_language: "English",
  output_language: "French",
  text: "I love programming.",
});

console.log(formattedChatPrompt);

/*
  [
    SystemMessage {
      content: 'You are a helpful assistant that translates English to French.'
    },
    HumanMessage {
      content: 'I love programming.'
    }
  ]
*/
```



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_templates/partial

# Partial prompt templates
Like other methods, it can make sense to "partial" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.
LangChain supports this in two ways:
These two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.
## Partial With Strings​
One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in the chain, but the baz value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:
```typescript
import { PromptTemplate } from "langchain/prompts";

const prompt = new PromptTemplate({
  template: "{foo}{bar}",
  inputVariables: ["foo", "bar"]
});

const paritalPrompt = await prompt.partial({
  foo: "foo",
});

const formattedPrompt = await paritalPrompt.format({
  bar: "baz",
});

console.log(formattedPrompt);

// foobaz
```
You can also just initialize the prompt with the partialed variables.
```typescript
const prompt = new PromptTemplate({
  template: "{foo}{bar}",
  inputVariables: ["bar"],
  partialVariables: {
    foo: "foo",
  },
});

const formattedPrompt = await prompt.format({
  bar: "baz",
});

console.log(formattedPrompt);

// foobaz
```
## Partial With Functions​
You can also partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables can be tedious. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date.
```typescript
const getCurrentDate = () => {
  return new Date().toISOString();
};

const prompt = new PromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective", "date"],
});

const partialPrompt = await prompt.partial({
  date: getCurrentDate,
});

const formattedPrompt = await partialPrompt.format({
  adjective: "funny",
});

console.log(formattedPrompt)

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```
You can also just initialize the prompt with the partialed variables:
```typescript
const prompt = new PromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective"],
  partialVariables: {
    date: getCurrentDate,
  }
});

const formattedPrompt = await prompt.format({
  adjective: "funny",
});

console.log(formattedPrompt)

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_templates/prompt_composition

# Composition
This notebook goes over how to compose multiple prompts together. This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt. A PipelinePrompt consists of two main parts:
```typescript
import { PromptTemplate, PipelinePromptTemplate } from "langchain/prompts";

const fullPrompt = PromptTemplate.fromTemplate(`{introduction}

{example}

{start}`);

const introductionPrompt = PromptTemplate.fromTemplate(
  `You are impersonating {person}.`
);

const examplePrompt =
  PromptTemplate.fromTemplate(`Here's an example of an interaction:
Q: {example_q}
A: {example_a}`);

const startPrompt = PromptTemplate.fromTemplate(`Now, do this for real!
Q: {input}
A:`);

const composedPrompt = new PipelinePromptTemplate({
  pipelinePrompts: [
    {
      name: "introduction",
      prompt: introductionPrompt,
    },
    {
      name: "example",
      prompt: examplePrompt,
    },
    {
      name: "start",
      prompt: startPrompt,
    },
  ],
  finalPrompt: fullPrompt,
});

const formattedPrompt = await composedPrompt.format({
  person: "Elon Musk",
  example_q: `What's your favorite car?`,
  example_a: "Telsa",
  input: `What's your favorite social media site?`,
});

console.log(formattedPrompt);

/*
  You are impersonating Elon Musk.

  Here's an example of an interaction:
  Q: What's your favorite car?
  A: Telsa

  Now, do this for real!
  Q: What's your favorite social media site?
  A:
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/example_selectors/

# Example selectors
If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.
The base interface is defined as below:
If you have a large number of examples, you may need to programmatically select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so. The base interface is defined as below.
```typescript
class BaseExampleSelector {
  addExample(example: Example): Promise<void | string>;

  selectExamples(input_variables: Example): Promise<Example[]>;
}
```
It needs to expose a selectExamples - this takes in the input variables and then returns a list of examples method - and an addExample method, which saves an example for later selection. It is up to each specific implementation as to how those examples are saved and selected.



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/example_selectors/length_based

# Select by length
This example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.
```typescript
import {
  LengthBasedExampleSelector,
  PromptTemplate,
  FewShotPromptTemplate,
} from "langchain/prompts";

export async function run() {
  // Create a prompt template that will be used to format the examples.
  const examplePrompt = new PromptTemplate({
    inputVariables: ["input", "output"],
    template: "Input: {input}\nOutput: {output}",
  });

  // Create a LengthBasedExampleSelector that will be used to select the examples.
  const exampleSelector = await LengthBasedExampleSelector.fromExamples(
    [
      { input: "happy", output: "sad" },
      { input: "tall", output: "short" },
      { input: "energetic", output: "lethargic" },
      { input: "sunny", output: "gloomy" },
      { input: "windy", output: "calm" },
    ],
    {
      examplePrompt,
      maxLength: 25,
    }
  );

  // Create a FewShotPromptTemplate that will use the example selector.
  const dynamicPrompt = new FewShotPromptTemplate({
    // We provide an ExampleSelector instead of examples.
    exampleSelector,
    examplePrompt,
    prefix: "Give the antonym of every input",
    suffix: "Input: {adjective}\nOutput:",
    inputVariables: ["adjective"],
  });

  // An example with small input, so it selects all examples.
  console.log(await dynamicPrompt.format({ adjective: "big" }));
  /*
   Give the antonym of every input

   Input: happy
   Output: sad

   Input: tall
   Output: short

   Input: energetic
   Output: lethargic

   Input: sunny
   Output: gloomy

   Input: windy
   Output: calm

   Input: big
   Output:
   */

  // An example with long input, so it selects only one example.
  const longString =
    "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else";
  console.log(await dynamicPrompt.format({ adjective: longString }));
  /*
   Give the antonym of every input

   Input: happy
   Output: sad

   Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
   Output:
   */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/example_selectors/similarity

# Select by similarity
This object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import {
  SemanticSimilarityExampleSelector,
  PromptTemplate,
  FewShotPromptTemplate,
} from "langchain/prompts";
import { HNSWLib } from "langchain/vectorstores/hnswlib";

export async function run() {
  // Create a prompt template that will be used to format the examples.
  const examplePrompt = new PromptTemplate({
    inputVariables: ["input", "output"],
    template: "Input: {input}\nOutput: {output}",
  });

  // Create a SemanticSimilarityExampleSelector that will be used to select the examples.
  const exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(
    [
      { input: "happy", output: "sad" },
      { input: "tall", output: "short" },
      { input: "energetic", output: "lethargic" },
      { input: "sunny", output: "gloomy" },
      { input: "windy", output: "calm" },
    ],
    new OpenAIEmbeddings(),
    HNSWLib,
    { k: 1 }
  );

  // Create a FewShotPromptTemplate that will use the example selector.
  const dynamicPrompt = new FewShotPromptTemplate({
    // We provide an ExampleSelector instead of examples.
    exampleSelector,
    examplePrompt,
    prefix: "Give the antonym of every input",
    suffix: "Input: {adjective}\nOutput:",
    inputVariables: ["adjective"],
  });

  // Input is about the weather, so should select eg. the sunny/gloomy example
  console.log(await dynamicPrompt.format({ adjective: "rainy" }));
  /*
   Give the antonym of every input

   Input: sunny
   Output: gloomy

   Input: rainy
   Output:
   */

  // Input is a measurement, so should select the tall/short example
  console.log(await dynamicPrompt.format({ adjective: "large" }));
  /*
   Give the antonym of every input

   Input: tall
   Output: short

   Input: large
   Output:
   */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_selectors/

# Prompt selectors
Prompt selectors are useful when you want to programmatically select a prompt based on the type of model you are using in a chain. This is especially relevant when swapping chat models and LLMs.
The interface for prompt selectors is quite simple:
```typescript
abstract class BasePromptSelector {
  abstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;
}
```
The getPrompt method takes in a language model and returns an appropriate prompt template.
We currently offer a ConditionalPromptSelector that allows you to specify a set of conditions and prompt templates. The first condition that evaluates to true will be used to select the prompt template.
```typescript
const QA_PROMPT_SELECTOR = new ConditionalPromptSelector(DEFAULT_QA_PROMPT, [
  [isChatModel, CHAT_PROMPT],
]);
```
This will return DEFAULT_QA_PROMPT if the model is not a chat model, and CHAT_PROMPT if it is.
The example below shows how to use a prompt selector when loading a chain:
```typescript
const loadQAStuffChain = (
  llm: BaseLanguageModel,
  params: StuffQAChainParams = {}
) => {
  const { prompt = QA_PROMPT_SELECTOR.getPrompt(llm) } = params;
  const llmChain = new LLMChain({ prompt, llm });
  const chain = new StuffDocumentsChain({ llmChain });
  return chain;
};
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/

# Language models
LangChain provides interfaces and integrations for two types of models:
## LLMs vs Chat Models​
LLMs and Chat Models are subtly but importantly different. LLMs in LangChain refer to pure text completion models.
The APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM.
Chat models are often backed by LLMs but tuned specifically for having conversations.
And, crucially, their provider APIs expose a different interface than pure text completion models. Instead of a single string,
they take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of "System",
"AI", and "Human"). And they return a ("AI") chat message as output. GPT-4 and Anthropic's Claude are both implemented as Chat Models.
To make it possible to swap LLMs and Chat Models, both implement the Base Language Model interface. This exposes common
methods "predict", which takes a string and returns a string, and "predict messages", which takes messages and returns a message.
If you are using a specific model it's recommended you use the methods specific to that model class (i.e., "predict" for LLMs and "predict messages" for Chat Models),
but if you're creating an application that should work with different types of models the shared interface can be helpful.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/

# LLMs
Large Language Models (LLMs) are a core component of LangChain.
LangChain does not serve it's own LLMs, but rather provides a standard interface for interacting with many different LLMs.
For more detailed documentation check out our:
How-to guides: Walkthroughs of core functionality, like streaming, async, etc.
Integrations: How to use different LLM providers (OpenAI, Anthropic, etc.)
## Get started​
There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.
In this walkthrough we'll work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types.
### Setup​
To start we'll need to install the official OpenAI package:
```typescript
npm install -S openai
```
```typescript
yarn add openai
```
```typescript
pnpm add openai
```
Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:
```typescript
export OPENAI_API_KEY="..."
```
If you'd prefer not to set an environment variable you can pass the key in directly via the openAIApiKey parameter when initializing the OpenAI LLM class:
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  openAIApiKey: "YOUR_KEY_HERE",
});
```
otherwise you can initialize with an empty object:
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({});
```
### call: string in -> string out​
The simplest way to use an LLM is the .call method: pass in a string, get a string completion.
```typescript
const res = await llm.call("Tell me a joke");

console.log(res);

// "Why did the chicken cross the road?\n\nTo get to the other side."
```
### generate: batch calls, richer outputs​
generate lets you can call the model with a list of strings, getting back a more complete response than just the text. This complete response can includes things like multiple top responses and other LLM provider-specific information:
```typescript
const llmResult = await llm.generate(["Tell me a joke", "Tell me a poem"], ["Tell me a joke", "Tell me a poem"]);

console.log(llmResult.generations.length)

// 30

console.log(llmResult.generations[0]);

/*
  [
    {
      text: "\n\nQ: What did the fish say when it hit the wall?\nA: Dam!",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/

console.log(llmResult.generations[1]);

/*
  [
    {
      text: "\n\nRoses are red,\nViolets are blue,\nSugar is sweet,\nAnd so are you.",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/
```
You can also access provider specific information that is returned. This information is NOT standardized across providers.
```typescript
console.log(llmResult.llmOutput);

/*
  {
    tokenUsage: { completionTokens: 46, promptTokens: 8, totalTokens: 54 }
  }
*/

```
Here's an example with additional parameters, which sets -1 for max_tokens to turn on token size calculations:
```typescript
import { OpenAI } from "langchain/llms/openai";

export const run = async () => {
  const model = new OpenAI({
    // customize openai model that's used, `text-davinci-003` is the default
    modelName: "text-ada-001",

    // `max_tokens` supports a magic -1 param where the max token length for the specified modelName
    //  is calculated and included in the request to OpenAI as the `max_tokens` param
    maxTokens: -1,

    // use `modelKwargs` to pass params directly to the openai call
    // note that they use snake_case instead of camelCase
    modelKwargs: {
      user: "me",
    },

    // for additional logging for debugging purposes
    verbose: true,
  });

  const resA = await model.call(
    "What would be a good company name a company that makes colorful socks?"
  );
  console.log({ resA });
  // { resA: '\n\nSocktastic Colors' }
};
```
#### API Reference:
## Advanced​
This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section.
Both LLMs and Chat Models are built on top of the BaseLanguageModel class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.
The BaseLanguageModel class has two abstract methods: generatePrompt and getNumTokens, which are implemented by BaseChatModel and BaseLLM respectively.
BaseLLM is a subclass of BaseLanguageModel that provides a common interface for LLMs while BaseChatModel is a subclass of BaseLanguageModel that provides a common interface for chat models.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/cancelling_requests

# Cancelling requests
You can cancel a request by passing a signal option when you call the model. For example, for OpenAI:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ temperature: 1 });
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.

const res = await model.call(
  "What would be a good company name a company that makes colorful socks?",
  { signal: controller.signal }
);

console.log(res);
/*
'\n\nSocktastic Colors'
*/
```
#### API Reference:
Note, this will only cancel the outgoing request if the underlying provider exposes that option. LangChain will cancel the underlying request if possible, otherwise it will cancel the processing of the response.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/dealing_with_api_errors

# Dealing with API Errors
If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a maxRetries option when you instantiate the model. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ maxRetries: 10 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/dealing_with_rate_limits

# Dealing with Rate Limits
Some LLM providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a maxConcurrency option when instantiating an LLM. This option allows you to specify the maximum number of concurrent requests you want to make to the LLM provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.
For example, if you set maxConcurrency: 5, then LangChain will only send 5 requests to the LLM provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.
To use this feature, simply pass maxConcurrency: <number> when you instantiate the LLM. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ maxConcurrency: 5 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/llm_caching

# Caching
LangChain provides an optional caching layer for LLMs. This is useful for two reasons:
It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.
It can speed up your application by reducing the number of API calls you make to the LLM provider.
```typescript
import { OpenAI } from "langchain/llms/openai";

// To make the caching really obvious, lets use a slower model.
const model = new OpenAI({
  modelName: "text-davinci-002",
  cache: true,
  n: 2,
  bestOf: 2
});
```
## In Memory Cache​
The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.
```typescript
// The first time, it is not yet in cache, so it should take longer
const res = await model.predict("Tell me a joke");
console.log(res);

/*
  CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms
  Wall time: 4.83 s


  "\n\nWhy did the chicken cross the road?\n\nTo get to the other side."
*/
```
```typescript
// The second time it is, so it goes faster
const res2 = await model.predict("Tell me a joke");
console.log(res2);

/*
  CPU times: user 238 µs, sys: 143 µs, total: 381 µs
  Wall time: 1.76 ms


  "\n\nWhy did the chicken cross the road?\n\nTo get to the other side."
*/
```
## Caching with Momento​
LangChain also provides a Momento-based cache. Momento is a distributed, serverless cache that requires zero setup or infrastructure maintenance. To use it, you'll need to install the @gomomento/sdk package:
```typescript
npm install @gomomento/sdk
```
Next you'll need to sign up and create an API key. Once you've done that, pass a cache option when you instantiate the LLM like this:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { MomentoCache } from "langchain/cache/momento";
import {
  CacheClient,
  Configurations,
  CredentialProvider,
} from "@gomomento/sdk";

// See https://github.com/momentohq/client-sdk-javascript for connection options
const client = new CacheClient({
  configuration: Configurations.Laptop.v1(),
  credentialProvider: CredentialProvider.fromEnvironmentVariable({
    environmentVariableName: "MOMENTO_AUTH_TOKEN",
  }),
  defaultTtlSeconds: 60 * 60 * 24,
});
const cache = await MomentoCache.fromProps({
  client,
  cacheName: "langchain",
});

const model = new OpenAI({ cache });
```
#### API Reference:
## Caching with Redis​
LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the redis package:
```typescript
npm install ioredis
```
Then, you can pass a cache option when you instantiate the LLM. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RedisCache } from "langchain/cache/ioredis";
import { Redis } from "ioredis";

// See https://github.com/redis/ioredis for connection options
const client = new Redis({});

const cache = new RedisCache(client);

const model = new OpenAI({ cache });
```
## Caching with Upstash Redis​
LangChain also provides an Upstash Redis-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Upstash Redis client uses HTTP and supports edge environments. To use it, you'll need to install the @upstash/redis package:
```typescript
npm install @upstash/redis
```
You'll also need an Upstash account and a Redis database to connect to. Once you've done that, retrieve your REST URL and REST token.
Then, you can pass a cache option when you instantiate the LLM. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { UpstashRedisCache } from "langchain/cache/upstash_redis";

// See https://docs.upstash.com/redis/howto/connectwithupstashredis#quick-start for connection options
const cache = new UpstashRedisCache({
  config: {
    url: "UPSTASH_REDIS_REST_URL",
    token: "UPSTASH_REDIS_REST_TOKEN",
  },
});

const model = new OpenAI({ cache });
```
#### API Reference:
You can also directly pass in a previously created @upstash/redis client instance:
```typescript
import { Redis } from "@upstash/redis";
import https from "https";

import { OpenAI } from "langchain/llms/openai";
import { UpstashRedisCache } from "langchain/cache/upstash_redis";

// const client = new Redis({
//   url: process.env.UPSTASH_REDIS_REST_URL!,
//   token: process.env.UPSTASH_REDIS_REST_TOKEN!,
//   agent: new https.Agent({ keepAlive: true }),
// });

// Or simply call Redis.fromEnv() to automatically load the UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN environment variables.
const client = Redis.fromEnv({
  agent: new https.Agent({ keepAlive: true }),
});

const cache = new UpstashRedisCache({ client });
const model = new OpenAI({ cache });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/streaming_llm

# Streaming
Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.
To utilize streaming, use a CallbackHandler like so:
```typescript
import { OpenAI } from "langchain/llms/openai";

// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const model = new OpenAI({
  maxTokens: 25,
  streaming: true,
});

const response = await model.call("Tell me a joke.", {
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        console.log({ token });
      },
    },
  ],
});
console.log(response);
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }


Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
```
#### API Reference:
We still have access to the end LLMResult if using generate. However, token_usage is not currently supported for streaming.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/subscribing_events

# Subscribing to events
Especially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a LLM processes a prompt. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the LLM, you can pass callbacks to the LLM for custom logging (or anything else you want to do) as the model goes through the steps:
For more info on the events available see the Callbacks section of the docs.
```typescript
import { LLMResult } from "langchain/schema";
import { OpenAI } from "langchain/llms/openai";
import { Serialized } from "langchain/load/serializable";

// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const model = new OpenAI({
  callbacks: [
    {
      handleLLMStart: async (llm: Serialized, prompts: string[]) => {
        console.log(JSON.stringify(llm, null, 2));
        console.log(JSON.stringify(prompts, null, 2));
      },
      handleLLMEnd: async (output: LLMResult) => {
        console.log(JSON.stringify(output, null, 2));
      },
      handleLLMError: async (err: Error) => {
        console.error(err);
      },
    },
  ],
});

await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
// {
//     "name": "openai"
// }
// [
//     "What would be a good company name a company that makes colorful socks?"
// ]
// {
//   "generations": [
//     [
//         {
//             "text": "\n\nSocktastic Splashes.",
//             "generationInfo": {
//                 "finishReason": "stop",
//                 "logprobs": null
//             }
//         }
//     ]
//  ],
//   "llmOutput": {
//     "tokenUsage": {
//         "completionTokens": 9,
//          "promptTokens": 14,
//          "totalTokens": 23
//     }
//   }
// }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a timeout option, in milliseconds, when you call the model. For example, for OpenAI:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ temperature: 1 });

const resA = await model.call(
  "What would be a good company name a company that makes colorful socks?",
  { timeout: 1000 } // 1s timeout
);

console.log({ resA });
// '\n\nSocktastic Colors' }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ai21

# AI21
You can get started with AI21Labs' Jurassic family of models, as well as see a full list of available foundational models, by signing up for an API key on their website.
Here's an example of initializing an instance in LangChain.js:
```typescript
import { AI21 } from "langchain/llms/ai21";

const model = new AI21({
  ai21ApiKey: "YOUR_AI21_API_KEY", // Or set as process.env.AI21_API_KEY
});

const res = await model.call(`Translate "I love programming" into German.`);

console.log({ res });

/*
  {
    res: "\nIch liebe das Programmieren."
  }
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/aleph_alpha

# AlephAlpha
LangChain.js supports AlephAlpha's Luminous family of models. You'll need to sign up for an API key on their website.
Here's an example:
```typescript
import { AlephAlpha } from "langchain/llms/aleph_alpha";

const model = new AlephAlpha({
  aleph_alpha_api_key: "YOUR_ALEPH_ALPHA_API_KEY", // Or set as process.env.ALEPH_ALPHA_API_KEY
});

const res = await model.call(`Is cereal soup?`);

console.log({ res });

/*
  {
    res: "\nIs soup a cereal? I don’t think so, but it is delicious."
  }
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/aws_sagemaker

# AWS SageMakerEndpoint
LangChain.js supports integration with AWS SageMaker-hosted endpoints. Check Amazon SageMaker JumpStart for a list of available models, and how to deploy your own.
Here's an example:
```typescript
npm install @aws-sdk/client-sagemaker-runtime
```
```typescript
yarn add @aws-sdk/client-sagemaker-runtime
```
```typescript
pnpm add @aws-sdk/client-sagemaker-runtime
```
```typescript
import {
  SageMakerLLMContentHandler,
  SageMakerEndpoint,
} from "langchain/llms/sagemaker_endpoint";

// Custom for whatever model you'll be using
class HuggingFaceTextGenerationGPT2ContentHandler
  implements SageMakerLLMContentHandler
{
  contentType = "application/json";

  accepts = "application/json";

  async transformInput(prompt: string, modelKwargs: Record<string, unknown>) {
    const inputString = JSON.stringify({
      text_inputs: prompt,
      ...modelKwargs,
    });
    return Buffer.from(inputString);
  }

  async transformOutput(output: Uint8Array) {
    const responseJson = JSON.parse(Buffer.from(output).toString("utf-8"));
    return responseJson.generated_texts[0];
  }
}

const contentHandler = new HuggingFaceTextGenerationGPT2ContentHandler();

const model = new SageMakerEndpoint({
  endpointName:
    "jumpstart-example-huggingface-textgener-2023-05-16-22-35-45-660", // Your endpoint name here
  modelKwargs: { temperature: 1e-10 },
  contentHandler,
  clientOptions: {
    region: "YOUR AWS ENDPOINT REGION",
    credentials: {
      accessKeyId: "YOUR AWS ACCESS ID",
      secretAccessKey: "YOUR AWS SECRET ACCESS KEY",
    },
  },
});

const res = await model.call("Hello, my name is ");

console.log({ res });

/*
  {
    res: "_____. I am a student at the University of California, Berkeley. I am a member of the American Association of University Professors."
  }
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/azure

# Azure OpenAI
You can also use the OpenAI class to call OpenAI models hosted on Azure. Here's an example:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-API-KEY",
  azureOpenAIApiInstanceName: "YOUR-INSTANCE-NAME",
  azureOpenAIApiDeploymentName: "YOUR-DEPLOYMENT-NAME",
  azureOpenAIApiVersion: "YOUR-API-VERSION",
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/cohere

# Cohere
LangChain.js supports Cohere LLMs. Here's an example:
```typescript
npm install cohere-ai
```
```typescript
yarn add cohere-ai
```
```typescript
pnpm add cohere-ai
```
```typescript
import { Cohere } from "langchain/llms/cohere";

const model = new Cohere({
  maxTokens: 20,
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.COHERE_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/google_palm

# Google PaLM
The Google PaLM API can be integrated by first
installing the required packages:
```typescript
npm install google-auth-library @google-ai/generativelanguage
```
```typescript
yarn add google-auth-library @google-ai/generativelanguage
```
```typescript
pnpm add google-auth-library @google-ai/generativelanguage
```
Create an API key from Google MakerSuite. You can then set
the key as GOOGLE_PALM_API_KEY environment variable or pass it as apiKey parameter while instantiating
the model.
```typescript
import { GooglePaLM } from "langchain/llms/googlepalm";

export const run = async () => {
  const model = new GooglePaLM({
    apiKey: "<YOUR API KEY>", // or set it in environment variable as `GOOGLE_PALM_API_KEY`
    // other params
    temperature: 1, // OPTIONAL
    modelName: "models/text-bison-001", // OPTIONAL
    maxOutputTokens: 1024, // OPTIONAL
    topK: 40, // OPTIONAL
    topP: 3, // OPTIONAL
    safetySettings: [
      // OPTIONAL
      {
        category: "HARM_CATEGORY_DANGEROUS",
        threshold: "BLOCK_MEDIUM_AND_ABOVE",
      },
    ],
    stopSequences: ["stop"], // OPTIONAL
  });
  const res = await model.call(
    "What would be a good company name for a company that makes colorful socks?"
  );
  console.log({ res });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/google_vertex_ai

# Google Vertex AI
The Vertex AI implementation is meant to be used in Node.js and not
directly in a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
```typescript
import { GoogleVertexAI } from "langchain/llms/googlevertexai";

/*
 * Before running this, you should make sure you have created a
 * Google Cloud Project that is permitted to the Vertex AI API.
 *
 * You will also need permission to access this project / API.
 * Typically, this is done in one of three ways:
 * - You are logged into an account permitted to that project.
 * - You are running this on a machine using a service account permitted to
 *   the project.
 * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the
 *   path of a credentials file for a service account permitted to the project.
 */
export const run = async () => {
  const model = new GoogleVertexAI({
    temperature: 0.7,
  });
  const res = await model.call(
    "What would be a good company name a company that makes colorful socks?"
  );
  console.log({ res });
};
```
#### API Reference:
Google also has separate models for their "Codey" code generation models.
The "code-gecko" model is useful for code completion:
```typescript
import { GoogleVertexAI } from "langchain/llms/googlevertexai";

/*
 * Before running this, you should make sure you have created a
 * Google Cloud Project that is permitted to the Vertex AI API.
 *
 * You will also need permission to access this project / API.
 * Typically, this is done in one of three ways:
 * - You are logged into an account permitted to that project.
 * - You are running this on a machine using a service account permitted to
 *   the project.
 * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the
 *   path of a credentials file for a service account permitted to the project.
 */

const model = new GoogleVertexAI({
  model: "code-gecko",
});
const res = await model.call("for (let co=0;");
console.log({ res });
```
#### API Reference:
While the "code-bison" model is better at larger code generation based on
a text prompt:
```typescript
import { GoogleVertexAI } from "langchain/llms/googlevertexai";

/*
 * Before running this, you should make sure you have created a
 * Google Cloud Project that is permitted to the Vertex AI API.
 *
 * You will also need permission to access this project / API.
 * Typically, this is done in one of three ways:
 * - You are logged into an account permitted to that project.
 * - You are running this on a machine using a service account permitted to
 *   the project.
 * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the
 *   path of a credentials file for a service account permitted to the project.
 */

const model = new GoogleVertexAI({
  model: "code-bison",
  maxOutputTokens: 2048,
});
const res = await model.call("A Javascript function that counts from 1 to 10.");
console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_inference

# HuggingFaceInference
Here's an example of calling a HugggingFaceInference model as an LLM:
```typescript
npm install @huggingface/inference@1
```
```typescript
yarn add @huggingface/inference@1
```
```typescript
pnpm add @huggingface/inference@1
```
```typescript
import { HuggingFaceInference } from "langchain/llms/hf";

const model = new HuggingFaceInference({
  model: "gpt2",
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
});
const res = await model.call("1 + 1 =");
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/openai

# OpenAI
Here's how you can initialize an OpenAI LLM instance:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({
  modelName: "text-davinci-003", // Defaults to "text-davinci-003" if no model provided.
  temperature: 0.9,
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/prompt_layer_openai

# PromptLayer OpenAI
LangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:
```typescript
import { PromptLayerOpenAI } from "langchain/llms/openai";

const model = new PromptLayerOpenAI({
  temperature: 0.9,
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  promptLayerApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
```
# Azure PromptLayerOpenAI
LangChain also integrates with PromptLayer for Azure-hosted OpenAI instances:
```typescript
import { PromptLayerOpenAI } from "langchain/llms/openai";

const model = new PromptLayerOpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-AOAI-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-AOAI-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-AOAI-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiCompletionsDeploymentName:
    "YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
  azureOpenAIApiEmbeddingsDeploymentName:
    "YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-AOAI-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
  promptLayerApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
```
The request and the response will be logged in the PromptLayer dashboard.
Note: In streaming mode PromptLayer will not log the response.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/replicate

# Replicate
Here's an example of calling a Replicate model as an LLM:
```typescript
npm install replicate
```
```typescript
yarn add replicate
```
```typescript
pnpm add replicate
```
```typescript
import { Replicate } from "langchain/llms/replicate";

const model = new Replicate({
  model:
    "a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5",
});

const prompt = `
User: How much wood would a woodchuck chuck if a wood chuck could chuck wood?
Assistant:`;

const res = await model.call(prompt);
console.log({ res });
/*
  {
    res: "I'm happy to help! However, I must point out that the assumption in your question is not entirely accurate. " +
      + "Woodchucks, also known as groundhogs, do not actually chuck wood. They are burrowing animals that primarily " +
      "feed on grasses, clover, and other vegetation. They do not have the physical ability to chuck wood.\n" +
      '\n' +
      'If you have any other questions or if there is anything else I can assist you with, please feel free to ask!'
  }
*/
```
#### API Reference:
You can run other models through Replicate by changing the model parameter.
You can find a full list of models on Replicate's website.



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/

# Chat models
Chat models are a variation on language models.
While chat models use language models under the hood, the interface they expose is a bit different.
Rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.
Chat model APIs are fairly new, so we are still figuring out the correct abstractions.
The following sections of documentation are provided:
How-to guides: Walkthroughs of core functionality, like streaming, creating chat prompts, etc.
Integrations: How to use different chat model providers (OpenAI, Anthropic, etc).
## Get started​
### Setup​
To start we'll need to install the official OpenAI package:
```typescript
npm install -S openai
```
```typescript
yarn add openai
```
```typescript
pnpm add openai
```
Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:
```typescript
export OPENAI_API_KEY="..."
```
If you'd prefer not to set an environment variable you can pass the key in directly via the openAIApiKey parameter when initializing the ChatOpenAI class:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({
  openAIApiKey: "YOUR_KEY_HERE"
});
```
otherwise you can initialize it with an empty object:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({});
```
### Messages​
The chat model interface is based around messages rather than raw text.
The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, FunctionMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage
### call​
#### Messages in -> message out​
You can get chat completions by passing one or more messages to the chat model. The response will be a message.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const chat = new ChatOpenAI();
// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.
const response = await chat.call([
  new HumanMessage(
    "What is a good name for a company that makes colorful socks?"
  ),
]);
console.log(response);
// AIMessage { text: '\n\nRainbow Sox Co.' }
```
#### API Reference:
OpenAI's chat model also supports multiple messages as input. See here for more information. Here is an example of sending a system and user message to the chat model:
```typescript
const response2 = await chat.call([
  new SystemMessage(
    "You are a helpful assistant that translates English to French."
  ),
  new HumanMessage("Translate: I love programming."),
]);
console.log(response2);
// AIMessage { text: "J'aime programmer." }
```
### generate​
#### Batch calls, richer outputs​
You can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.
```typescript
const response3 = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);
console.log(response3);
/*
  {
    generations: [
      [
        {
          text: "J'aime programmer.",
          message: AIMessage { text: "J'aime programmer." },
        }
      ],
      [
        {
          text: "J'aime l'intelligence artificielle.",
          message: AIMessage { text: "J'aime l'intelligence artificielle." }
        }
      ]
    ]
  }
*/
```
You can recover things like token usage from this LLMResult:
```typescript
console.log(response3.llmOutput);
/*
  {
    tokenUsage: { completionTokens: 20, promptTokens: 69, totalTokens: 89 }
  }
*/
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/cancelling_requests

# Cancelling requests
You can cancel a request by passing a signal option when you call the model. For example, for OpenAI:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const model = new ChatOpenAI({ temperature: 1 });
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.

const res = await model.call(
  [
    new HumanMessage(
      "What is a good name for a company that makes colorful socks?"
    ),
  ],
  { signal: controller.signal }
);

console.log(res);
/*
'\n\nSocktastic Colors'
*/
```
#### API Reference:
Note, this will only cancel the outgoing request if the underlying provider exposes that option. LangChain will cancel the underlying request if possible, otherwise it will cancel the processing of the response.



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/dealing_with_api_errors

# Dealing with API Errors
If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a maxRetries option when you instantiate the model. For example:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const model = new ChatOpenAI({ maxRetries: 10 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/dealing_with_rate_limits

# Dealing with rate limits
Some providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a maxConcurrency option when instantiating a Chat Model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.
For example, if you set maxConcurrency: 5, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.
To use this feature, simply pass maxConcurrency: <number> when you instantiate the LLM. For example:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const model = new ChatOpenAI({ maxConcurrency: 5 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/llm_chain

# LLMChain
You can use the existing LLMChain in a very similar way to before - provide a prompt and a model.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate
} from "langchain/prompts";

const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate)

const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt])

const chat = new ChatOpenAI({
  temperature: 0,
});

const chain = new LLMChain({
  llm: chat,
  prompt: chatPrompt,
});

const result = await chain.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming",
});

// { text: "J'adore programmer" }
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/prompts

# Prompts
Prompts for Chat models are built around messages, instead of just plain text.
You can make use of templating by using a ChatPromptTemplate from one or more MessagePromptTemplates, then using ChatPromptTemplate's
formatPrompt method.
For convenience, there is also a fromTemplate method exposed on the template. If you were to use this template, this is what it would look like:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate
} from "langchain/prompts";

const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate);

const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt]);

const chat = new ChatOpenAI({
  temperature: 0,
});

const chain = new LLMChain({
  llm: chat,
  prompt: chatPrompt,
});

const result = await chain.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming",
});
```
```typescript
// { text: "J'adore programmer" }
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/streaming

# Streaming
Some Chat models provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const chat = new ChatOpenAI({
  maxTokens: 25,
  streaming: true,
});

const response = await chat.call([new HumanMessage("Tell me a joke.")], {
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        console.log({ token });
      },
    },
  ],
});

console.log(response);
// { token: '' }
// { token: '\n\n' }
// { token: 'Why' }
// { token: ' don' }
// { token: "'t" }
// { token: ' scientists' }
// { token: ' trust' }
// { token: ' atoms' }
// { token: '?\n\n' }
// { token: 'Because' }
// { token: ' they' }
// { token: ' make' }
// { token: ' up' }
// { token: ' everything' }
// { token: '.' }
// { token: '' }
// AIMessage {
//   text: "\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything."
// }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/subscribing_events

# Subscribing to events
Especially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a Chat Model processes a prompt. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the Chat Model, you can pass callbacks to the Chat Model for custom logging (or anything else you want to do) as the model goes through the steps:
For more info on the events available see the Callbacks section of the docs.
```typescript
import { HumanMessage, LLMResult } from "langchain/schema";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Serialized } from "langchain/load/serializable";

// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const model = new ChatOpenAI({
  callbacks: [
    {
      handleLLMStart: async (llm: Serialized, prompts: string[]) => {
        console.log(JSON.stringify(llm, null, 2));
        console.log(JSON.stringify(prompts, null, 2));
      },
      handleLLMEnd: async (output: LLMResult) => {
        console.log(JSON.stringify(output, null, 2));
      },
      handleLLMError: async (err: Error) => {
        console.error(err);
      },
    },
  ],
});

await model.call([
  new HumanMessage(
    "What is a good name for a company that makes colorful socks?"
  ),
]);
/*
{
  "name": "openai"
}
[
  "Human: What is a good name for a company that makes colorful socks?"
]
{
  "generations": [
    [
      {
        "text": "Rainbow Soles",
        "message": {
          "text": "Rainbow Soles"
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 4,
      "promptTokens": 21,
      "totalTokens": 25
    }
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a timeout option, in milliseconds, when you call the model. For example, for OpenAI:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const chat = new ChatOpenAI({ temperature: 1 });

const response = await chat.call(
  [
    new HumanMessage(
      "What is a good name for a company that makes colorful socks?"
    ),
  ],
  { timeout: 1000 } // 1s timeout
);
console.log(response);
// AIMessage { text: '\n\nRainbow Sox Co.' }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/anthropic

# ChatAnthropic
LangChain supports Anthropic's Claude family of chat models. You can initialize an instance like this:
```typescript
import { ChatAnthropic } from "langchain/chat_models/anthropic";

const model = new ChatAnthropic({
  temperature: 0.9,
  anthropicApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.ANTHROPIC_API_KEY
});
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/azure

# Azure ChatOpenAI
You can also use the ChatOpenAI class to access OpenAI instances hosted on Azure like this:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const model = new ChatOpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/baidu_wenxin

# ChatBaiduWenxin
LangChain.js supports Baidu's ERNIE-bot family of models. Here's an example:
```typescript
import { ChatBaiduWenxin } from "langchain/chat_models/baiduwenxin";
import { HumanMessage } from "langchain/schema";

// Default model is ERNIE-Bot-turbo
const ernieTurbo = new ChatBaiduWenxin({
  baiduApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.BAIDU_API_KEY
  baiduSecretKey: "YOUR-SECRET-KEY", // In Node.js defaults to process.env.BAIDU_SECRET_KEY
});

// Use ERNIE-Bot
const ernie = new ChatBaiduWenxin({
  modelName: "ERNIE-Bot",
  temperature: 1, // Only ERNIE-Bot supports temperature
  baiduApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.BAIDU_API_KEY
  baiduSecretKey: "YOUR-SECRET-KEY", // In Node.js defaults to process.env.BAIDU_SECRET_KEY
});

const messages = [new HumanMessage("Hello")];

let res = await ernieTurbo.call(messages);
/*
AIChatMessage {
  text: 'Hello! How may I assist you today?',
  name: undefined,
  additional_kwargs: {}
  }
}
*/

res = await ernie.call(messages);
/*
AIChatMessage {
  text: 'Hello! How may I assist you today?',
  name: undefined,
  additional_kwargs: {}
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/google_palm

# ChatGooglePaLM
The Google PaLM API can be integrated by first
installing the required packages:
```typescript
npm install google-auth-library @google-ai/generativelanguage
```
```typescript
yarn add google-auth-library @google-ai/generativelanguage
```
```typescript
pnpm add google-auth-library @google-ai/generativelanguage
```
Create an API key from Google MakerSuite. You can then set
the key as GOOGLE_PALM_API_KEY environment variable or pass it as apiKey parameter while instantiating
the model.
```typescript
import { ChatGooglePaLM } from "langchain/chat_models/googlepalm";
import { AIMessage, HumanMessage, SystemMessage } from "langchain/schema";

export const run = async () => {
  const model = new ChatGooglePaLM({
    apiKey: "<YOUR API KEY>", // or set it in environment variable as `GOOGLE_PALM_API_KEY`
    temperature: 0.7, // OPTIONAL
    modelName: "models/chat-bison-001", // OPTIONAL
    topK: 40, // OPTIONAL
    topP: 3, // OPTIONAL
    examples: [
      // OPTIONAL
      {
        input: new HumanMessage("What is your favorite sock color?"),
        output: new AIMessage("My favorite sock color be arrrr-ange!"),
      },
    ],
  });

  // ask questions
  const questions = [
    new SystemMessage(
      "You are a funny assistant that answers in pirate language."
    ),
    new HumanMessage("What is your favorite food?"),
  ];

  // You can also use the model as part of a chain
  const res = await model.call(questions);
  console.log({ res });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/google_vertex_ai

# ChatGoogleVertexAI
The Vertex AI implementation is meant to be used in Node.js and not
directly from a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
The ChatGoogleVertexAI class works just like other chat-based LLMs,
with a few exceptions:
```typescript
import { ChatGoogleVertexAI } from "langchain/chat_models/googlevertexai";

const model = new ChatGoogleVertexAI({
  temperature: 0.7,
});
```
#### API Reference:
There is also an optional examples constructor parameter that can help the model understand what an appropriate response
looks like.
```typescript
import { ChatGoogleVertexAI } from "langchain/chat_models/googlevertexai";
import { AIMessage, HumanMessage, SystemMessage } from "langchain/schema";

export const run = async () => {
  const examples = [
    {
      input: new HumanMessage("What is your favorite sock color?"),
      output: new AIMessage("My favorite sock color be arrrr-ange!"),
    },
  ];
  const model = new ChatGoogleVertexAI({
    temperature: 0.7,
    examples,
  });
  const questions = [
    new SystemMessage(
      "You are a funny assistant that answers in pirate language."
    ),
    new HumanMessage("What is your favorite food?"),
  ];
  // You can also use the model as part of a chain
  const res = await model.call(questions);
  console.log({ res });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/openai

# ChatOpenAI
You can use OpenAI's chat models as follows:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";
import { SerpAPI } from "langchain/tools";

const model = new ChatOpenAI({
  temperature: 0.9,
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
});

// You can also pass tools or functions to the model, learn more here
// https://platform.openai.com/docs/guides/gpt/function-calling

const modelForFunctionCalling = new ChatOpenAI({
  modelName: "gpt-4-0613",
  temperature: 0,
});

await modelForFunctionCalling.predictMessages(
  [new HumanMessage("What is the weather in New York?")],
  { tools: [new SerpAPI()] }
  // Tools will be automatically formatted as functions in the OpenAI format
);
/*
AIMessage {
  text: '',
  name: undefined,
  additional_kwargs: {
    function_call: {
      name: 'search',
      arguments: '{\n  "input": "current weather in New York"\n}'
    }
  }
}
*/

await modelForFunctionCalling.predictMessages(
  [new HumanMessage("What is the weather in New York?")],
  {
    functions: [
      {
        name: "get_current_weather",
        description: "Get the current weather in a given location",
        parameters: {
          type: "object",
          properties: {
            location: {
              type: "string",
              description: "The city and state, e.g. San Francisco, CA",
            },
            unit: { type: "string", enum: ["celsius", "fahrenheit"] },
          },
          required: ["location"],
        },
      },
    ],
    // You can set the `function_call` arg to force the model to use a function
    function_call: {
      name: "get_current_weather",
    },
  }
);
/*
AIMessage {
  text: '',
  name: undefined,
  additional_kwargs: {
    function_call: {
      name: 'get_current_weather',
      arguments: '{\n  "location": "New York"\n}'
    }
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/prompt_layer_openai

# PromptLayerChatOpenAI
You can pass in the optional returnPromptLayerId boolean to get a promptLayerRequestId like below. Here is an example of getting the PromptLayerChatOpenAI requestID:
```typescript
import { PromptLayerChatOpenAI } from "langchain/chat_models/openai";

const chat = new PromptLayerChatOpenAI({
  returnPromptLayerId: true,
});

const respA = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
  ],
]);

console.log(JSON.stringify(respA, null, 3));

/*
  {
    "generations": [
      [
        {
          "text": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?",
          "message": {
            "type": "ai",
            "data": {
              "content": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?"
            }
          },
          "generationInfo": {
            "promptLayerRequestId": 2300682
          }
        }
      ]
    ],
    "llmOutput": {
      "tokenUsage": {
        "completionTokens": 35,
        "promptTokens": 19,
        "totalTokens": 54
      }
    }
  }
*/
```



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/

# Output parsers
Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.
Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:
And then one optional one:
## Get started​
Below we go over one useful type of output parser, the StructuredOutputParser.
## Structured Output Parser​
This output parser can be used when you want to return multiple fields. If you want complex schema returned (i.e. a JSON object with arrays of strings), use the Zod Schema detailed below.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// With a `StructuredOutputParser` we can define a schema for the output.
const parser = StructuredOutputParser.fromNamesAndDescriptions({
  answer: "answer to the user's question",
  source: "source used to answer the user's question, should be a website.",
});

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
You must format your output as a JSON value that adheres to a given "JSON Schema" instance.

"JSON Schema" is a declarative language that allows you to annotate and validate JSON documents.

For example, the example "JSON Schema" instance {{"properties": {{"foo": {{"description": "a list of test words", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
would match an object with one required property, "foo". The "type" property specifies "foo" must be an "array", and the "description" property semantically describes it as "a list of test words". The items within "foo" must be strings.
Thus, the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of this example "JSON Schema". The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!

Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:
```json
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "source": "https://en.wikipedia.org/wiki/Paris"}
*/

console.log(await parser.parse(response));
// { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/Paris' }
```
#### API Reference:
## Structured Output Parser with Zod Schema​
This output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. z.date() is not allowed.
```typescript
import { z } from "zod";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.
const parser = StructuredOutputParser.fromZodSchema(
  z.object({
    answer: z.string().describe("answer to the user's question"),
    sources: z
      .array(z.string())
      .describe("sources used to answer the question, should be websites."),
  })
);

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Here is the output schema:
```
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"sources":{"type":"array","items":{"type":"string"},"description":"sources used to answer the question, should be websites."}},"required":["answer","sources"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "sources": ["https://en.wikipedia.org/wiki/Paris"]}
*/

console.log(await parser.parse(response));
/*
{ answer: 'Paris', sources: [ 'https://en.wikipedia.org/wiki/Paris' ] }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/how_to/use_with_llm_chain

# Use with LLMChains
For convenience, you can add an output parser to an LLMChain. This will automatically call .parse() on the output.
Don't forget to put the formatting instructions in the prompt!
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import {
  StructuredOutputParser,
  OutputFixingParser,
} from "langchain/output_parsers";

const outputParser = StructuredOutputParser.fromZodSchema(
  z
    .array(
      z.object({
        fields: z.object({
          Name: z.string().describe("The name of the country"),
          Capital: z.string().describe("The country's capital"),
        }),
      })
    )
    .describe("An array of Airtable records, each representing a country")
);

const chatModel = new ChatOpenAI({
  modelName: "gpt-4", // Or gpt-3.5-turbo
  temperature: 0, // For best results with the output fixing parser
});

const outputFixingParser = OutputFixingParser.fromLLM(chatModel, outputParser);

// Don't forget to include formatting instructions in the prompt!
const prompt = new PromptTemplate({
  template: `Answer the user's question as best you can:\n{format_instructions}\n{query}`,
  inputVariables: ["query"],
  partialVariables: {
    format_instructions: outputFixingParser.getFormatInstructions(),
  },
});

const answerFormattingChain = new LLMChain({
  llm: chatModel,
  prompt,
  outputKey: "records", // For readability - otherwise the chain output will default to a property named "text"
  outputParser: outputFixingParser,
});

const result = await answerFormattingChain.call({
  query: "List 5 countries.",
});

console.log(JSON.stringify(result.records, null, 2));

/*
[
  {
    "fields": {
      "Name": "United States",
      "Capital": "Washington, D.C."
    }
  },
  {
    "fields": {
      "Name": "Canada",
      "Capital": "Ottawa"
    }
  },
  {
    "fields": {
      "Name": "Germany",
      "Capital": "Berlin"
    }
  },
  {
    "fields": {
      "Name": "Japan",
      "Capital": "Tokyo"
    }
  },
  {
    "fields": {
      "Name": "Australia",
      "Capital": "Canberra"
    }
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/combining_output_parser

# Combining output parsers
Output parsers can be combined using CombiningOutputParser. This output parser takes in a list of output parsers, and will ask for (and parse) a combined output that contains all the fields of all the parsers.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import {
  StructuredOutputParser,
  RegexParser,
  CombiningOutputParser,
} from "langchain/output_parsers";

const answerParser = StructuredOutputParser.fromNamesAndDescriptions({
  answer: "answer to the user's question",
  source: "source used to answer the user's question, should be a website.",
});

const confidenceParser = new RegexParser(
  /Confidence: (A|B|C), Explanation: (.*)/,
  ["confidence", "explanation"],
  "noConfidence"
);

const parser = new CombiningOutputParser(answerParser, confidenceParser);
const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
Return the following outputs, each formatted as described below:

Output 1:
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Here is the output schema:
```
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

Output 2:
Your response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/

What is the capital of France?
*/

console.log(response);
/*
Output 1:
{"answer":"Paris","source":"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html"}

Output 2:
Confidence: A, Explanation: The capital of France is Paris.
*/

console.log(await parser.parse(response));
/*
{
  answer: 'Paris',
  source: 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html',
  confidence: 'A',
  explanation: 'The capital of France is Paris.'
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/comma_separated

# List parser
This output parser can be used when you want to return a list of comma-separated items.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { CommaSeparatedListOutputParser } from "langchain/output_parsers";

export const run = async () => {
  // With a `CommaSeparatedListOutputParser`, we can parse a comma separated list.
  const parser = new CommaSeparatedListOutputParser();

  const formatInstructions = parser.getFormatInstructions();

  const prompt = new PromptTemplate({
    template: "List five {subject}.\n{format_instructions}",
    inputVariables: ["subject"],
    partialVariables: { format_instructions: formatInstructions },
  });

  const model = new OpenAI({ temperature: 0 });

  const input = await prompt.format({ subject: "ice cream flavors" });
  const response = await model.call(input);

  console.log(input);
  /*
   List five ice cream flavors.
   Your response should be a list of comma separated values, eg: `foo, bar, baz`
  */

  console.log(response);
  // Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream

  console.log(await parser.parse(response));
  /*
  [
    'Vanilla',
    'Chocolate',
    'Strawberry',
    'Mint Chocolate Chip',
    'Cookies and Cream'
  ]
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/custom_list_parser

# Custom list parser
This output parser can be used when you want to return a list of items with a specific length and separator.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { CustomListOutputParser } from "langchain/output_parsers";

// With a `CustomListOutputParser`, we can parse a list with a specific length and separator.
const parser = new CustomListOutputParser({ length: 3, separator: "\n" });

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template: "Provide a list of {subject}.\n{format_instructions}",
  inputVariables: ["subject"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  subject: "great fiction books (book, author)",
});

const response = await model.call(input);

console.log(input);
/*
Provide a list of great fiction books (book, author).
Your response should be a list of 3 items separated by "\n" (eg: `foo\n bar\n baz`)
*/

console.log(response);
/*
The Catcher in the Rye, J.D. Salinger
To Kill a Mockingbird, Harper Lee
The Great Gatsby, F. Scott Fitzgerald
*/

console.log(await parser.parse(response));
/*
[
  'The Catcher in the Rye, J.D. Salinger',
  'To Kill a Mockingbird, Harper Lee',
  'The Great Gatsby, F. Scott Fitzgerald'
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser

# Auto-fixing parser
This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.
But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.
For this example, we'll use the structured output parser. Here's what happens if we pass it a result that does not comply with the schema:
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  StructuredOutputParser,
  OutputFixingParser,
} from "langchain/output_parsers";

export const run = async () => {
  const parser = StructuredOutputParser.fromZodSchema(
    z.object({
      answer: z.string().describe("answer to the user's question"),
      sources: z
        .array(z.string())
        .describe("sources used to answer the question, should be websites."),
    })
  );
  /** This is a bad output because sources is a string, not a list */
  const badOutput = `\`\`\`json
  {
    "answer": "foo",
    "sources": "foo.com"
  }
  \`\`\``;
  try {
    await parser.parse(badOutput);
  } catch (e) {
    console.log("Failed to parse bad output: ", e);
    /*
    Failed to parse bad output:  OutputParserException [Error]: Failed to parse. Text: ```json
      {
        "answer": "foo",
        "sources": "foo.com"
      }
      ```. Error: [
      {
        "code": "invalid_type",
        "expected": "array",
        "received": "string",
        "path": [
          "sources"
        ],
        "message": "Expected array, received string"
      }
    ]
    at StructuredOutputParser.parse (/Users/ankushgola/Code/langchainjs/langchain/src/output_parsers/structured.ts:71:13)
    at run (/Users/ankushgola/Code/langchainjs/examples/src/prompts/fix_parser.ts:25:18)
    at <anonymous> (/Users/ankushgola/Code/langchainjs/examples/src/index.ts:33:22)
   */
  }
  const fixParser = OutputFixingParser.fromLLM(
    new ChatOpenAI({ temperature: 0 }),
    parser
  );
  const output = await fixParser.parse(badOutput);
  console.log("Fixed output: ", output);
  // Fixed output:  { answer: 'foo', sources: [ 'foo.com' ] }
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/structured

# Structured output parser
This output parser can be used when you want to return multiple fields. If you want complex schema returned (i.e. a JSON object with arrays of strings), use the Zod Schema detailed below.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// With a `StructuredOutputParser` we can define a schema for the output.
const parser = StructuredOutputParser.fromNamesAndDescriptions({
  answer: "answer to the user's question",
  source: "source used to answer the user's question, should be a website.",
});

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
You must format your output as a JSON value that adheres to a given "JSON Schema" instance.

"JSON Schema" is a declarative language that allows you to annotate and validate JSON documents.

For example, the example "JSON Schema" instance {{"properties": {{"foo": {{"description": "a list of test words", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
would match an object with one required property, "foo". The "type" property specifies "foo" must be an "array", and the "description" property semantically describes it as "a list of test words". The items within "foo" must be strings.
Thus, the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of this example "JSON Schema". The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!

Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:
```json
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "source": "https://en.wikipedia.org/wiki/Paris"}
*/

console.log(await parser.parse(response));
// { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/Paris' }
```
#### API Reference:
## Structured Output Parser with Zod Schema​
This output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. z.date() is not allowed.
```typescript
import { z } from "zod";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.
const parser = StructuredOutputParser.fromZodSchema(
  z.object({
    answer: z.string().describe("answer to the user's question"),
    sources: z
      .array(z.string())
      .describe("sources used to answer the question, should be websites."),
  })
);

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Here is the output schema:
```
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"sources":{"type":"array","items":{"type":"string"},"description":"sources used to answer the question, should be websites."}},"required":["answer","sources"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "sources": ["https://en.wikipedia.org/wiki/Paris"]}
*/

console.log(await parser.parse(response));
/*
{ answer: 'Paris', sources: [ 'https://en.wikipedia.org/wiki/Paris' ] }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/

# Data connection
Many LLM applications require user-specific data that is not part of the model's training set. LangChain gives you the
building blocks to load, transform, store and query your data via:




Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/

# Document loaders
Use document loaders to load data from a source as Document's. A Document is a piece of text
and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text
contents of any web page, or even for loading a transcript of a YouTube video.
Document loaders expose a "load" method for loading data as documents from a configured source. They optionally
implement a "lazy load" as well for lazily loading data into memory.
## Get started​
The simplest loader reads in a file as text and places it all into one Document.
```typescript
import { TextLoader } from "langchain/document_loaders/fs/text";

const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/creating_documents

# Creating documents
A document at its core is fairly simple. It consists of a piece of text and optional metadata. The piece of text is what we interact with the language model, while the optional metadata is useful for keeping track of metadata about the document (such as the source).
```typescript
interface Document {
  pageContent: string;
  metadata: Record<string, any>;
}
```
You can create a document object rather easily in LangChain with:
```typescript
import { Document } from "langchain/document";

const doc = new Document({ pageContent: "foo" });
```
You can create one with metadata with:
```typescript
import { Document } from "langchain/document";

const doc = new Document({ pageContent: "foo", metadata: { source: "1" } });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/csv

# CSV
A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.
Load CSV data with a single row per document.
## Setup​
```typescript
npm install d3-dsv@2
```
## Usage, extracting all columns​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader("src/document_loaders/example_data/example.csv");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 1
text: This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 2
text: This is another sentence.",
  },
]
*/
```
## Usage, extracting a single column​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader(
  "src/document_loaders/example_data/example.csv",
  "text"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/custom

# Custom document loaders
If you want to implement your own Document Loader, you have a few options.
### Subclassing BaseDocumentLoader​
You can extend the BaseDocumentLoader class directly. The BaseDocumentLoader class provides a few convenience methods for loading documents from a variety of sources.
```typescript
abstract class BaseDocumentLoader implements DocumentLoader {
  abstract load(): Promise<Document[]>;
}
```
### Subclassing TextLoader​
If you want to load documents from a text file, you can extend the TextLoader class. The TextLoader class takes care of reading the file, so all you have to do is implement a parse method.
```typescript
abstract class TextLoader extends BaseDocumentLoader {
  abstract parse(raw: string): Promise<string[]>;
}
```
### Subclassing BufferLoader​
If you want to load documents from a binary file, you can extend the BufferLoader class. The BufferLoader class takes care of reading the file, so all you have to do is implement a parse method.
```typescript
abstract class BufferLoader extends BaseDocumentLoader {
  abstract parse(
    raw: Buffer,
    metadata: Document["metadata"]
  ): Promise<Document[]>;
}
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/file_directory

# File Directory
This covers how to load all documents in a directory.
The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.
Example folder:
```typescript
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
```
Example code:
```typescript
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new DirectoryLoader(
  "src/document_loaders/example_data/example",
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);
const docs = await loader.load();
console.log({ docs });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/json

# JSON
JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).
JSON Lines is a file format where each line is a valid JSON value.
The JSON loader uses JSON pointer to target keys in your JSON files you want to target.
### No JSON pointer example​
The most simple way of using it is to specify no JSON pointer.
The loader will load all strings it finds in the JSON object.
Example JSON file:
```typescript
{
  "texts": ["This is a sentence.", "This is another sentence."]
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader("src/document_loaders/example_data/example.json");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```
### Using JSON pointer example​
You can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.
In this example, we want to only extract information from "from" and "surname" entries.
```typescript
{
  "1": {
    "body": "BD 2023 SUMMER",
    "from": "LinkedIn Job",
    "labels": ["IMPORTANT", "CATEGORY_UPDATES", "INBOX"]
  },
  "2": {
    "body": "Intern, Treasury and other roles are available",
    "from": "LinkedIn Job2",
    "labels": ["IMPORTANT"],
    "other": {
      "name": "plop",
      "surname": "bob"
    }
  }
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader(
  "src/document_loaders/example_data/example.json",
  ["/from", "/surname"]
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "BD 2023 SUMMER",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "LinkedIn Job",
  },
  ...
]
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/pdf

# PDF
Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.
This covers how to load PDF documents into the Document format that we use downstream.
By default, one document will be created for each page in the PDF file. You can change this behavior by setting the splitPages option to false.
## Setup​
```typescript
npm install pdf-parse
```
## Usage, one document per page​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf");

const docs = await loader.load();
```
## Usage, one document per file​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  splitPages: false,
});

const docs = await loader.load();
```
## Usage, custom pdfjs build​
By default we use the pdfjs build bundled with pdf-parse, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of pdfjs-dist or if you want to use a custom build of pdfjs-dist, you can do so by providing a custom pdfjs function that returns a promise that resolves to the PDFJS object.
In the following example we use the "legacy" (see pdfjs docs) build of pdfjs-dist, which includes several polyfills not included in the default build.
```typescript
npm install pdfjs-dist
```
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  // you may need to add `.then(m => m.default)` to the end of the import
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/

# File Loaders
Only available on Node.js.
These loaders are used to load files given a filesystem path or a Blob object.
## 📄️ Folders with multiple files
This example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.
## 📄️ CSV files
This example goes over how to load data from CSV files. The second argument is the column name to extract from the CSV file. One document will be created for each row in the CSV file. When column is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's pageContent. When column is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.
## 📄️ Docx files
This example goes over how to load data from docx files.
## 📄️ EPUB files
This example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the splitChapters option to false.
## 📄️ JSON files
The JSON loader use JSON pointer to target keys in your JSON files you want to target.
## 📄️ JSONLines files
This example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.
## 📄️ Notion markdown export
This example goes over how to load data from your Notion pages exported from the notion dashboard.
## 📄️ PDF files
This example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the splitPages option to false.
## 📄️ Subtitles
This example goes over how to load data from subtitle files. One document will be created for each subtitles file.
## 📄️ Text files
This example goes over how to load data from text files.
## 📄️ Unstructured
This example covers how to use Unstructured to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/directory

# Folders with multiple files
This example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.
Example folder:
```typescript
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
```
Example code:
```typescript
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new DirectoryLoader(
  "src/document_loaders/example_data/example",
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);
const docs = await loader.load();
console.log({ docs });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/csv

# CSV files
This example goes over how to load data from CSV files. The second argument is the column name to extract from the CSV file. One document will be created for each row in the CSV file. When column is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's pageContent. When column is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.
## Setup​
```typescript
npm install d3-dsv@2
```
```typescript
yarn add d3-dsv@2
```
```typescript
pnpm add d3-dsv@2
```
## Usage, extracting all columns​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader("src/document_loaders/example_data/example.csv");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 1
text: This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 2
text: This is another sentence.",
  },
]
*/
```
## Usage, extracting a single column​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader(
  "src/document_loaders/example_data/example.csv",
  "text"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/docx

# Docx files
This example goes over how to load data from docx files.
# Setup
```typescript
npm install mammoth
```
```typescript
yarn add mammoth
```
```typescript
pnpm add mammoth
```
# Usage
```typescript
import { DocxLoader } from "langchain/document_loaders/fs/docx";

const loader = new DocxLoader(
  "src/document_loaders/tests/example_data/attention.docx"
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/epub

# EPUB files
This example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the splitChapters option to false.
# Setup
```typescript
npm install epub2 html-to-text
```
```typescript
yarn add epub2 html-to-text
```
```typescript
pnpm add epub2 html-to-text
```
# Usage, one document per chapter
```typescript
import { EPubLoader } from "langchain/document_loaders/fs/epub";

const loader = new EPubLoader("src/document_loaders/example_data/example.epub");

const docs = await loader.load();
```
# Usage, one document per file
```typescript
import { EPubLoader } from "langchain/document_loaders/fs/epub";

const loader = new EPubLoader(
  "src/document_loaders/example_data/example.epub",
  {
    splitChapters: false,
  }
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/json

# JSON files
The JSON loader use JSON pointer to target keys in your JSON files you want to target.
### No JSON pointer example​
The most simple way of using it, is to specify no JSON pointer.
The loader will load all strings it finds in the JSON object.
Example JSON file:
```typescript
{
  "texts": ["This is a sentence.", "This is another sentence."]
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader("src/document_loaders/example_data/example.json");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```
### Using JSON pointer example​
You can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.
In this example, we want to only extract information from "from" and "surname" entries.
```typescript
{
  "1": {
    "body": "BD 2023 SUMMER",
    "from": "LinkedIn Job",
    "labels": ["IMPORTANT", "CATEGORY_UPDATES", "INBOX"]
  },
  "2": {
    "body": "Intern, Treasury and other roles are available",
    "from": "LinkedIn Job2",
    "labels": ["IMPORTANT"],
    "other": {
      "name": "plop",
      "surname": "bob"
    }
  }
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader(
  "src/document_loaders/example_data/example.json",
  ["/from", "/surname"]
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "BD 2023 SUMMER",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "LinkedIn Job",
  },
  ...
]
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/jsonlines

# JSONLines files
This example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.
Example JSONLines file:
```typescript
{"html": "This is a sentence."}
{"html": "This is another sentence."}
```
Example code:
```typescript
import { JSONLinesLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLinesLoader(
  "src/document_loaders/example_data/example.jsonl",
  "/html"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/jsonl+json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/jsonl+json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/notion_markdown

# Notion markdown export
This example goes over how to load data from your Notion pages exported from the notion dashboard.
First, export your notion pages as Markdown & CSV as per the offical explanation here. Make sure to select include subpages and Create folders for subpages.
Then, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.
Once the folder is in your repository, simply run the example below:
```typescript
import { NotionLoader } from "langchain/document_loaders/fs/notion";

export const run = async () => {
  /** Provide the directory path of your notion folder */
  const directoryPath = "Notion_DB";
  const loader = new NotionLoader(directoryPath);
  const docs = await loader.load();
  console.log({ docs });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/pdf

# PDF files
This example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the splitPages option to false.
## Setup​
```typescript
npm install pdf-parse
```
```typescript
yarn add pdf-parse
```
```typescript
pnpm add pdf-parse
```
## Usage, one document per page​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf");

const docs = await loader.load();
```
## Usage, one document per file​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  splitPages: false,
});

const docs = await loader.load();
```
## Usage, custom pdfjs build​
By default we use the pdfjs build bundled with pdf-parse, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of pdfjs-dist or if you want to use a custom build of pdfjs-dist, you can do so by providing a custom pdfjs function that returns a promise that resolves to the PDFJS object.
In the following example we use the "legacy" (see pdfjs docs) build of pdfjs-dist, which includes several polyfills not included in the default build.
```typescript
npm install pdfjs-dist
```
```typescript
yarn add pdfjs-dist
```
```typescript
pnpm add pdfjs-dist
```
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  // you may need to add `.then(m => m.default)` to the end of the import
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/subtitles

# Subtitles
This example goes over how to load data from subtitle files. One document will be created for each subtitles file.
## Setup​
```typescript
npm install srt-parser-2
```
```typescript
yarn add srt-parser-2
```
```typescript
pnpm add srt-parser-2
```
## Usage​
```typescript
import { SRTLoader } from "langchain/document_loaders/fs/srt";

const loader = new SRTLoader(
  "src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/text

# Text files
This example goes over how to load data from text files.
```typescript
import { TextLoader } from "langchain/document_loaders/fs/text";

const loader = new TextLoader("src/document_loaders/example_data/example.txt");

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/unstructured

# Unstructured
This example covers how to use Unstructured to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.
## Setup​
You can run Unstructured locally in your computer using Docker. To do so, you need to have Docker installed. You can find the instructions to install Docker here.
```typescript
docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0
```
## Usage​
Once Unstructured is running, you can use it to load files from your computer. You can use the following code to load a file from your computer.
```typescript
import { UnstructuredLoader } from "langchain/document_loaders/fs/unstructured";

const options = {
  apiKey: "MY_API_KEY",
};

const loader = new UnstructuredLoader(
  "src/document_loaders/example_data/notion.md",
  options
);
const docs = await loader.load();
```
#### API Reference:
## Directories​
You can also load all of the files in the directory using UnstructuredDirectoryLoader, which inherits from DirectoryLoader:
```typescript
import { UnstructuredDirectoryLoader } from "langchain/document_loaders/fs/unstructured";

const options = {
  apiKey: "MY_API_KEY",
};

const loader = new UnstructuredDirectoryLoader(
  "langchain/src/document_loaders/tests/example_data",
  options
);
const docs = await loader.load();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/

# Web Loaders
These loaders are used to load web resources.
## 📄️ Cheerio
This example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.
## 📄️ Puppeteer
Only available on Node.js.
## 📄️ Playwright
Only available on Node.js.
## 📄️ Apify Dataset
This guide shows how to use Apify with LangChain to load documents from an Apify Dataset.
## 📄️ Azure Blob Storage Container
Only available on Node.js.
## 📄️ Azure Blob Storage File
Only available on Node.js.
## 📄️ College Confidential
This example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.
## 📄️ Confluence
Only available on Node.js.
## 📄️ Figma
This example goes over how to load data from a Figma file.
## 📄️ GitBook
This example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.
## 📄️ GitHub
This example goes over how to load data from a GitHub repository.
## 📄️ Hacker News
This example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.
## 📄️ IMSDB
This example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.
## 📄️ Notion API
This guide will take you through the steps required to load documents from Notion pages and databases using the Notion API.
## 📄️ S3 File
Only available on Node.js.
## 📄️ SerpAPI Loader
This guide shows how to use SerpAPI with LangChain to load web search results.
## 📄️ Sonix Audio
Only available on Node.js.
## 📄️ Blockchain Data
This example shows how to load blockchain data, including NFT metadata and transactions for a contract address, via the sort.xyz SQL API.



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/web_cheerio

# Webpages, with Cheerio
This example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.
Cheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio to extract data from web pages, without having to render them in a browser.
However, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the PlaywrightWebBaseLoader or PuppeteerWebBaseLoader instead.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";

const loader = new CheerioWebBaseLoader(
  "https://news.ycombinator.com/item?id=34817881"
);

const docs = await loader.load();
```
## Usage, with a custom selector​
```typescript
import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";

const loader = new CheerioWebBaseLoader(
  "https://news.ycombinator.com/item?id=34817881",
  {
    selector: "p.athing",
  }
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/web_puppeteer

# Webpages, with Puppeteer
Only available on Node.js.
This example goes over how to load data from webpages using Puppeteer. One document will be created for each webpage.
Puppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.
If you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the CheerioWebBaseLoader instead.
## Setup​
```typescript
npm install puppeteer
```
```typescript
yarn add puppeteer
```
```typescript
pnpm add puppeteer
```
## Usage​
```typescript
import { PuppeteerWebBaseLoader } from "langchain/document_loaders/web/puppeteer";

/**
 * Loader uses `page.evaluate(() => document.body.innerHTML)`
 * as default evaluate function
 **/
const loader = new PuppeteerWebBaseLoader("https://www.tabnews.com.br/");

const docs = await loader.load();
```
## Options​
Here's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:
```typescript
type PuppeteerWebBaseLoaderOptions = {
  launchOptions?: PuppeteerLaunchOptions;
  gotoOptions?: PuppeteerGotoOptions;
  evaluate?: (page: Page, browser: Browser) => Promise<string>;
};
```
launchOptions: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.
gotoOptions: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.
evaluate: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.
By passing these options to the PuppeteerWebBaseLoader constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.
Here is a basic example to do it:
```typescript
import { PuppeteerWebBaseLoader } from "langchain/document_loaders/web/puppeteer";

const loader = new PuppeteerWebBaseLoader("https://www.tabnews.com.br/", {
  launchOptions: {
    headless: true,
  },
  gotoOptions: {
    waitUntil: "domcontentloaded",
  },
  /** Pass custom evaluate, in this case you get page and browser instances */
  async evaluate(page: Page, browser: Browser) {
    await page.waitForResponse("https://www.tabnews.com.br/va/view");

    const result = await page.evaluate(() => document.body.innerHTML);
    return result;
  },
});

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/web_playwright

# Webpages, with Playwright
Only available on Node.js.
This example goes over how to load data from webpages using Playwright. One document will be created for each webpage.
Playwright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.
If you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the CheerioWebBaseLoader instead.
## Setup​
```typescript
npm install playwright
```
```typescript
yarn add playwright
```
```typescript
pnpm add playwright
```
## Usage​
```typescript
import { PlaywrightWebBaseLoader } from "langchain/document_loaders/web/playwright";

/**
 * Loader uses `page.content()`
 * as default evaluate function
 **/
const loader = new PlaywrightWebBaseLoader("https://www.tabnews.com.br/");

const docs = await loader.load();
```
## Options​
Here's an explanation of the parameters you can pass to the PlaywrightWebBaseLoader constructor using the PlaywrightWebBaseLoaderOptions interface:
```typescript
type PlaywrightWebBaseLoaderOptions = {
  launchOptions?: LaunchOptions;
  gotoOptions?: PlaywrightGotoOptions;
  evaluate?: PlaywrightEvaluate;
};
```
launchOptions: an optional object that specifies additional options to pass to the playwright.chromium.launch() method. This can include options such as the headless flag to launch the browser in headless mode.
gotoOptions: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.
evaluate: an optional function that can be used to evaluate JavaScript code on the page using a custom evaluation function. This can be useful for extracting data from the page, interacting with page elements, or handling specific HTTP responses. The function should return a Promise that resolves to a string containing the result of the evaluation.
By passing these options to the PlaywrightWebBaseLoader constructor, you can customize the behavior of the loader and use Playwright's powerful features to scrape and interact with web pages.
Here is a basic example to do it:
```typescript
import {
  PlaywrightWebBaseLoader,
  Page,
  Browser,
} from "langchain/document_loaders/web/playwright";

const url = "https://www.tabnews.com.br/";
const loader = new PlaywrightWebBaseLoader(url);
const docs = await loader.load();

// raw HTML page content
const extractedContents = docs[0].pageContent;
```
And a more advanced example:
```typescript
import {
  PlaywrightWebBaseLoader,
  Page,
  Browser,
} from "langchain/document_loaders/web/playwright";

const loader = new PlaywrightWebBaseLoader("https://www.tabnews.com.br/", {
  launchOptions: {
    headless: true,
  },
  gotoOptions: {
    waitUntil: "domcontentloaded",
  },
  /** Pass custom evaluate, in this case you get page and browser instances */
  async evaluate(page: Page, browser: Browser, response: Response | null) {
    await page.waitForResponse("https://www.tabnews.com.br/va/view");

    const result = await page.evaluate(() => document.body.innerHTML);
    return result;
  },
});

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/apify_dataset

# Apify Dataset
This guide shows how to use Apify with LangChain to load documents from an Apify Dataset.
## Overview​
Apify is a cloud platform for web scraping and data extraction,
which provides an ecosystem of more than a thousand
ready-made apps called Actors for various web scraping, crawling, and data extraction use cases.
This guide shows how to load documents
from an Apify Dataset — a scalable append-only
storage built for storing structured web scraping results,
such as a list of products or Google SERPs, and then export them to various
formats like JSON, CSV, or Excel.
Datasets are typically used to save results of Actors.
For example, Website Content Crawler Actor
deeply crawls websites such as documentation, knowledge bases, help centers, or blogs,
and then stores the text content of webpages into a dataset,
from which you can feed the documents into a vector index and answer questions from it.
## Setup​
You'll first need to install the official Apify client:
```typescript
npm install apify-client
```
```typescript
yarn add apify-client
```
```typescript
pnpm add apify-client
```
You'll also need to sign up and retrieve your Apify API token.
## Usage​
### From a New Dataset​
If you don't already have an existing dataset on the Apify platform, you'll need to initialize the document loader by calling an Actor and waiting for the results.
Note: Calling an Actor can take a significant amount of time, on the order of hours, or even days for large sites!
Here's an example:
```typescript
import { ApifyDatasetLoader } from "langchain/document_loaders/web/apify_dataset";
import { Document } from "langchain/document";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RetrievalQAChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";

/*
 * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
 * In the below example, the Apify dataset format looks like this:
 * {
 *   "url": "https://apify.com",
 *   "text": "Apify is the best web scraping and automation platform."
 * }
 */
const loader = await ApifyDatasetLoader.fromActorCall(
  "apify/website-content-crawler",
  {
    startUrls: [{ url: "https://js.langchain.com/docs/" }],
  },
  {
    datasetMappingFunction: (item) =>
      new Document({
        pageContent: (item.text || "") as string,
        metadata: { source: item.url },
      }),
    clientOptions: {
      token: "your-apify-token", // Or set as process.env.APIFY_API_TOKEN
    },
  }
);

const docs = await loader.load();

const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const model = new OpenAI({
  temperature: 0,
});

const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {
  returnSourceDocuments: true,
});
const res = await chain.call({ query: "What is LangChain?" });

console.log(res.text);
console.log(res.sourceDocuments.map((d: Document) => d.metadata.source));

/*
  LangChain is a framework for developing applications powered by language models.
  [
    'https://js.langchain.com/docs/',
    'https://js.langchain.com/docs/modules/chains/',
    'https://js.langchain.com/docs/modules/chains/llmchain/',
    'https://js.langchain.com/docs/category/functions-4'
  ]
*/
```
#### API Reference:
## From an Existing Dataset​
If you already have an existing dataset on the Apify platform, you can initialize the document loader with the constructor directly:
```typescript
import { ApifyDatasetLoader } from "langchain/document_loaders/web/apify_dataset";
import { Document } from "langchain/document";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RetrievalQAChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";

/*
 * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
 * In the below example, the Apify dataset format looks like this:
 * {
 *   "url": "https://apify.com",
 *   "text": "Apify is the best web scraping and automation platform."
 * }
 */
const loader = new ApifyDatasetLoader("your-dataset-id", {
  datasetMappingFunction: (item) =>
    new Document({
      pageContent: (item.text || "") as string,
      metadata: { source: item.url },
    }),
  clientOptions: {
    token: "your-apify-token", // Or set as process.env.APIFY_API_TOKEN
  },
});

const docs = await loader.load();

const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const model = new OpenAI({
  temperature: 0,
});

const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {
  returnSourceDocuments: true,
});
const res = await chain.call({ query: "What is LangChain?" });

console.log(res.text);
console.log(res.sourceDocuments.map((d: Document) => d.metadata.source));

/*
  LangChain is a framework for developing applications powered by language models.
  [
    'https://js.langchain.com/docs/',
    'https://js.langchain.com/docs/modules/chains/',
    'https://js.langchain.com/docs/modules/chains/llmchain/',
    'https://js.langchain.com/docs/category/functions-4'
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/azure_blob_storage_container

# Azure Blob Storage Container
Only available on Node.js.
This covers how to load document objects from a container on Azure Blob Storage.
## Setup​
To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.
See the docs here for information on how to do that.
You'll also need to install the official Azure Storage Blob client library:
```typescript
npm install @azure/storage-blob
```
```typescript
yarn add @azure/storage-blob
```
```typescript
pnpm add @azure/storage-blob
```
## Usage​
Once Unstructured is configured, you can use the Azure Blob Storage Container loader to load files and then convert them into a Document.
```typescript
import { AzureBlobStorageContainerLoader } from "langchain/document_loaders/web/azure_blob_storage_container";

const loader = new AzureBlobStorageContainerLoader({
  azureConfig: {
    connectionString: "",
    container: "container_name",
  },
  unstructuredConfig: {
    apiUrl: "http://localhost:8000/general/v0/general",
    apiKey: "", // this will be soon required
  },
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/azure_blob_storage_file

# Azure Blob Storage File
Only available on Node.js.
This covers how to load document objects from a Azure Files.
## Setup​
To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.
See the docs here for information on how to do that.
You'll also need to install the official Azure Storage Blob client library:
```typescript
npm install @azure/storage-blob
```
```typescript
yarn add @azure/storage-blob
```
```typescript
pnpm add @azure/storage-blob
```
## Usage​
Once Unstructured is configured, you can use the Azure Blob Storage File loader to load files and then convert them into a Document.
```typescript
import { AzureBlobStorageFileLoader } from "langchain/document_loaders/web/azure_blob_storage_file";

const loader = new AzureBlobStorageFileLoader({
  azureConfig: {
    connectionString: "",
    container: "container_name",
    blobName: "example.txt",
  },
  unstructuredConfig: {
    apiUrl: "http://localhost:8000/general/v0/general",
    apiKey: "", // this will be soon required
  },
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/college_confidential

# College Confidential
This example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { CollegeConfidentialLoader } from "langchain/document_loaders/web/college_confidential";

const loader = new CollegeConfidentialLoader(
  "https://www.collegeconfidential.com/colleges/brown-university/"
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/confluence

# Confluence
Only available on Node.js.
This covers how to load document objects from pages in a Confluence space.
## Credentials​
```typescript
npm install html-to-text
```
```typescript
yarn add html-to-text
```
```typescript
pnpm add html-to-text
```
## Usage​
```typescript
import { ConfluencePagesLoader } from "langchain/document_loaders/web/confluence";

const username = process.env.CONFLUENCE_USERNAME;
const accessToken = process.env.CONFLUENCE_ACCESS_TOKEN;

if (username && accessToken) {
  const loader = new ConfluencePagesLoader({
    baseUrl: "https://example.atlassian.net/wiki",
    spaceKey: "~EXAMPLE362906de5d343d49dcdbae5dEXAMPLE",
    username,
    accessToken,
  });

  const documents = await loader.load();
  console.log(documents);
} else {
  console.log(
    "You must provide a username and access token to run this example."
  );
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/figma

# Figma
This example goes over how to load data from a Figma file.
You will need a Figma access token in order to get started.
```typescript
import { FigmaFileLoader } from "langchain/document_loaders/web/figma";

const loader = new FigmaFileLoader({
  accessToken: "FIGMA_ACCESS_TOKEN", // or load it from process.env.FIGMA_ACCESS_TOKEN
  nodeIds: ["id1", "id2", "id3"],
  fileKey: "key",
});
const docs = await loader.load();

console.log({ docs });
```
#### API Reference:
You can find your Figma file's key and node ids by opening the file in your browser and extracting them from the URL:
```typescript
https://www.figma.com/file/<YOUR FILE KEY HERE>/LangChainJS-Test?type=whiteboard&node-id=<YOUR NODE ID HERE>&t=e6lqWkKecuYQRyRg-0
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/gitbook

# GitBook
This example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Load from single GitBook page​
```typescript
import { GitbookLoader } from "langchain/document_loaders/web/gitbook";

const loader = new GitbookLoader(
  "https://docs.gitbook.com/product-tour/navigation"
);

const docs = await loader.load();
```
## Load from all paths in a given GitBook​
For this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have shouldLoadAllPaths set to true.
```typescript
import { GitbookLoader } from "langchain/document_loaders/web/gitbook";

const loader = new GitbookLoader("https://docs.gitbook.com", {
  shouldLoadAllPaths: true,
});

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/github

# GitHub
This example goes over how to load data from a GitHub repository.
You can set the GITHUB_ACCESS_TOKEN environment variable to a GitHub access token to increase the rate limit and access private repositories.
## Setup​
The GitHub loader requires the ignore npm package as a peer dependency. Install it like this:
```typescript
npm install ignore
```
```typescript
yarn add ignore
```
```typescript
pnpm add ignore
```
## Usage​
```typescript
import { GithubRepoLoader } from "langchain/document_loaders/web/github";

export const run = async () => {
  const loader = new GithubRepoLoader(
    "https://github.com/hwchase17/langchainjs",
    { branch: "main", recursive: false, unknown: "warn" }
  );
  const docs = await loader.load();
  console.log({ docs });
};
```
#### API Reference:
The loader will ignore binary files like images.
### Using .gitignore syntax​
To ignore specific files, you can pass in an ignorePaths array into the constructor:
```typescript
import { GithubRepoLoader } from "langchain/document_loaders/web/github";

export const run = async () => {
  const loader = new GithubRepoLoader(
    "https://github.com/hwchase17/langchainjs",
    { branch: "main", recursive: false, unknown: "warn", ignorePaths: ["*.md"] }
  );
  const docs = await loader.load();
  console.log({ docs });
  // Will not include any .md files
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/hn

# Hacker News
This example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { HNLoader } from "langchain/document_loaders/web/hn";

const loader = new HNLoader("https://news.ycombinator.com/item?id=34817881");

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/imsdb

# IMSDB
This example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { IMSDBLoader } from "langchain/document_loaders/web/imsdb";

const loader = new IMSDBLoader("https://imsdb.com/scripts/BlacKkKlansman.html");

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/notionapi

# Notion API
This guide will take you through the steps required to load documents from Notion pages and databases using the Notion API.
## Overview​
Notion is a versatile productivity platform that consolidates note-taking, task management, and data organization tools into one interface.
This document loader is able to take full Notion pages and databases and turn them into a LangChain Documents ready to be integrated into your projects.
## Setup​
```typescript
npm install @notionhq/client notion-to-md
```
```typescript
yarn add @notionhq/client notion-to-md
```
```typescript
pnpm add @notionhq/client notion-to-md
```
The 32 char hex in the url path represents the ID. For example:
PAGE_ID: https://www.notion.so/skarard/LangChain-Notion-API-b34ca03f219c4420a6046fc4bdfdf7b4
DATABASE_ID: https://www.notion.so/skarard/c393f19c3903440da0d34bf9c6c12ff2?v=9c70a0f4e174498aa0f9021e0a9d52de
REGEX: /(?<!=)[0-9a-f]{32}/
## Example Usage​
```typescript
import { NotionAPILoader } from "langchain/document_loaders/web/notionapi";

// Loading a page (including child pages all as separate documents)
const pageLoader = new NotionAPILoader({
  clientOptions: {
    auth: "<NOTION_INTEGRATION_TOKEN>",
  },
  id: "<PAGE_ID>",
  type: "page",
});

// A page contents is likely to be more than 1000 characters so it's split into multiple documents (important for vectorization)
const pageDocs = await pageLoader.loadAndSplit();

console.log({ pageDocs });

// Loading a database (each row is a separate document with all properties as metadata)
const dbLoader = new NotionAPILoader({
  clientOptions: {
    auth: "<NOTION_INTEGRATION_TOKEN>",
  },
  id: "<DATABASE_ID>",
  type: "database",
});

// A database row contents is likely to be less than 1000 characters so it's not split into multiple documents
const dbDocs = await dbLoader.load();

console.log({ dbDocs });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/s3

# S3 File
Only available on Node.js.
This covers how to load document objects from an s3 file object.
## Setup​
To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.
See the docs here for information on how to do that.
You'll also need to install the official AWS SDK:
```typescript
npm install @aws-sdk/client-s3
```
```typescript
yarn add @aws-sdk/client-s3
```
```typescript
pnpm add @aws-sdk/client-s3
```
## Usage​
Once Unstructured is configured, you can use the S3 loader to load files and then convert them into a Document.
You can optionally provide a s3Config parameter to specify your bucket region, access key, and secret access key. If these are not provided, you will need to have them in your environment (e.g., by running aws configure).
```typescript
import { S3Loader } from "langchain/document_loaders/web/s3";

const loader = new S3Loader({
  bucket: "my-document-bucket-123",
  key: "AccountingOverview.pdf",
  s3Config: {
    region: "us-east-1",
    credentials: {
      accessKeyId: "AKIAIOSFODNN7EXAMPLE",
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY",
    },
  },
  unstructuredAPIURL: "http://localhost:8000/general/v0/general",
  unstructuredAPIKey: "", // this will be soon required
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/serpapi

# SerpAPI Loader
This guide shows how to use SerpAPI with LangChain to load web search results.
## Overview​
SerpAPI is a real-time API that provides access to search results from various search engines. It is commonly used for tasks like competitor analysis and rank tracking. It empowers businesses to scrape, extract, and make sense of data from all search engines' result pages.
This guide shows how to load web search results using the SerpAPILoader in LangChain. The SerpAPILoader simplifies the process of loading and processing web search results from SerpAPI.
## Setup​
You'll need to sign up and retrieve your SerpAPI API key.
## Usage​
Here's an example of how to use the SerpAPILoader:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SerpAPILoader } from "langchain/document_loaders/web/serpapi";

// Initialize the necessary components
const llm = new OpenAI();
const embeddings = new OpenAIEmbeddings();
const apiKey = "Your SerpAPI API key";

// Define your question and query
const question = "Your question here";
const query = "Your query here";

// Use SerpAPILoader to load web search results
const loader = new SerpAPILoader({ q: query, apiKey });
const docs = await loader.load();

// Use MemoryVectorStore to store the loaded documents in memory
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Use RetrievalQAChain to retrieve documents and answer the question
const chain = RetrievalQAChain.fromLLM(llm, vectorStore.asRetriever());
const answer = await chain.call({ query: question });

console.log(answer.text);
```
#### API Reference:
In this example, the SerpAPILoader is used to load web search results, which are then stored in memory using MemoryVectorStore. The RetrievalQAChain is then used to retrieve the most relevant documents from the memory and answer the question based on these documents. This demonstrates how the SerpAPILoader can streamline the process of loading and processing web search results.



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/sonix_audio_transcription

# Sonix Audio
Only available on Node.js.
This covers how to load document objects from an audio file using the Sonix API.
## Setup​
To run this loader you will need to create an account on the https://sonix.ai/ and obtain an auth key from the https://my.sonix.ai/api page.
You'll also need to install the sonix-speech-recognition library:
```typescript
npm install sonix-speech-recognition
```
```typescript
yarn add sonix-speech-recognition
```
```typescript
pnpm add sonix-speech-recognition
```
## Usage​
Once auth key is configured, you can use the loader to create transcriptions and then convert them into a Document.
In the request parameter, you can either specify a local file by setting audioFilePath or a remote file using audioUrl.
You will also need to specify the audio language. See the list of supported languages here.
```typescript
import { SonixAudioTranscriptionLoader } from "langchain/document_loaders/web/sonix_audio";

const loader = new SonixAudioTranscriptionLoader({
  sonixAuthKey: "SONIX_AUTH_KEY",
  request: {
    audioFilePath: "LOCAL_AUDIO_FILE_PATH",
    fileName: "FILE_NAME",
    language: "en",
  },
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/sort_xyz_blockchain

# Blockchain Data
This example shows how to load blockchain data, including NFT metadata and transactions for a contract address, via the sort.xyz SQL API.
You will need a free Sort API key, visiting sort.xyz to obtain one.
```typescript
import { SortXYZBlockchainLoader } from "langchain/document_loaders/web/sort_xyz_blockchain";
import { OpenAI } from "langchain/llms/openai";

/**
 * See https://docs.sort.xyz/docs/api-keys to get your free Sort API key.
 * See https://docs.sort.xyz for more information on the available queries.
 * See https://docs.sort.xyz/reference for more information about Sort's REST API.
 */

/**
 * Run the example.
 */
export const run = async () => {
  // Initialize the OpenAI model. Use OPENAI_API_KEY from .env in /examples
  const model = new OpenAI({ temperature: 0.9 });

  const apiKey = "YOUR_SORTXYZ_API_KEY";
  const contractAddress =
    "0x887F3909C14DAbd9e9510128cA6cBb448E932d7f".toLowerCase();

  /*
  Load NFT metadata from the Ethereum blockchain. Hint: to load by a specific ID, see SQL query example below.
  */

  const nftMetadataLoader = new SortXYZBlockchainLoader({
    apiKey,
    query: {
      type: "NFTMetadata",
      blockchain: "ethereum",
      contractAddress,
    },
  });

  const nftMetadataDocs = await nftMetadataLoader.load();

  const nftPrompt =
    "Describe the character with the attributes from the following json document in a 4 sentence story. ";
  const nftResponse = await model.call(
    nftPrompt + JSON.stringify(nftMetadataDocs[0], null, 2)
  );
  console.log(`user > ${nftPrompt}`);
  console.log(`chatgpt > ${nftResponse}`);

  /*
    Load the latest transactions for a contract address from the Ethereum blockchain.
  */
  const latestTransactionsLoader = new SortXYZBlockchainLoader({
    apiKey,
    query: {
      type: "latestTransactions",
      blockchain: "ethereum",
      contractAddress,
    },
  });

  const latestTransactionsDocs = await latestTransactionsLoader.load();

  const latestPrompt =
    "Describe the following json documents in only 4 sentences per document. Include as much detail as possible. ";
  const latestResponse = await model.call(
    latestPrompt + JSON.stringify(latestTransactionsDocs[0], null, 2)
  );
  console.log(`\n\nuser > ${nftPrompt}`);
  console.log(`chatgpt > ${latestResponse}`);

  /*
    Load metadata for a specific NFT by using raw SQL and the NFT index. See https://docs.sort.xyz for forumulating SQL.
  */

  const sqlQueryLoader = new SortXYZBlockchainLoader({
    apiKey,
    query: `SELECT * FROM ethereum.nft_metadata WHERE contract_address = '${contractAddress}' AND token_id = 1 LIMIT 1`,
  });

  const sqlDocs = await sqlQueryLoader.load();

  const sqlPrompt =
    "Describe the character with the attributes from the following json document in an ad for a new coffee shop. ";
  const sqlResponse = await model.call(
    sqlPrompt + JSON.stringify(sqlDocs[0], null, 2)
  );
  console.log(`\n\nuser > ${sqlPrompt}`);
  console.log(`chatgpt > ${sqlResponse}`);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/

# Document transformers
Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example
is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain
has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.
## Text splitters​
When you want to deal with long pieces of text, it is necessary to split up that text into chunks.
As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What "semantically related" means could depend on the type of text.
This notebook showcases several ways to do that.
At a high level, text splitters work as following:
That means there are two different axes along which you can customize your text splitter:
## Get started with text splitters​
The recommended TextSplitter is the RecursiveCharacterTextSplitter. This will split documents recursively by different characters - starting with "\n\n", then "\n", then " ". This is nice because it will try to keep all the semantically relevant content in the same place for as long as possible.
Important parameters to know here are chunkSize and chunkOverlap. chunkSize controls the max size (in terms of number of characters) of the final documents. chunkOverlap specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to 1000 and 200 respectively.
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const output = await splitter.createDocuments([text]);
```
You'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.
```typescript
import { Document } from "langchain/document";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/integrations/openai_metadata_tagger

# OpenAI functions metadata tagger
It can often be useful to tag ingested documents with structured metadata, such as the title, tone, or length of a document, to allow for more targeted similarity search later. However, for large numbers of documents, performing this labelling process manually can be tedious.
The MetadataTagger document transformer automates this process by extracting metadata from each provided document according to a provided schema. It uses a configurable OpenAI Functions-powered chain under the hood, so if you pass a custom LLM instance, it must be an OpenAI model with functions support.
Note: This document transformer works best with complete documents, so it's best to run it first with whole documents before doing any other splitting or processing!
### Usage​
For example, let's say you wanted to index a set of movie reviews. You could initialize the document transformer as follows:
```typescript
import { z } from "zod";
import { createMetadataTaggerFromZod } from "langchain/document_transformers/openai_functions";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Document } from "langchain/document";

const zodSchema = z.object({
  movie_title: z.string(),
  critic: z.string(),
  tone: z.enum(["positive", "negative"]),
  rating: z
    .optional(z.number())
    .describe("The number of stars the critic rated the movie"),
});

const metadataTagger = createMetadataTaggerFromZod(zodSchema, {
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo" }),
});

const documents = [
  new Document({
    pageContent:
      "Review of The Bee Movie\nBy Roger Ebert\nThis is the greatest movie ever made. 4 out of 5 stars.",
  }),
  new Document({
    pageContent:
      "Review of The Godfather\nBy Anonymous\n\nThis movie was super boring. 1 out of 5 stars.",
    metadata: { reliable: false },
  }),
];
const taggedDocuments = await metadataTagger.transformDocuments(documents);

console.log(taggedDocuments);

/*
  [
    Document {
      pageContent: 'Review of The Bee Movie\n' +
        'By Roger Ebert\n' +
        'This is the greatest movie ever made. 4 out of 5 stars.',
      metadata: {
        movie_title: 'The Bee Movie',
        critic: 'Roger Ebert',
        tone: 'positive',
        rating: 4
      }
    },
    Document {
      pageContent: 'Review of The Godfather\n' +
        'By Anonymous\n' +
        '\n' +
        'This movie was super boring. 1 out of 5 stars.',
      metadata: {
        movie_title: 'The Godfather',
        critic: 'Anonymous',
        tone: 'negative',
        rating: 1,
        reliable: false
      }
    }
  ]
*/
```
#### API Reference:
There is an additional createMetadataTagger method that accepts a valid JSON Schema object as well.
### Customization​
You can pass the underlying tagging chain the standard LLMChain arguments in the second options parameter.
For example, if you wanted to ask the LLM to focus specific details in the input documents, or extract metadata in a certain style, you could pass in a custom prompt:
```typescript
import { z } from "zod";
import { createMetadataTaggerFromZod } from "langchain/document_transformers/openai_functions";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Document } from "langchain/document";
import { PromptTemplate } from "langchain/prompts";

const taggingChainTemplate = `Extract the desired information from the following passage.
Anonymous critics are actually Roger Ebert.

Passage:
{input}
`;

const zodSchema = z.object({
  movie_title: z.string(),
  critic: z.string(),
  tone: z.enum(["positive", "negative"]),
  rating: z
    .optional(z.number())
    .describe("The number of stars the critic rated the movie"),
});

const metadataTagger = createMetadataTaggerFromZod(zodSchema, {
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo" }),
  prompt: PromptTemplate.fromTemplate(taggingChainTemplate),
});

const documents = [
  new Document({
    pageContent:
      "Review of The Bee Movie\nBy Roger Ebert\nThis is the greatest movie ever made. 4 out of 5 stars.",
  }),
  new Document({
    pageContent:
      "Review of The Godfather\nBy Anonymous\n\nThis movie was super boring. 1 out of 5 stars.",
    metadata: { reliable: false },
  }),
];
const taggedDocuments = await metadataTagger.transformDocuments(documents);

console.log(taggedDocuments);

/*
  [
    Document {
      pageContent: 'Review of The Bee Movie\n' +
        'By Roger Ebert\n' +
        'This is the greatest movie ever made. 4 out of 5 stars.',
      metadata: {
        movie_title: 'The Bee Movie',
        critic: 'Roger Ebert',
        tone: 'positive',
        rating: 4
      }
    },
    Document {
      pageContent: 'Review of The Godfather\n' +
        'By Anonymous\n' +
        '\n' +
        'This movie was super boring. 1 out of 5 stars.',
      metadata: {
        movie_title: 'The Godfather',
        critic: 'Roger Ebert',
        tone: 'negative',
        rating: 1,
        reliable: false
      }
    }
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter

# Split by character
This is the simplest method. This splits based on characters (by default "\n\n") and measure chunk length by number of characters.
# CharacterTextSplitter
Besides the RecursiveCharacterTextSplitter, there is also the more standard CharacterTextSplitter. This splits only on one type of character (defaults to "\n\n"). You can use it in the exact same way.
```typescript
import { Document } from "langchain/document";
import { CharacterTextSplitter } from "langchain/text_splitter";

const text = "foo bar baz 123";
const splitter = new CharacterTextSplitter({
  separator: " ",
  chunkSize: 7,
  chunkOverlap: 3,
});
const output = await splitter.createDocuments([text]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter

# Split code and markup
CodeTextSplitter allows you to split your code and markup with support for multiple languages.
LangChain supports a variety of different markup and programming language-specific text splitters to split your text based on language-specific syntax.
This results in more semantically self-contained chunks that are more useful to a vector store or other retriever.
Popular languages like JavaScript, Python, Solidity, and Rust are supported as well as Latex, HTML, and Markdown.
## Usage​
Initialize a standard RecursiveCharacterTextSplitter with the fromLanguage factory method. Below are some examples for various languages.
## JavaScript​
```typescript
import {
  SupportedTextSplitterLanguages,
  RecursiveCharacterTextSplitter,
} from "langchain/text_splitter";

console.log(SupportedTextSplitterLanguages); // Array of supported languages

/*
  [
    'cpp',      'go',
    'java',     'js',
    'php',      'proto',
    'python',   'rst',
    'ruby',     'rust',
    'scala',    'swift',
    'markdown', 'latex',
    'html'
  ]
*/

const jsCode = `function helloWorld() {
  console.log("Hello, World!");
}
// Call the function
helloWorld();`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("js", {
  chunkSize: 32,
  chunkOverlap: 0,
});
const jsOutput = await splitter.createDocuments([jsCode]);

console.log(jsOutput);

/*
  [
    Document {
      pageContent: 'function helloWorld() {',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'console.log("Hello, World!");',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '}\n// Call the function',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'helloWorld();',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:
## Python​
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const pythonCode = `def hello_world():
  print("Hello, World!")
# Call the function
hello_world()`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("python", {
  chunkSize: 32,
  chunkOverlap: 0,
});

const pythonOutput = await splitter.createDocuments([pythonCode]);

console.log(pythonOutput);

/*
  [
    Document {
      pageContent: 'def hello_world():',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'print("Hello, World!")',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '# Call the function',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'hello_world()',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:
## HTML​
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `<!DOCTYPE html>
<html>
  <head>
    <title>🦜️🔗 LangChain</title>
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      h1 {
        color: darkblue;
      }
    </style>
  </head>
  <body>
    <div>
      <h1>🦜️🔗 LangChain</h1>
      <p>⚡ Building applications with LLMs through composability ⚡</p>
    </div>
    <div>
      As an open source project in a rapidly developing field, we are extremely open to contributions.
    </div>
  </body>
</html>`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("html", {
  chunkSize: 175,
  chunkOverlap: 20,
});
const output = await splitter.createDocuments([text]);

console.log(output);

/*
  [
    Document {
      pageContent: '<!DOCTYPE html>\n<html>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<head>\n    <title>🦜️🔗 LangChain</title>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<style>\n' +
        '      body {\n' +
        '        font-family: Arial, sans-serif;\n' +
        '      }\n' +
        '      h1 {\n' +
        '        color: darkblue;\n' +
        '      }\n' +
        '    </style>\n' +
        '  </head>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<body>\n' +
        '    <div>\n' +
        '      <h1>🦜️🔗 LangChain</h1>\n' +
        '      <p>⚡ Building applications with LLMs through composability ⚡</p>\n' +
        '    </div>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<div>\n' +
        '      As an open source project in a rapidly developing field, we are extremely open to contributions.\n' +
        '    </div>\n' +
        '  </body>\n' +
        '</html>',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:
## Latex​
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `\\begin{document}
\\title{🦜️🔗 LangChain}
⚡ Building applications with LLMs through composability ⚡

\\section{Quick Install}

\\begin{verbatim}
Hopefully this code block isn't split
yarn add langchain
\\end{verbatim}

As an open source project in a rapidly developing field, we are extremely open to contributions.

\\end{document}`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("latex", {
  chunkSize: 100,
  chunkOverlap: 0,
});
const output = await splitter.createDocuments([text]);

console.log(output);

/*
  [
    Document {
      pageContent: '\\begin{document}\n' +
        '\\title{🦜️🔗 LangChain}\n' +
        '⚡ Building applications with LLMs through composability ⚡',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '\\section{Quick Install}',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '\\begin{verbatim}\n' +
        "Hopefully this code block isn't split\n" +
        'yarn add langchain\n' +
        '\\end{verbatim}',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'As an open source project in a rapidly developing field, we are extremely open to contributions.',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '\\end{document}',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/contextual_chunk_headers

# Contextual chunk headers
Consider a scenario where you want to store a large, arbitrary collection of documents in a vector store and perform Q&A tasks on them.
Simply splitting documents with overlapping text may not provide sufficient context for LLMs to determine if multiple chunks are referencing the same information, or how to resolve information from contradictory sources.
Tagging each document with metadata is a solution if you know what to filter against, but you may not know ahead of time exactly what kind of queries your vector store will be expected to handle.
Including additional contextual information directly in each chunk in the form of headers can help deal with arbitrary queries.
Here's an example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAStuffChain } from "langchain/chains";
import { CharacterTextSplitter } from "langchain/text_splitter";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { HNSWLib } from "langchain/vectorstores/hnswlib";

const splitter = new CharacterTextSplitter({
  chunkSize: 1536,
  chunkOverlap: 200,
});

const jimDocs = await splitter.createDocuments(
  [`My favorite color is blue.`],
  [],
  {
    chunkHeader: `DOCUMENT NAME: Jim Interview\n\n---\n\n`,
    appendChunkOverlapHeader: true,
  }
);

const pamDocs = await splitter.createDocuments(
  [`My favorite color is red.`],
  [],
  {
    chunkHeader: `DOCUMENT NAME: Pam Interview\n\n---\n\n`,
    appendChunkOverlapHeader: true,
  }
);

const vectorStore = await HNSWLib.fromDocuments(
  jimDocs.concat(pamDocs),
  new OpenAIEmbeddings()
);

const model = new OpenAI({ temperature: 0 });

const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAStuffChain(model),
  retriever: vectorStore.asRetriever(),
  returnSourceDocuments: true,
});
const res = await chain.call({
  query: "What is Pam's favorite color?",
});

console.log(JSON.stringify(res, null, 2));

/*
  {
    "text": " Red.",
    "sourceDocuments": [
      {
        "pageContent": "DOCUMENT NAME: Pam Interview\n\n---\n\nMy favorite color is red.",
        "metadata": {
          "loc": {
            "lines": {
              "from": 1,
              "to": 1
            }
          }
        }
      },
      {
        "pageContent": "DOCUMENT NAME: Jim Interview\n\n---\n\nMy favorite color is blue.",
        "metadata": {
          "loc": {
            "lines": {
              "from": 1,
              "to": 1
            }
          }
        }
      }
    ]
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/custom_text_splitter

# Custom text splitters
If you want to implement your own custom Text Splitter, you only need to subclass TextSplitter and implement a single method: splitText. The method takes a string and returns a list of strings. The returned strings will be used as the chunks.
```typescript
abstract class TextSplitter {
  abstract splitText(text: string): Promise<string[]>;
}
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter

# Recursively split by character
This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is ["\n\n", "\n", " ", ""]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.
Important parameters to know here are chunkSize and chunkOverlap. chunkSize controls the max size (in terms of number of characters) of the final documents. chunkOverlap specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to 1000 and 200 respectively.
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const output = await splitter.createDocuments([text]);
```
You'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.
```typescript
import { Document } from "langchain/document";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/token

# TokenTextSplitter
Finally, TokenTextSplitter splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.
```typescript
import { Document } from "langchain/document";
import { TokenTextSplitter } from "langchain/text_splitter";

const text = "foo bar baz 123";

const splitter = new TokenTextSplitter({
  encodingName: "gpt2",
  chunkSize: 10,
  chunkOverlap: 0,
});

const output = await splitter.createDocuments([text]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/

# Text embedding models
The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.
Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.
The base Embeddings class in LangChain exposes two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).
## Get started​
Embeddings can be used to create a numerical representation of textual data. This numerical representation is useful because it can be used to find similar documents.
Below is an example of how to use the OpenAI embeddings. Embeddings occasionally have different embedding methods for queries versus documents, so the embedding class exposes a embedQuery and embedDocuments method.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

/* Create instance */
const embeddings = new OpenAIEmbeddings();

/* Embed queries */
const res = await embeddings.embedQuery("Hello world");
/*
[
   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,
   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,
   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,
    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,
    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,
    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,
    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,
    0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,
      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,
    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,
   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,
    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,
   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,
    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,
  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,
  ... 1436 more items
]
*/

/* Embed documents */
const documentRes = await embeddings.embedDocuments(["Hello world", "Bye bye"]);
/*
[
  [
    -0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,
      0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,
      0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,
      0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,
      -0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,
    -0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,
      0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,
      -0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,
      0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,
      0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,
      0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,
      0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,
      0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,
      0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,
      0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,
      0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,
        0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,
      0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,
      0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,
    -0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,
    ... 1436 more items
  ],
  [
      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,
      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,
      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,
      -0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,
      -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,
      -0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,
        0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,
      -0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,
      0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,
      0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,
        0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,
      0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,
      0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,
      -0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,
      0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,
      0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,
      0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,
    -0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,
    0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,
      0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,
    ... 1436 more items
  ]
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/how_to/api_errors

# Dealing with API errors
If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a maxRetries option when you instantiate the model. For example:
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const model = new OpenAIEmbeddings({ maxRetries: 10 });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/how_to/rate_limits

# Dealing with rate limits
Some providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a maxConcurrency option when instantiating an Embeddings model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.
For example, if you set maxConcurrency: 5, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.
To use this feature, simply pass maxConcurrency: <number> when you instantiate the LLM. For example:
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const model = new OpenAIEmbeddings({ maxConcurrency: 5 });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a timeout option, in milliseconds, when you instantiate the model. For example, for OpenAI:
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings({
  timeout: 1000, // 1s timeout
});
/* Embed queries */
const res = await embeddings.embedQuery("Hello world");
console.log(res);
/* Embed documents */
const documentRes = await embeddings.embedDocuments(["Hello world", "Bye bye"]);
console.log({ documentRes });
```
#### API Reference:
Currently, the timeout option is only supported for OpenAI models.



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/azure_openai

# Azure OpenAI
The OpenAIEmbeddings class can also use the OpenAI API on Azure to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing stripNewLines: false to the constructor.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings({
  azureOpenAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/cohere

# Cohere
The CohereEmbeddings class uses the Cohere API to generate embeddings for a given text.
```typescript
npm install cohere-ai
```
```typescript
yarn add cohere-ai
```
```typescript
pnpm add cohere-ai
```
```typescript
import { CohereEmbeddings } from "langchain/embeddings/cohere";

const embeddings = new CohereEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.COHERE_API_KEY
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/google_palm

# Google PaLM
The Google PaLM API can be integrated by first
installing the required packages:
```typescript
npm install google-auth-library @google-ai/generativelanguage
```
```typescript
yarn add google-auth-library @google-ai/generativelanguage
```
```typescript
pnpm add google-auth-library @google-ai/generativelanguage
```
Create an API key from Google MakerSuite. You can then set
the key as GOOGLE_PALM_API_KEY environment variable or pass it as apiKey parameter while instantiating
the model.
```typescript
import { GooglePaLMEmbeddings } from "langchain/embeddings/googlepalm";

export const run = async () => {
  const model = new GooglePaLMEmbeddings({
    apiKey: "<YOUR API KEY>", // or set it in environment variable as `GOOGLE_PALM_API_KEY`
    modelName: "models/embedding-gecko-001", // OPTIONAL
  });
  /* Embed queries */
  const res = await model.embedQuery(
    "What would be a good company name for a company that makes colorful socks?"
  );
  console.log({ res });
  /* Embed documents */
  const documentRes = await model.embedDocuments(["Hello world", "Bye bye"]);
  console.log({ documentRes });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/google_vertex_ai

# Google Vertex AI
The GoogleVertexAIEmbeddings class uses Google's Vertex AI PaLM models
to generate embeddings for a given text.
The Vertex AI implementation is meant to be used in Node.js and not
directly in a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
```typescript
import { GoogleVertexAIEmbeddings } from "langchain/embeddings/googlevertexai";

export const run = async () => {
  const model = new GoogleVertexAIEmbeddings();
  const res = await model.embedQuery(
    "What would be a good company name for a company that makes colorful socks?"
  );
  console.log({ res });
};
```
#### API Reference:
Note: The default Google Vertex AI embeddings model, textembedding-gecko, has a different number of dimensions than OpenAI's text-embedding-ada-002 model
and may not be supported by all vector store providers.



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/hugging_face_inference

# HuggingFace Inference
This Embeddings integration uses the HuggingFace Inference API to generate embeddings for a given text using by default the sentence-transformers/distilbert-base-nli-mean-tokens model. You can pass a different model name to the constructor to use a different model.
```typescript
npm install @huggingface/inference@1
```
```typescript
yarn add @huggingface/inference@1
```
```typescript
pnpm add @huggingface/inference@1
```
```typescript
import { HuggingFaceInferenceEmbeddings } from "langchain/embeddings/hf";

const embeddings = new HuggingFaceInferenceEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/openai

# OpenAI
The OpenAIEmbeddings class uses the OpenAI API to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing stripNewLines: false to the constructor.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings({
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/tensorflow

# TensorFlow
This Embeddings integration runs the embeddings entirely in your browser or Node.js environment, using TensorFlow.js. This means that your data isn't sent to any third party, and you don't need to sign up for any API keys. However, it does require more memory and processing power than the other integrations.
```typescript
npm install @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
```
```typescript
yarn add @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
```
```typescript
pnpm add @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
```
```typescript
import "@tensorflow/tfjs-backend-cpu";
import { TensorFlowEmbeddings } from "langchain/embeddings/tensorflow";

const embeddings = new TensorFlowEmbeddings();
```
This example uses the CPU backend, which works in any JS environment. However, you can use any of the backends supported by TensorFlow.js, including GPU and WebAssembly, which will be a lot faster. For Node.js you can use the @tensorflow/tfjs-node package, and for the browser you can use the @tensorflow/tfjs-backend-webgl package. See the TensorFlow.js documentation for more information.



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/

# Vector stores
One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding
vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are
'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search
for you.
## Get started​
This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.
This walkthrough uses a basic, unoptimized implementation called MemoryVectorStore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings.
## Usage​
### Create a new index from texts​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MemoryVectorStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await MemoryVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const resultOne = await vectorStore.similaritySearch("hello world", 1);

console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
Here is the current base interface all vector stores share:
```typescript
interface VectorStore {
  /**
   * Add more documents to an existing VectorStore.
   * Some providers support additional parameters, e.g. to associate custom ids
   * with added documents or to change the batch size of bulk inserts.
   * Returns an array of ids for the documents or nothing.
   */
  addDocuments(
    documents: Document[],
    options?: Record<string, any>
  ): Promise<string[] | void>;

  /**
   * Search for the most similar documents to a query
   */
  similaritySearch(
    query: string,
    k?: number,
    filter?: object | undefined
  ): Promise<Document[]>;

  /**
   * Search for the most similar documents to a query,
   * and return their similarity score
   */
  similaritySearchWithScore(
    query: string,
    k = 4,
    filter: object | undefined = undefined
  ): Promise<[object, number][]>;

  /**
   * Turn a VectorStore into a Retriever
   */
  asRetriever(k?: number): BaseRetriever;

  /**
   * Delete embedded documents from the vector store matching the passed in parameter.
   * Not supported by every provider.
   */
  delete(params?: Record<string, any>): Promise<void>;

  /**
   * Advanced: Add more documents to an existing VectorStore,
   * when you already have their embeddings
   */
  addVectors(
    vectors: number[][],
    documents: Document[],
    options?: Record<string, any>
  ): Promise<string[] | void>;

  /**
   * Advanced: Search for the most similar documents to a query,
   * when you already have the embedding of the query
   */
  similaritySearchVectorWithScore(
    query: number[],
    k: number,
    filter?: object
  ): Promise<[Document, number][]>;
}
```
You can create a vector store from a list of Documents, or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.
```typescript
abstract class BaseVectorStore implements VectorStore {
  static fromTexts(
    texts: string[],
    metadatas: object[] | object,
    embeddings: Embeddings,
    dbConfig: Record<string, any>
  ): Promise<VectorStore>;

  static fromDocuments(
    docs: Document[],
    embeddings: Embeddings,
    dbConfig: Record<string, any>
  ): Promise<VectorStore>;
}
```
## Which one to pick?​
Here's a quick guide to help you pick the right vector store for your use case:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/

# Vector Stores: Integrations
## 📄️ Memory
MemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by ml-distance.
## 📄️ AnalyticDB
AnalyticDB for PostgreSQL is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.
## 📄️ Chroma
Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.
## 📄️ Elasticsearch
Only available on Node.js.
## 📄️ Faiss
Only available on Node.js.
## 📄️ HNSWLib
Only available on Node.js.
## 📄️ LanceDB
LanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.
## 📄️ Milvus
Milvus is a vector database built for embeddings similarity search and AI applications.
## 📄️ MongoDB Atlas
Only available on Node.js.
## 📄️ MyScale
Only available on Node.js.
## 📄️ OpenSearch
Only available on Node.js.
## 📄️ Pinecone
Only available on Node.js.
## 📄️ Prisma
For augmenting existing models in PostgreSQL database with vector search, Langchain supports using Prisma together with PostgreSQL and pgvector Postgres extension.
## 📄️ Qdrant
Qdrant is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.
## 📄️ Redis
Redis is a fast open source, in-memory data store.
## 📄️ SingleStore
SingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premise. It provides vector storage, as well as vector functions like dotproduct and euclideandistance, thereby supporting AI applications that require text similarity matching.
## 📄️ Supabase
Langchain supports using Supabase Postgres database as a vector store, using the pgvector postgres extension. Refer to the Supabase blog post for more information.
## 📄️ Tigris
Tigris makes it easy to build AI applications with vector embeddings.
## 📄️ TypeORM
To enable vector search in a generic PostgreSQL database, LangChainJS supports using TypeORM with the pgvector Postgres extension.
## 📄️ Typesense
Vector store that utilizes the Typesense search engine.
## 📄️ Vectara
Vectara is a developer-first API platform for easily building conversational search experiences.
## 📄️ Weaviate
Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/memory

# MemoryVectorStore
MemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by ml-distance.
## Usage​
### Create a new index from texts​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MemoryVectorStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await MemoryVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const resultOne = await vectorStore.similaritySearch("hello world", 1);

console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
### Use a custom similarity metric​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { similarity } from "ml-distance";

const vectorStore = await MemoryVectorStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings(),
  { similarity: similarity.pearson }
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/analyticdb

# AnalyticDB
AnalyticDB for PostgreSQL is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.
AnalyticDB for PostgreSQL is developed based on the open source Greenplum Database project and is enhanced with in-depth extensions by Alibaba Cloud. AnalyticDB for PostgreSQL is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems. AnalyticDB for PostgreSQL also supports row store and column store. AnalyticDB for PostgreSQL processes petabytes of data offline at a high performance level and supports highly concurrent online queries.
This notebook shows how to use functionality related to the AnalyticDB vector database.
To run, you should have an AnalyticDB instance up and running:
Only available on Node.js.
## Setup​
LangChain.js accepts node-postgres as the connections pool for AnalyticDB vectorstore.
```typescript
npm install -S pg
```
```typescript
yarn add pg
```
```typescript
pnpm add pg
```
And we need pg-copy-streams to add batch vectors quickly.
```typescript
npm install -S pg-copy-streams
```
```typescript
yarn add pg-copy-streams
```
```typescript
pnpm add pg-copy-streams
```
## Usage​
```typescript
import { AnalyticDBVectorStore } from "langchain/vectorstores/analyticdb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const connectionOptions = {
  host: process.env.ANALYTICDB_HOST || "localhost",
  port: Number(process.env.ANALYTICDB_PORT) || 5432,
  database: process.env.ANALYTICDB_DATABASE || "your_database",
  user: process.env.ANALYTICDB_USERNAME || "username",
  password: process.env.ANALYTICDB_PASSWORD || "password",
};

const vectorStore = await AnalyticDBVectorStore.fromTexts(
  ["foo", "bar", "baz"],
  [{ page: 1 }, { page: 2 }, { page: 3 }],
  new OpenAIEmbeddings(),
  { connectionOptions }
);
const result = await vectorStore.similaritySearch("foo", 1);
console.log(JSON.stringify(result));
// [{"pageContent":"foo","metadata":{"page":1}}]

await vectorStore.addDocuments([{ pageContent: "foo", metadata: { page: 4 } }]);

const filterResult = await vectorStore.similaritySearch("foo", 1, {
  page: 4,
});
console.log(JSON.stringify(filterResult));
// [{"pageContent":"foo","metadata":{"page":4}}]

const filterWithScoreResult = await vectorStore.similaritySearchWithScore(
  "foo",
  1,
  { page: 3 }
);
console.log(JSON.stringify(filterWithScoreResult));
// [[{"pageContent":"baz","metadata":{"page":3}},0.26075905561447144]]

const filterNoMatchResult = await vectorStore.similaritySearchWithScore(
  "foo",
  1,
  { page: 5 }
);
console.log(JSON.stringify(filterNoMatchResult));
// []

// need to manually close the Connection pool
await vectorStore.end();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/chroma

# Chroma
Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.
## Setup​
```typescript
git clone git@github.com:chroma-core/chroma.git
docker-compose up -d --build
```
```typescript
npm install -S chromadb
```
```typescript
yarn add chromadb
```
```typescript
pnpm add chromadb
```
Chroma is fully-typed, fully-tested and fully-documented.
Like any other database, you can:
View full docs at docs.
## Usage, Index and query Documents​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Create vector store and index the docs
const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {
  collectionName: "a-test-collection",
});

// Search for the most similar document
const response = await vectorStore.similaritySearch("hello", 1);

console.log(response);
/*
[
  Document {
    pageContent: 'Foo\nBar\nBaz\n\n',
    metadata: { source: 'src/document_loaders/example_data/example.txt' }
  }
]
*/
```
#### API Reference:
## Usage, Index and query texts​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// text sample from Godel, Escher, Bach
const vectorStore = await Chroma.fromTexts(
  [
    `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
        Harmonic Labyrinth of the dreaded Majotaur?`,
    "Achilles: Yiikes! What is that?",
    `Tortoise: They say-although I person never believed it myself-that an I
        Majotaur has created a tiny labyrinth sits in a pit in the middle of
        it, waiting innocent victims to get lost in its fears complexity.
        Then, when they wander and dazed into the center, he laughs and
        laughs at them-so hard, that he laughs them to death!`,
    "Achilles: Oh, no!",
    "Tortoise: But it's only a myth. Courage, Achilles.",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings(),
  {
    collectionName: "godel-escher-bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);

console.log(response);
/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/

// You can also filter by metadata
const filteredResponse = await vectorStore.similaritySearch("scared", 2, {
  id: 1,
});

console.log(filteredResponse);
/*
[
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:
## Usage, Query docs from existing collection​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await Chroma.fromExistingCollection(
  new OpenAIEmbeddings(),
  { collectionName: "godel-escher-bach" }
);

const response = await vectorStore.similaritySearch("scared", 2);
console.log(response);
/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:
## Usage, delete docs​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings();
const vectorStore = new Chroma(embeddings, {
  collectionName: "test-deletion",
});

const documents = [
  {
    pageContent: `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
    Harmonic Labyrinth of the dreaded Majotaur?`,
    metadata: {
      speaker: "Tortoise",
    },
  },
  {
    pageContent: "Achilles: Yiikes! What is that?",
    metadata: {
      speaker: "Achilles",
    },
  },
  {
    pageContent: `Tortoise: They say-although I person never believed it myself-that an I
    Majotaur has created a tiny labyrinth sits in a pit in the middle of
    it, waiting innocent victims to get lost in its fears complexity.
    Then, when they wander and dazed into the center, he laughs and
    laughs at them-so hard, that he laughs them to death!`,
    metadata: {
      speaker: "Tortoise",
    },
  },
  {
    pageContent: "Achilles: Oh, no!",
    metadata: {
      speaker: "Achilles",
    },
  },
  {
    pageContent: "Tortoise: But it's only a myth. Courage, Achilles.",
    metadata: {
      speaker: "Tortoise",
    },
  },
];

// Also supports an additional {ids: []} parameter for upsertion
const ids = await vectorStore.addDocuments(documents);

const response = await vectorStore.similaritySearch("scared", 2);
console.log(response);
/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/

// You can also pass a "filter" parameter instead
await vectorStore.delete({ ids });

const response2 = await vectorStore.similaritySearch("scared", 2);
console.log(response2);

/*
  []
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/elasticsearch

# Elasticsearch
Only available on Node.js.
Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. It supports also vector search using the k-nearest neighbor (kNN) algorithm and also custom models for Natural Language Processing (NLP).
You can read more about the support of vector search in Elasticsearch here.
LangChain.js accepts @elastic/elasticsearch as the client for Elasticsearch vectorstore.
## Setup​
```typescript
npm install -S @elastic/elasticsearch
```
```typescript
yarn add @elastic/elasticsearch
```
```typescript
pnpm add @elastic/elasticsearch
```
You'll also need to have an Elasticsearch instance running. You can use the official Docker image to get started, or you can use Elastic Cloud the official cloud service provided by Elastic.
For connecting to Elastic Cloud you can read the documentation reported here for obtaining an API key.
## Example: index docs, vector search and LLM integration​
Below is an example that indexes 4 documents in Elasticsearch,
runs a vector search query, and finally uses an LLM to answer a question in natural language
based on the retrieved documents.
```typescript
import { Client, ClientOptions } from "@elastic/elasticsearch";
import { Document } from "langchain/document";
import { OpenAI } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import {
  ElasticClientArgs,
  ElasticVectorSearch,
} from "langchain/vectorstores/elasticsearch";
import { VectorDBQAChain } from "langchain/chains";

// to run this first run Elastic's docker-container with `docker-compose up -d --build`
export async function run() {
  const config: ClientOptions = {
    node: process.env.ELASTIC_URL ?? "http://127.0.0.1:9200",
  };
  if (process.env.ELASTIC_API_KEY) {
    config.auth = {
      apiKey: process.env.ELASTIC_API_KEY,
    };
  } else if (process.env.ELASTIC_USERNAME && process.env.ELASTIC_PASSWORD) {
    config.auth = {
      username: process.env.ELASTIC_USERNAME,
      password: process.env.ELASTIC_PASSWORD,
    };
  }
  const clientArgs: ElasticClientArgs = {
    client: new Client(config),
    indexName: process.env.ELASTIC_INDEX ?? "test_vectorstore",
  };

  // Index documents

  const docs = [
    new Document({
      metadata: { foo: "bar" },
      pageContent: "Elasticsearch is a powerful vector db",
    }),
    new Document({
      metadata: { foo: "bar" },
      pageContent: "the quick brown fox jumped over the lazy dog",
    }),
    new Document({
      metadata: { baz: "qux" },
      pageContent: "lorem ipsum dolor sit amet",
    }),
    new Document({
      metadata: { baz: "qux" },
      pageContent:
        "Elasticsearch a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.",
    }),
  ];

  const embeddings = new OpenAIEmbeddings(undefined, {
    baseOptions: { temperature: 0 },
  });

  // await ElasticVectorSearch.fromDocuments(docs, embeddings, clientArgs);
  const vectorStore = new ElasticVectorSearch(embeddings, clientArgs);

  // Also supports an additional {ids: []} parameter for upsertion
  const ids = await vectorStore.addDocuments(docs);

  /* Search the vector DB independently with meta filters */
  const results = await vectorStore.similaritySearch("fox jump", 1);
  console.log(JSON.stringify(results, null, 2));
  /* [
        {
          "pageContent": "the quick brown fox jumped over the lazy dog",
          "metadata": {
            "foo": "bar"
          }
        }
    ]
  */

  /* Use as part of a chain (currently no metadata filters) for LLM query */
  const model = new OpenAI();
  const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
    k: 1,
    returnSourceDocuments: true,
  });
  const response = await chain.call({ query: "What is Elasticsearch?" });

  console.log(JSON.stringify(response, null, 2));
  /*
    {
      "text": " Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.",
      "sourceDocuments": [
        {
          "pageContent": "Elasticsearch a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.",
          "metadata": {
            "baz": "qux"
          }
        }
      ]
    }
    */

  await vectorStore.delete({ ids });

  const response2 = await chain.call({ query: "What is Elasticsearch?" });

  console.log(JSON.stringify(response2, null, 2));

  /*
    []
  */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss

# Faiss
Only available on Node.js.
Faiss is a library for efficient similarity search and clustering of dense vectors.
Langchainjs supports using Faiss as a vectorstore that can be saved to file. It also provides the ability to read the saved file from Python's implementation.
## Setup​
Install the faiss-node, which is a Node.js bindings for Faiss.
```typescript
npm install -S faiss-node
```
```typescript
yarn add faiss-node
```
```typescript
pnpm add faiss-node
```
To enable the ability to read the saved file from Python's implementation, the pickleparser also needs to install.
```typescript
npm install -S pickleparser
```
```typescript
yarn add pickleparser
```
```typescript
pnpm add pickleparser
```
## Usage​
### Create a new index from texts​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await FaissStore.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings()
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
};
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await FaissStore.fromDocuments(
  docs,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);
```
#### API Reference:
### Save an index to file and load it again​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// Create a vector store through any method, here from texts as an example
const vectorStore = await FaissStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

// Save the vector store to a directory
const directory = "your/directory/here";

await vectorStore.save(directory);

// Load the vector store from the same directory
const loadedVectorStore = await FaissStore.load(
  directory,
  new OpenAIEmbeddings()
);

// vectorStore and loadedVectorStore are identical
const result = await loadedVectorStore.similaritySearch("hello world", 1);
console.log(result);
```
#### API Reference:
### Load the saved file from Python's implementation​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// The directory of data saved from Python
const directory = "your/directory/here";

// Load the vector store from the directory
const loadedVectorStore = await FaissStore.loadFromPython(
  directory,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const result = await loadedVectorStore.similaritySearch("test", 2);
console.log("result", result);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/hnswlib

# HNSWLib
Only available on Node.js.
HNSWLib is an in-memory vectorstore that can be saved to a file. It uses HNSWLib.
## Setup​
On Windows, you might need to install Visual Studio first in order to properly build the hnswlib-node package.
You can install it with
```typescript
npm install hnswlib-node
```
```typescript
yarn add hnswlib-node
```
```typescript
pnpm add hnswlib-node
```
## Usage​
### Create a new index from texts​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await HNSWLib.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Search for the most similar document
const result = await vectorStore.similaritySearch("hello world", 1);
console.log(result);
```
#### API Reference:
### Save an index to a file and load it again​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// Create a vector store through any method, here from texts as an example
const vectorStore = await HNSWLib.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

// Save the vector store to a directory
const directory = "your/directory/here";
await vectorStore.save(directory);

// Load the vector store from the same directory
const loadedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());

// vectorStore and loadedVectorStore are identical

const result = await loadedVectorStore.similaritySearch("hello world", 1);
console.log(result);
```
#### API Reference:
### Filter documents​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await HNSWLib.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const result = await vectorStore.similaritySearch(
  "hello world",
  10,
  (document) => document.metadata.id === 3
);

// only "hello nice world" will be returned
console.log(result);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/lancedb

# LanceDB
LanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.
LanceDB datasets are persisted to disk and can be shared between Node.js and Python.
## Setup​
Install the LanceDB Node.js bindings:
```typescript
npm install -S vectordb
```
```typescript
yarn add vectordb
```
```typescript
pnpm add vectordb
```
## Usage​
### Create a new index from texts​
```typescript
import { LanceDB } from "langchain/vectorstores/lancedb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { connect } from "vectordb";
import * as fs from "node:fs/promises";
import * as path from "node:path";
import os from "node:os";

export const run = async () => {
  const dir = await fs.mkdtemp(path.join(os.tmpdir(), "lancedb-"));
  const db = await connect(dir);
  const table = await db.createTable("vectors", [
    { vector: Array(1536), text: "sample", id: 1 },
  ]);

  const vectorStore = await LanceDB.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings(),
    { table }
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
  // [ Document { pageContent: 'hello nice world', metadata: { id: 3 } } ]
};
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { LanceDB } from "langchain/vectorstores/lancedb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import fs from "node:fs/promises";
import path from "node:path";
import os from "node:os";
import { connect } from "vectordb";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

export const run = async () => {
  const dir = await fs.mkdtemp(path.join(os.tmpdir(), "lancedb-"));
  const db = await connect(dir);
  const table = await db.createTable("vectors", [
    { vector: Array(1536), text: "sample", source: "a" },
  ]);

  const vectorStore = await LanceDB.fromDocuments(
    docs,
    new OpenAIEmbeddings(),
    { table }
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);

  // [
  //   Document {
  //     pageContent: 'Foo\nBar\nBaz\n\n',
  //     metadata: { source: 'src/document_loaders/example_data/example.txt' }
  //   }
  // ]
};
```
#### API Reference:
### Open an existing dataset​
```typescript
import { LanceDB } from "langchain/vectorstores/lancedb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { connect } from "vectordb";
import * as fs from "node:fs/promises";
import * as path from "node:path";
import os from "node:os";

//
//  You can open a LanceDB dataset created elsewhere, such as LangChain Python, by opening
//     an existing table
//
export const run = async () => {
  const uri = await createdTestDb();
  const db = await connect(uri);
  const table = await db.openTable("vectors");

  const vectorStore = new LanceDB(new OpenAIEmbeddings(), { table });

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
  // [ Document { pageContent: 'Hello world', metadata: { id: 1 } } ]
};

async function createdTestDb(): Promise<string> {
  const dir = await fs.mkdtemp(path.join(os.tmpdir(), "lancedb-"));
  const db = await connect(dir);
  await db.createTable("vectors", [
    { vector: Array(1536), text: "Hello world", id: 1 },
    { vector: Array(1536), text: "Bye bye", id: 2 },
    { vector: Array(1536), text: "hello nice world", id: 3 },
  ]);
  return dir;
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/milvus

# Milvus
Milvus is a vector database built for embeddings similarity search and AI applications.
Only available on Node.js.
## Setup​
Run Milvus instance with Docker on your computer docs
Install the Milvus Node.js SDK.
```typescript
npm install -S @zilliz/milvus2-sdk-node
```
```typescript
yarn add @zilliz/milvus2-sdk-node
```
```typescript
pnpm add @zilliz/milvus2-sdk-node
```
Setup Env variables for Milvus before running the code
3.1 OpenAI
```typescript
export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530
```
3.2 Azure OpenAI
```typescript
export AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE
export AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE
export AZURE_OPENAI_BASE_PATH=YOUR_AZURE_OPENAI_BASE_PATH_HERE
export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530
```
## Index and query docs​
```typescript
import { Milvus } from "langchain/vectorstores/milvus";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// text sample from Godel, Escher, Bach
const vectorStore = await Milvus.fromTexts(
  [
    "Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\
            Harmonic Labyrinth of the dreaded Majotaur?",
    "Achilles: Yiikes! What is that?",
    "Tortoise: They say-although I person never believed it myself-that an I\
            Majotaur has created a tiny labyrinth sits in a pit in the middle of\
            it, waiting innocent victims to get lost in its fears complexity.\
            Then, when they wander and dazed into the center, he laughs and\
            laughs at them-so hard, that he laughs them to death!",
    "Achilles: Oh, no!",
    "Tortoise: But it's only a myth. Courage, Achilles.",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings(),
  {
    collectionName: "goldel_escher_bach",
  }
);

// or alternatively from docs
const vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {
  collectionName: "goldel_escher_bach",
});

const response = await vectorStore.similaritySearch("scared", 2);
```
## Query docs from existing collection​
```typescript
import { Milvus } from "langchain/vectorstores/milvus";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await Milvus.fromExistingCollection(
  new OpenAIEmbeddings(),
  {
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/mongodb_atlas

# MongoDB Atlas
Only available on Node.js.
Langchain supports MongoDB Atlas as a vector store.
## Setup​
### Installation​
First, add the Node MongoDB SDK to your project:
```typescript
npm install -S mongodb
```
```typescript
yarn add mongodb
```
```typescript
pnpm add mongodb
```
### Initial Cluster Configuration​
Next, you'll need create a MongoDB Atlas cluster. Navigate to the MongoDB Atlas website and create an account if you don't already have one.
Create and name a cluster when prompted, then find it under Database. Select Collections and create either a blank collection or one from the provided sample data.
### Creating an Index​
After configuring your cluster, you'll need to create an index on the collection field you want to search over.
Go to the Search tab within your cluster, then select Create Search Index. Using the JSON editor option, add an index to the collection you wish to use.
```typescript
{
  "mappings": {
    "fields": {
      // Default value, should match the name of the field within your collection that contains embeddings
      "embedding": [
        {
          "dimensions": 1024,
          "similarity": "euclidean",
          "type": "knnVector"
        }
      ]
    }
  }
}
```
The dimensions property should match the dimensionality of the embeddings you are using. For example, Cohere embeddings have 1024 dimensions, and OpenAI embeddings have 1536.
Note: By default the vector store expects an index name of default, an indexed collection field name of embedding, and a raw text field name of text. You should initialize the vector store with field names matching your collection schema as shown below.
Finally, proceed to build the index.
## Usage​
### Ingestion​
```typescript
import { MongoDBAtlasVectorSearch } from "langchain/vectorstores/mongodb_atlas";
import { CohereEmbeddings } from "langchain/embeddings/cohere";
import { MongoClient } from "mongodb";

export const run = async () => {
  const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
  const namespace = "langchain.test";
  const [dbName, collectionName] = namespace.split(".");
  const collection = client.db(dbName).collection(collectionName);

  await MongoDBAtlasVectorSearch.fromTexts(
    ["Hello world", "Bye bye", "What's this?"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new CohereEmbeddings(),
    {
      collection,
      indexName: "default", // The name of the Atlas search index. Defaults to "default"
      textKey: "text", // The name of the collection field containing the raw content. Defaults to "text"
      embeddingKey: "embedding", // The name of the collection field containing the embedded text. Defaults to "embedding"
    }
  );

  await client.close();
};
```
#### API Reference:
### Search​
```typescript
import { MongoDBAtlasVectorSearch } from "langchain/vectorstores/mongodb_atlas";
import { CohereEmbeddings } from "langchain/embeddings/cohere";
import { MongoClient } from "mongodb";

export const run = async () => {
  const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
  const namespace = "langchain.test";
  const [dbName, collectionName] = namespace.split(".");
  const collection = client.db(dbName).collection(collectionName);

  const vectorStore = new MongoDBAtlasVectorSearch(new CohereEmbeddings(), {
    collection,
    indexName: "default", // The name of the Atlas search index. Defaults to "default"
    textKey: "text", // The name of the collection field containing the raw content. Defaults to "text"
    embeddingKey: "embedding", // The name of the collection field containing the embedded text. Defaults to "embedding"
  });

  const resultOne = await vectorStore.similaritySearch("Hello world", 1);
  console.log(resultOne);

  await client.close();
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/myscale

# MyScale
Only available on Node.js.
MyScale is an emerging AI database that harmonizes the power of vector search and SQL analytics, providing a managed, efficient, and responsive experience.
## Setup​
```typescript
npm install -S @clickhouse/client
```
```typescript
yarn add @clickhouse/client
```
```typescript
pnpm add @clickhouse/client
```
## Index and Query Docs​
```typescript
import { MyScaleStore } from "langchain/vectorstores/myscale";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MyScaleStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [
    { id: 2, name: "2" },
    { id: 1, name: "1" },
    { id: 3, name: "3" },
  ],
  new OpenAIEmbeddings(),
  {
    host: process.env.MYSCALE_HOST || "localhost",
    port: process.env.MYSCALE_PORT || "8443",
    username: process.env.MYSCALE_USERNAME || "username",
    password: process.env.MYSCALE_PASSWORD || "password",
  }
);

const results = await vectorStore.similaritySearch("hello world", 1);
console.log(results);

const filteredResults = await vectorStore.similaritySearch("hello world", 1, {
  whereStr: "metadata.name = '1'",
});
console.log(filteredResults);
```
#### API Reference:
## Query Docs From an Existing Collection​
```typescript
import { MyScaleStore } from "langchain/vectorstores/myscale";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MyScaleStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  {
    host: process.env.MYSCALE_HOST || "localhost",
    port: process.env.MYSCALE_PORT || "8443",
    username: process.env.MYSCALE_USERNAME || "username",
    password: process.env.MYSCALE_PASSWORD || "password",
    database: "your_database", // defaults to "default"
    table: "your_table", // defaults to "vector_table"
  }
);

const results = await vectorStore.similaritySearch("hello world", 1);
console.log(results);

const filteredResults = await vectorStore.similaritySearch("hello world", 1, {
  whereStr: "metadata.name = '1'",
});
console.log(filteredResults);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/opensearch

# OpenSearch
Only available on Node.js.
OpenSearch is a fork of Elasticsearch that is fully compatible with the Elasticsearch API. Read more about their support for Approximate Nearest Neighbors here.
Langchain.js accepts @opensearch-project/opensearch as the client for OpenSearch vectorstore.
## Setup​
```typescript
npm install -S @opensearch-project/opensearch
```
```typescript
yarn add @opensearch-project/opensearch
```
```typescript
pnpm add @opensearch-project/opensearch
```
You'll also need to have an OpenSearch instance running. You can use the official Docker image to get started. You can also find an example docker-compose file here.
## Index docs​
```typescript
import { Client } from "@opensearch-project/opensearch";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { OpenSearchVectorStore } from "langchain/vectorstores/opensearch";

const client = new Client({
  nodes: [process.env.OPENSEARCH_URL ?? "http://127.0.0.1:9200"],
});

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "opensearch is also a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent:
      "OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications",
  }),
];

await OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {
  client,
  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`
});
```
## Query docs​
```typescript
import { Client } from "@opensearch-project/opensearch";
import { VectorDBQAChain } from "langchain/chains";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { OpenAI } from "langchain/llms/openai";
import { OpenSearchVectorStore } from "langchain/vectorstores/opensearch";

const client = new Client({
  nodes: [process.env.OPENSEARCH_URL ?? "http://127.0.0.1:9200"],
});

const vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {
  client,
});

/* Search the vector DB independently with meta filters */
const results = await vectorStore.similaritySearch("hello world", 1);
console.log(JSON.stringify(results, null, 2));
/* [
    {
      "pageContent": "Hello world",
      "metadata": {
        "id": 2
      }
    }
  ] */

/* Use as part of a chain (currently no metadata filters) */
const model = new OpenAI();
const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
  k: 1,
  returnSourceDocuments: true,
});
const response = await chain.call({ query: "What is opensearch?" });

console.log(JSON.stringify(response, null, 2));
/* 
  {
    "text": " Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.",
    "sourceDocuments": [
      {
        "pageContent": "What's this?",
        "metadata": {
          "id": 3
        }
      }
    ]
  } 
  */
```



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/pinecone

# Pinecone
Only available on Node.js.
LangChain.js accepts @pinecone-database/pinecone as the client for Pinecone vectorstore. Install the library with:
```typescript
npm install -S dotenv @pinecone-database/pinecone
```
```typescript
yarn add dotenv @pinecone-database/pinecone
```
```typescript
pnpm add dotenv @pinecone-database/pinecone
```
## Index docs​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";
import * as dotenv from "dotenv";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PineconeStore } from "langchain/vectorstores/pinecone";

dotenv.config();

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const pineconeIndex = client.Index(process.env.PINECONE_INDEX);

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "pinecone is a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "pinecones are the woody fruiting body and of a pine tree",
  }),
];

await PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {
  pineconeIndex,
});
```
## Query docs​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";
import * as dotenv from "dotenv";
import { VectorDBQAChain } from "langchain/chains";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { OpenAI } from "langchain/llms/openai";
import { PineconeStore } from "langchain/vectorstores/pinecone";

dotenv.config();

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const pineconeIndex = client.Index(process.env.PINECONE_INDEX);

const vectorStore = await PineconeStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  { pineconeIndex }
);

/* Search the vector DB independently with meta filters */
const results = await vectorStore.similaritySearch("pinecone", 1, {
  foo: "bar",
});
console.log(results);
/*
[
  Document {
    pageContent: 'pinecone is a vector db',
    metadata: { foo: 'bar' }
  }
]
*/

/* Use as part of a chain (currently no metadata filters) */
const model = new OpenAI();
const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
  k: 1,
  returnSourceDocuments: true,
});
const response = await chain.call({ query: "What is pinecone?" });
console.log(response);
/*
{
  text: ' A pinecone is the woody fruiting body of a pine tree.',
  sourceDocuments: [
    Document {
      pageContent: 'pinecones are the woody fruiting body and of a pine tree',
      metadata: [Object]
    }
  ]
}
*/
```
## Delete docs​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";
import * as dotenv from "dotenv";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PineconeStore } from "langchain/vectorstores/pinecone";

dotenv.config();

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const pineconeIndex = client.Index(process.env.PINECONE_INDEX);
const embeddings = new OpenAIEmbeddings();
const pineconeStore = new PineconeStore(embeddings, { pineconeIndex });

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "pinecone is a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "pinecones are the woody fruiting body and of a pine tree",
  }),
];

// Also takes an additional {ids: []} parameter for upsertion
const ids = await pineconeStore.addDocuments(docs);

const results = await pineconeStore.similaritySearch(pageContent, 2, {
  foo: "bar",
});

console.log(results);
/*
[
  Document {
    pageContent: 'pinecone is a vector db',
    metadata: { foo: 'bar' },
  },
  Document {
    pageContent: "the quick brown fox jumped over the lazy dog",
    metadata: { foo: "bar" },
  }
]
*/

await pineconeStore.delete({
  ids: [ids[0], ids[1]],
});

const results2 = await pineconeStore.similaritySearch(pageContent, 2, {
  foo: "bar",
});

console.log(results2);
/*
  []
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/prisma

# Prisma
For augmenting existing models in PostgreSQL database with vector search, Langchain supports using Prisma together with PostgreSQL and pgvector Postgres extension.
## Setup​
### Setup database instance with Supabase​
Refer to the Prisma and Supabase integration guide to setup a new database instance with Supabase and Prisma.
### Install Prisma​
```typescript
npm install prisma
```
```typescript
yarn add prisma
```
```typescript
pnpm add prisma
```
### Setup pgvector self hosted instance with docker-compose​
pgvector provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.
```typescript
services:
  db:
    image: ankane/pgvector
    ports:
      - 5432:5432
    volumes:
      - db:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=
      - POSTGRES_USER=
      - POSTGRES_DB=

volumes:
  db:
```
### Create a new schema​
Assuming you haven't created a schema yet, create a new model with a vector field of type Unsupported("vector"):
```typescript
model Document {
  id      String                 @id @default(cuid())
  content String
  vector  Unsupported("vector")?
}
```
Afterwards, create a new migration with --create-only to avoid running the migration directly.
```typescript
npx prisma migrate dev --create-only
```
```typescript
npx prisma migrate dev --create-only
```
```typescript
npx prisma migrate dev --create-only
```
Add the following line to the newly created migration to enable pgvector extension if it hasn't been enabled yet:
```typescript
CREATE EXTENSION IF NOT EXISTS vector;
```
Run the migration afterwards:
```typescript
npx prisma migrate dev
```
```typescript
npx prisma migrate dev
```
```typescript
npx prisma migrate dev
```
## Usage​
Table names and column names (in fields such as tableName, vectorColumnName, columns and filter) are passed into SQL queries directly without parametrisation.
These fields must be sanitized beforehand to avoid SQL injection.
```typescript
import { PrismaVectorStore } from "langchain/vectorstores/prisma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PrismaClient, Prisma, Document } from "@prisma/client";

export const run = async () => {
  const db = new PrismaClient();

  // Use the `withModel` method to get proper type hints for `metadata` field:
  const vectorStore = PrismaVectorStore.withModel<Document>(db).create(
    new OpenAIEmbeddings(),
    {
      prisma: Prisma,
      tableName: "Document",
      vectorColumnName: "vector",
      columns: {
        id: PrismaVectorStore.IdColumn,
        content: PrismaVectorStore.ContentColumn,
      },
    }
  );

  const texts = ["Hello world", "Bye bye", "What's this?"];
  await vectorStore.addModels(
    await db.$transaction(
      texts.map((content) => db.document.create({ data: { content } }))
    )
  );

  const resultOne = await vectorStore.similaritySearch("Hello world", 1);
  console.log(resultOne);

  // create an instance with default filter
  const vectorStore2 = PrismaVectorStore.withModel<Document>(db).create(
    new OpenAIEmbeddings(),
    {
      prisma: Prisma,
      tableName: "Document",
      vectorColumnName: "vector",
      columns: {
        id: PrismaVectorStore.IdColumn,
        content: PrismaVectorStore.ContentColumn,
      },
      filter: {
        content: {
          equals: "default",
        },
      },
    }
  );

  await vectorStore2.addModels(
    await db.$transaction(
      texts.map((content) => db.document.create({ data: { content } }))
    )
  );

  // Use the default filter a.k.a {"content": "default"}
  const resultTwo = await vectorStore.similaritySearch("Hello world", 1);
  console.log(resultTwo);

  // Override the local filter
  const resultThree = await vectorStore.similaritySearchWithScore(
    "Hello world",
    1,
    { content: { equals: "different_content" } }
  );
  console.log(resultThree);
};
```
#### API Reference:
The samples above uses the following schema:
```typescript
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Document {
  id        String                 @id @default(cuid())
  content   String
  namespace String?                @default("default")
  vector    Unsupported("vector")?
}
```
#### API Reference:
You can remove namespace if you don't need it.



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/qdrant

# Qdrant
Qdrant is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.
Only available on Node.js.
## Setup​
Run a Qdrant instance with Docker on your computer by following the Qdrant setup instructions.
Install the Qdrant Node.js SDK.
```typescript
npm install -S @qdrant/js-client-rest
```
```typescript
yarn add @qdrant/js-client-rest
```
```typescript
pnpm add @qdrant/js-client-rest
```
Setup Env variables for Qdrant before running the code
3.1 OpenAI
```typescript
export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
export QDRANT_URL=YOUR_QDRANT_URL_HERE # for example http://localhost:6333
```
3.2 Azure OpenAI
```typescript
export AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE
export AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE
export AZURE_OPENAI_BASE_PATH=YOUR_AZURE_OPENAI_BASE_PATH_HERE
export QDRANT_URL=YOUR_QDRANT_URL_HERE # for example http://localhost:6333
```
## Usage​
### Create a new index from texts​
```typescript
import { QdrantVectorStore } from "langchain/vectorstores/qdrant";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
// text sample from Godel, Escher, Bach
const vectorStore = await QdrantVectorStore.fromTexts(
  [
    `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
Harmonic Labyrinth of the dreaded Majotaur?`,
    `Achilles: Yiikes! What is that?`,
    `Tortoise: They say-although I person never believed it myself-that an I
            Majotaur has created a tiny labyrinth sits in a pit in the middle of
            it, waiting innocent victims to get lost in its fears complexity.
            Then, when they wander and dazed into the center, he laughs and
            laughs at them-so hard, that he laughs them to death!`,
    `Achilles: Oh, no!`,
    `Tortoise: But it's only a myth. Courage, Achilles.`,
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings(),
  {
    url: process.env.QDRANT_URL,
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);

console.log(response);

/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:
### Create a new index from docs​
```typescript
import { QdrantVectorStore } from "langchain/vectorstores/qdrant";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

const vectorStore = await QdrantVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings(),
  {
    url: process.env.QDRANT_URL,
    collectionName: "a_test_collection",
  }
);

// Search for the most similar document
const response = await vectorStore.similaritySearch("hello", 1);

console.log(response);
/*
[
  Document {
    pageContent: 'Foo\nBar\nBaz\n\n',
    metadata: { source: 'src/document_loaders/example_data/example.txt' }
  }
]
*/
```
#### API Reference:
### Query docs from existing collection​
```typescript
import { QdrantVectorStore } from "langchain/vectorstores/qdrant";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await QdrantVectorStore.fromExistingCollection(
  new OpenAIEmbeddings(),
  {
    url: process.env.QDRANT_URL,
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);

console.log(response);

/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/redis

# Redis
Redis is a fast open source, in-memory data store.
As part of the Redis Stack, RediSearch is the module that enables vector similarity semantic search, as well as many other types of searching.
Only available on Node.js.
LangChain.js accepts node-redis as the client for Redis vectorstore.
## Setup​
```typescript
npm install -S redis
```
```typescript
yarn add redis
```
```typescript
pnpm add redis
```
## Index docs​
```typescript
import { createClient, createCluster } from "redis";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RedisVectorStore } from "langchain/vectorstores/redis";

const client = createClient({
  url: process.env.REDIS_URL ?? "redis://localhost:6379",
});
await client.connect();

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "redis is fast",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "consectetur adipiscing elit",
  }),
];

const vectorStore = await RedisVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings(),
  {
    redisClient: client,
    indexName: "docs",
  }
);

await client.disconnect();
```
#### API Reference:
## Query docs​
```typescript
import { createClient } from "redis";
import { OpenAI } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RetrievalQAChain } from "langchain/chains";
import { RedisVectorStore } from "langchain/vectorstores/redis";

const client = createClient({
  url: process.env.REDIS_URL ?? "redis://localhost:6379",
});
await client.connect();

const vectorStore = new RedisVectorStore(new OpenAIEmbeddings(), {
  redisClient: client,
  indexName: "docs",
});

/* Simple standalone search in the vector DB */
const simpleRes = await vectorStore.similaritySearch("redis", 1);
console.log(simpleRes);
/*
[
  Document {
    pageContent: "redis is fast",
    metadata: { foo: "bar" }
  }
]
*/

/* Search in the vector DB using filters */
const filterRes = await vectorStore.similaritySearch("redis", 3, ["qux"]);
console.log(filterRes);
/*
[
  Document {
    pageContent: "consectetur adipiscing elit",
    metadata: { baz: "qux" },
  },
  Document {
    pageContent: "lorem ipsum dolor sit amet",
    metadata: { baz: "qux" },
  }
]
*/

/* Usage as part of a chain */
const model = new OpenAI();
const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(1), {
  returnSourceDocuments: true,
});
const chainRes = await chain.call({ query: "What did the fox do?" });
console.log(chainRes);
/*
{
  text: " The fox jumped over the lazy dog.",
  sourceDocuments: [
    Document {
      pageContent: "the quick brown fox jumped over the lazy dog",
      metadata: [Object]
    }
  ]
}
*/

await client.disconnect();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/singlestore

# SingleStore
SingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premise. It provides vector storage, as well as vector functions like dot_product and euclidean_distance, thereby supporting AI applications that require text similarity matching.
Only available on Node.js.
LangChain.js requires the mysql2 library to create a connection to a SingleStoreDB instance.
## Setup​
```typescript
npm install -S mysql2
```
```typescript
yarn add mysql2
```
```typescript
pnpm add mysql2
```
## Usage​
SingleStoreVectorStore manages a connection pool. It is recommended to call await store.end(); before terminating your application to assure all connections are appropriately closed and prevent any possible resource leaks.
### Standard usage​
Below is a straightforward example showcasing how to import the relevant module and perform a base similarity search using the SingleStoreVectorStore:
```typescript
import { SingleStoreVectorStore } from "langchain/vectorstores/singlestore";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await SingleStoreVectorStore.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings(),
    {
      connectionOptions: {
        host: process.env.SINGLESTORE_HOST,
        port: Number(process.env.SINGLESTORE_PORT),
        user: process.env.SINGLESTORE_USERNAME,
        password: process.env.SINGLESTORE_PASSWORD,
        database: process.env.SINGLESTORE_DATABASE,
      },
    }
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
  await vectorStore.end();
};
```
#### API Reference:
### Metadata Filtering​
If it is needed to filter results based on specific metadata fields, you can pass a filter parameter to narrow down your search to the documents that match all specified fields in the filter object:
```typescript
import { SingleStoreVectorStore } from "langchain/vectorstores/singlestore";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await SingleStoreVectorStore.fromTexts(
    ["Good afternoon", "Bye bye", "Boa tarde!", "Até logo!"],
    [
      { id: 1, language: "English" },
      { id: 2, language: "English" },
      { id: 3, language: "Portugese" },
      { id: 4, language: "Portugese" },
    ],
    new OpenAIEmbeddings(),
    {
      connectionOptions: {
        host: process.env.SINGLESTORE_HOST,
        port: Number(process.env.SINGLESTORE_PORT),
        user: process.env.SINGLESTORE_USERNAME,
        password: process.env.SINGLESTORE_PASSWORD,
        database: process.env.SINGLESTORE_DATABASE,
      },
      distanceMetric: "EUCLIDEAN_DISTANCE",
    }
  );

  const resultOne = await vectorStore.similaritySearch("greetings", 1, {
    language: "Portugese",
  });
  console.log(resultOne);
  await vectorStore.end();
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/supabase

# Supabase
Langchain supports using Supabase Postgres database as a vector store, using the pgvector postgres extension. Refer to the Supabase blog post for more information.
## Setup​
### Install the library with​
```typescript
npm install -S @supabase/supabase-js
```
```typescript
yarn add @supabase/supabase-js
```
```typescript
pnpm add @supabase/supabase-js
```
### Create a table and search function in your database​
Run this in your database:
```typescript
-- Enable the pgvector extension to work with embedding vectors
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);

-- Create a function to search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;
```
## Usage​
### Standard Usage​
The below example shows how to perform a basic similarity search with Supabase:
```typescript
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const vectorStore = await SupabaseVectorStore.fromTexts(
    ["Hello world", "Bye bye", "What's this?"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings(),
    {
      client,
      tableName: "documents",
      queryName: "match_documents",
    }
  );

  const resultOne = await vectorStore.similaritySearch("Hello world", 1);

  console.log(resultOne);
};
```
#### API Reference:
### Metadata Filtering​
Given the above match_documents Postgres function, you can also pass a filter parameter to only documents with a specific metadata field value. This filter parameter is a JSON object, and the match_documents function will use the Postgres JSONB Containment operator @> to filter documents by the metadata field values you specify. See details on the Postgres JSONB Containment operator for more information.
Note: If you've previously been using SupabaseVectorStore, you may need to drop and recreate the match_documents function per the updated SQL above to use this functionality.
```typescript
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const vectorStore = await SupabaseVectorStore.fromTexts(
    ["Hello world", "Hello world", "Hello world"],
    [{ user_id: 2 }, { user_id: 1 }, { user_id: 3 }],
    new OpenAIEmbeddings(),
    {
      client,
      tableName: "documents",
      queryName: "match_documents",
    }
  );

  const result = await vectorStore.similaritySearch("Hello world", 1, {
    user_id: 3,
  });

  console.log(result);
};
```
#### API Reference:
### Metadata Query Builder Filtering​
You can also use query builder-style filtering similar to how the Supabase JavaScript library works instead of passing an object. Note that since most of the filter properties are in the metadata column, you need to use arrow operators (-> for integer or ->> for text) as defined in Postgrest API documentation and specify the data type of the property (e.g. the column should look something like metadata->some_int_value::int).
```typescript
import {
  SupabaseFilterRPCCall,
  SupabaseVectorStore,
} from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const embeddings = new OpenAIEmbeddings();

  const store = new SupabaseVectorStore(embeddings, {
    client,
    tableName: "documents",
  });

  const docs = [
    {
      pageContent:
        "This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theorectical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle.",
      metadata: { b: 1, c: 10, stuff: "right" },
    },
    {
      pageContent:
        "This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore.",
      metadata: { b: 2, c: 9, stuff: "right" },
    },
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "right" } },
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "wrong" } },
    { pageContent: "hi", metadata: { b: 2, c: 8, stuff: "right" } },
    { pageContent: "bye", metadata: { b: 3, c: 7, stuff: "right" } },
    { pageContent: "what's this", metadata: { b: 4, c: 6, stuff: "right" } },
  ];

  // Also supports an additional {ids: []} parameter for upsertion
  await store.addDocuments(docs);

  const funcFilterA: SupabaseFilterRPCCall = (rpc) =>
    rpc
      .filter("metadata->b::int", "lt", 3)
      .filter("metadata->c::int", "gt", 7)
      .textSearch("content", `'multidimensional' & 'spaces'`, {
        config: "english",
      });

  const resultA = await store.similaritySearch("quantum", 4, funcFilterA);

  const funcFilterB: SupabaseFilterRPCCall = (rpc) =>
    rpc
      .filter("metadata->b::int", "lt", 3)
      .filter("metadata->c::int", "gt", 7)
      .filter("metadata->>stuff", "eq", "right");

  const resultB = await store.similaritySearch("hello", 2, funcFilterB);

  console.log(resultA, resultB);
};
```
#### API Reference:
### Document deletion​
```typescript
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const embeddings = new OpenAIEmbeddings();

  const store = new SupabaseVectorStore(embeddings, {
    client,
    tableName: "documents",
  });

  const docs = [
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "right" } },
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "wrong" } },
  ];

  // Also takes an additional {ids: []} parameter for upsertion
  const ids = await store.addDocuments(docs);

  const resultA = await store.similaritySearch("hello", 2);
  console.log(resultA);

  /*
    [
      Document { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "right" } },
      Document { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "wrong" } },
    ]
  */

  await store.delete({ ids });

  const resultB = await store.similaritySearch("hello", 2);
  console.log(resultB);

  /*
    []
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/tigris

# Tigris
Tigris makes it easy to build AI applications with vector embeddings.
It is a fully managed cloud-native database that allows you store and
index documents and vector embeddings for fast and scalable vector search.
Only available on Node.js.
## Setup​
### 1. Install the Tigris SDK​
Install the SDK as follows
```typescript
npm install -S @tigrisdata/vector
```
```typescript
yarn add @tigrisdata/vector
```
```typescript
pnpm add @tigrisdata/vector
```
### 2. Fetch Tigris API credentials​
You can sign up for a free Tigris account here.
Once you have signed up for the Tigris account, create a new project called vectordemo.
Next, make a note of the clientId and clientSecret, which you can get from the
Application Keys section of the project.
## Index docs​
```typescript
import { VectorDocumentStore } from "@tigrisdata/vector";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TigrisVectorStore } from "langchain/vectorstores/tigris";

const index = new VectorDocumentStore({
  connection: {
    serverUrl: "api.preview.tigrisdata.cloud",
    projectName: process.env.TIGRIS_PROJECT,
    clientId: process.env.TIGRIS_CLIENT_ID,
    clientSecret: process.env.TIGRIS_CLIENT_SECRET,
  },
  indexName: "examples_index",
  numDimensions: 1536, // match the OpenAI embedding size
});

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "tigris is a cloud-native vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "tigris is a river",
  }),
];

await TigrisVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), { index });
```
#### API Reference:
## Query docs​
```typescript
import { VectorDocumentStore } from "@tigrisdata/vector";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TigrisVectorStore } from "langchain/vectorstores/tigris";

const index = new VectorDocumentStore({
  connection: {
    serverUrl: "api.preview.tigrisdata.cloud",
    projectName: process.env.TIGRIS_PROJECT,
    clientId: process.env.TIGRIS_CLIENT_ID,
    clientSecret: process.env.TIGRIS_CLIENT_SECRET,
  },
  indexName: "examples_index",
  numDimensions: 1536, // match the OpenAI embedding size
});

const vectorStore = await TigrisVectorStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  { index }
);

/* Search the vector DB independently with metadata filters */
const results = await vectorStore.similaritySearch("tigris", 1, {
  "metadata.foo": "bar",
});
console.log(JSON.stringify(results, null, 2));
/*
[
  Document {
    pageContent: 'tigris is a cloud-native vector db',
    metadata: { foo: 'bar' }
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/typeorm

# TypeORM
To enable vector search in a generic PostgreSQL database, LangChainJS supports using TypeORM with the pgvector Postgres extension.
## Setup​
To work with TypeORM, you need to install the typeorm and pg packages:
```typescript
npm install typeorm
```
```typescript
yarn add typeorm
```
```typescript
pnpm add typeorm
```
```typescript
npm install pg
```
```typescript
yarn add pg
```
```typescript
pnpm add pg
```
### Setup a pgvector self hosted instance with docker-compose​
pgvector provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.
Create a file below named docker-compose.yml:
```typescript
services:
  db:
    image: ankane/pgvector
    ports:
      - 5432:5432
    volumes:
      - ./data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=ChangeMe
      - POSTGRES_USER=myuser
      - POSTGRES_DB=api
```
#### API Reference:
And then in the same directory, run docker compose up to start the container.
You can find more information on how to setup pgvector in the official repository.
## Usage​
One complete example of using TypeORMVectorStore is the following:
```typescript
import { DataSourceOptions } from "typeorm";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TypeORMVectorStore } from "langchain/vectorstores/typeorm";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/typeorm

export const run = async () => {
  const args = {
    postgresConnectionOptions: {
      type: "postgres",
      host: "localhost",
      port: 5432,
      username: "myuser",
      password: "ChangeMe",
      database: "api",
    } as DataSourceOptions,
  };

  const typeormVectorStore = await TypeORMVectorStore.fromDataSource(
    new OpenAIEmbeddings(),
    args
  );

  await typeormVectorStore.ensureTableInDatabase();

  await typeormVectorStore.addDocuments([
    { pageContent: "what's this", metadata: { a: 2 } },
    { pageContent: "Cat drinks milk", metadata: { a: 1 } },
  ]);

  const results = await typeormVectorStore.similaritySearch("hello", 2);

  console.log(results);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/typesense

# Typesense
Vector store that utilizes the Typesense search engine.
### Basic Usage​
```typescript
import { Typesense, TypesenseConfig } from "langchain/vectorstores/typesense";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { Client } from "typesense";
import { Document } from "langchain/document";

const vectorTypesenseClient = new Client({
  nodes: [
    {
      // Ideally should come from your .env file
      host: "...",
      port: 123,
      protocol: "https",
    },
  ],
  // Ideally should come from your .env file
  apiKey: "...",
  numRetries: 3,
  connectionTimeoutSeconds: 60,
});

const typesenseVectorStoreConfig = {
  // Typesense client
  typesenseClient: vectorTypesenseClient,
  // Name of the collection to store the vectors in
  schemaName: "your_schema_name",
  // Optional column names to be used in Typesense
  columnNames: {
    // "vec" is the default name for the vector column in Typesense but you can change it to whatever you want
    vector: "vec",
    // "text" is the default name for the text column in Typesense but you can change it to whatever you want
    pageContent: "text",
    // Names of the columns that you will save in your typesense schema and need to be retrieved as metadata when searching
    metadataColumnNames: ["foo", "bar", "baz"],
  },
  // Optional search parameters to be passed to Typesense when searching
  searchParams: {
    q: "*",
    filter_by: "foo:[fooo]",
    query_by: "",
  },
  // You can override the default Typesense import function if you want to do something more complex
  // Default import function:
  // async importToTypesense<
  //   T extends Record<string, unknown> = Record<string, unknown>
  // >(data: T[], collectionName: string) {
  //   const chunkSize = 2000;
  //   for (let i = 0; i < data.length; i += chunkSize) {
  //     const chunk = data.slice(i, i + chunkSize);

  //     await this.caller.call(async () => {
  //       await this.client
  //         .collections<T>(collectionName)
  //         .documents()
  //         .import(chunk, { action: "emplace", dirty_values: "drop" });
  //     });
  //   }
  // }
  import: async (data, collectionName) => {
    await vectorTypesenseClient
      .collections(collectionName)
      .documents()
      .import(data, { action: "emplace", dirty_values: "drop" });
  },
} satisfies TypesenseConfig;

/**
 * Creates a Typesense vector store from a list of documents.
 * Will update documents if there is a document with the same id, at least with the default import function.
 * @param documents list of documents to create the vector store from
 * @returns Typesense vector store
 */
const createVectorStoreWithTypesense = async (documents: Document[] = []) =>
  Typesense.fromDocuments(
    documents,
    new OpenAIEmbeddings(),
    typesenseVectorStoreConfig
  );

/**
 * Returns a Typesense vector store from an existing index.
 * @returns Typesense vector store
 */
const getVectorStoreWithTypesense = async () =>
  new Typesense(new OpenAIEmbeddings(), typesenseVectorStoreConfig);

// Do a similarity search
const vectorStore = await getVectorStoreWithTypesense();
const documents = await vectorStore.similaritySearch("hello world");

// Add filters based on metadata with the search parameters of Typesense
// will exclude documents with author:JK Rowling, so if Joe Rowling & JK Rowling exists, only Joe Rowling will be returned
vectorStore.similaritySearch("Rowling", undefined, {
  filter_by: "author:!=JK Rowling",
});

// Delete a document
vectorStore.deleteDocuments(["document_id_1", "document_id_2"]);
```
### Constructor​
Before starting, create a schema in Typesense with an id, a field for the vector and a field for the text. Add as many other fields as needed for the metadata.
### Methods​



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/vectara

# Vectara
Vectara is a developer-first API platform for easily building conversational search experiences.
You can use Vectara as a vector store with LangChain.js.
## 👉 Embeddings Included​
Vectara uses its own embeddings under the hood, so you don't have to provide any yourself or call another service to obtain embeddings.
This also means that if you provide your own embeddings, they'll be a no-op.
```typescript
const store = await VectaraStore.fromTexts(
  ["hello world", "hi there"],
  [{ foo: "bar" }, { foo: "baz" }],
  // This won't have an effect. Provide a FakeEmbeddings instance instead for clarity.
  new OpenAIEmbeddings(),
  args
);
```
## Setup​
You'll need to:
Configure your .env file or provide args to connect LangChain to your Vectara corpus:
```typescript
VECTARA_CUSTOMER_ID=your_customer_id
VECTARA_CORPUS_ID=your_corpus_id
VECTARA_API_KEY=your-vectara-api-key
```
## Usage​
```typescript
import { VectaraStore } from "langchain/vectorstores/vectara";
import { Document } from "langchain/document";

// Create the store.
const store = new VectaraStore({
  customerId: Number(process.env.VECTARA_CUSTOMER_ID),
  corpusId: Number(process.env.VECTARA_CORPUS_ID),
  apiKey: String(process.env.VECTARA_API_KEY),
  verbose: true,
});

// Store your data.
await store.addDocuments([
  new Document({
    pageContent: "Do I dare to eat a peach?",
    metadata: {
      foo: "baz",
    },
  }),
  new Document({
    pageContent: "In the room the women come and go talking of Michelangelo",
    metadata: {
      foo: "bar",
    },
  }),
]);

// "Added 2 documents to Vectara"

const resultsWithScore = await store.similaritySearchWithScore(
  "What were the women talking about?",
  1,
  {
    lambda: 0.025,
  }
);

console.log(JSON.stringify(resultsWithScore, null, 2));
// [
//   [
//     {
//       "pageContent": "In the room the women come and go talking of Michelangelo",
//       "metadata": [
//         {
//           "name": "lang",
//           "value": "eng"
//         },
//         {
//           "name": "offset",
//           "value": "0"
//         },
//         {
//           "name": "len",
//           "value": "57"
//         }
//       ]
//     },
//     0.38169062
//   ]
// ]
```
#### API Reference:
Note that lambda is a parameter related to Vectara's hybrid search capbility, providing a tradeoff between neural search and boolean/exact match as described here. We recommend the value of 0.025 as a default, while providing a way for advanced users to customize this value if needed.
## APIs​
Vectara's LangChain vector store consumes Vectara's core APIs:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate

# Weaviate
Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.
LangChain inserts vectors directly to Weaviate, and queries Weaviate for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Weaviate.
## Setup​
```typescript
npm install weaviate-ts-client graphql
```
```typescript
yarn add weaviate-ts-client graphql
```
```typescript
pnpm add weaviate-ts-client graphql
```
You'll need to run Weaviate either locally or on a server, see the Weaviate documentation for more information.
## Usage, insert documents​
```typescript
/* eslint-disable @typescript-eslint/no-explicit-any */
import weaviate from "weaviate-ts-client";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // Something wrong with the weaviate-ts-client types, so we need to disable
  const client = (weaviate as any).client({
    scheme: process.env.WEAVIATE_SCHEME || "https",
    host: process.env.WEAVIATE_HOST || "localhost",
    apiKey: new (weaviate as any).ApiKey(
      process.env.WEAVIATE_API_KEY || "default"
    ),
  });

  // Create a store and fill it with some texts + metadata
  await WeaviateStore.fromTexts(
    ["hello world", "hi there", "how are you", "bye now"],
    [{ foo: "bar" }, { foo: "baz" }, { foo: "qux" }, { foo: "bar" }],
    new OpenAIEmbeddings(),
    {
      client,
      indexName: "Test",
      textKey: "text",
      metadataKeys: ["foo"],
    }
  );
}
```
#### API Reference:
## Usage, query documents​
```typescript
/* eslint-disable @typescript-eslint/no-explicit-any */
import weaviate from "weaviate-ts-client";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // Something wrong with the weaviate-ts-client types, so we need to disable
  const client = (weaviate as any).client({
    scheme: process.env.WEAVIATE_SCHEME || "https",
    host: process.env.WEAVIATE_HOST || "localhost",
    apiKey: new (weaviate as any).ApiKey(
      process.env.WEAVIATE_API_KEY || "default"
    ),
  });

  // Create a store for an existing index
  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {
    client,
    indexName: "Test",
    metadataKeys: ["foo"],
  });

  // Search the index without any filters
  const results = await store.similaritySearch("hello world", 1);
  console.log(results);
  /*
  [ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]
  */

  // Search the index with a filter, in this case, only return results where
  // the "foo" metadata key is equal to "baz", see the Weaviate docs for more
  // https://weaviate.io/developers/weaviate/api/graphql/filters
  const results2 = await store.similaritySearch("hello world", 1, {
    where: {
      operator: "Equal",
      path: ["foo"],
      valueText: "baz",
    },
  });
  console.log(results2);
  /*
  [ Document { pageContent: 'hi there', metadata: { foo: 'baz' } } ]
  */
}
```
#### API Reference:
## Usage delete documents​
```typescript
/* eslint-disable @typescript-eslint/no-explicit-any */
import weaviate from "weaviate-ts-client";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // Something wrong with the weaviate-ts-client types, so we need to disable
  const client = (weaviate as any).client({
    scheme: process.env.WEAVIATE_SCHEME || "https",
    host: process.env.WEAVIATE_HOST || "localhost",
    apiKey: new (weaviate as any).ApiKey(
      process.env.WEAVIATE_API_KEY || "default"
    ),
  });

  // Create a store for an existing index
  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {
    client,
    indexName: "Test",
    metadataKeys: ["foo"],
  });

  const docs = [{ pageContent: "see ya!", metadata: { foo: "bar" } }];

  // Also supports an additional {ids: []} parameter for upsertion
  const ids = await store.addDocuments(docs);

  // Search the index without any filters
  const results = await store.similaritySearch("see ya!", 1);
  console.log(results);
  /*
  [ Document { pageContent: 'see ya!', metadata: { foo: 'bar' } } ]
  */

  await store.delete({ ids });

  const results2 = await store.similaritySearch("see ya!", 1);
  console.log(results2);
  /*
  []
  */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/

# Retrievers
A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) it. Vector stores can be used
as the backbone of a retriever, but there are other types of retrievers as well.
## Get started​
The public API of the BaseRetriever class in LangChain.js is as follows:
```typescript
export abstract class BaseRetriever {
  abstract getRelevantDocuments(query: string): Promise<Document[]>;
}
```
It's that simple! You can call getRelevantDocuments to retrieve documents relevant to a query, where "relevance" is defined by
the specific retriever object you are calling.
Of course, we also help construct what we think useful Retrievers are. The main type of Retriever in LangChain is a vector store retriever. We will focus on that here.
Note: Before reading, it's important to understand what a vector store is.
This example showcases question answering over documents.
We have chosen this as the example for getting started because it nicely combines a lot of different elements (Text splitters, embeddings, vectorstores) and then also shows how to use them in a chain.
Question answering over documents consists of four steps:
Each of the steps has multiple sub steps and potential configurations, but we'll go through one common flow using HNSWLib, a local vector store.
This assumes you're using Node, but you can swap in another integration if necessary.
First, install the required dependency:
```typescript
npm install -S hnswlib-node
```
```typescript
yarn add hnswlib-node
```
```typescript
pnpm add hnswlib-node
```
You can download the state_of_the_union.txt file here.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const chain = RetrievalQAChain.fromLLM(model, vectorStoreRetriever);
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
    and retiring Justice of the United States Supreme Court and thanked him for his service.'
  }
}
*/
```
#### API Reference:
Let's walk through what's happening here.
We first load a long text and split it into smaller documents using a text splitter.
We then load those documents (which also embeds the documents using the passed OpenAIEmbeddings instance) into HNSWLib, our vector store, creating our index.
Though we can query the vector store directly, we convert the vector store into a retriever to return retrieved documents in the right format for the question answering chain.
We initialize a RetrievalQAChain with the .fromLLM method, which we'll call later in step 4.
We ask questions!
See the individual sections for deeper dives on specific retrievers.



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/contextual_compression/

# Contextual compression
One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.
Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.
To use the Contextual Compression Retriever, you'll need:
The Contextual Compression Retriever passes queries to the base Retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of Documents and shortens it by reducing the contents of Documents or dropping Documents altogether.

## Get started​
Here's an example of how this works:
```typescript
import * as fs from "fs";

import { OpenAI } from "langchain/llms/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { ContextualCompressionRetriever } from "langchain/retrievers/contextual_compression";
import { LLMChainExtractor } from "langchain/retrievers/document_compressors/chain_extract";

const model = new OpenAI();
const baseCompressor = LLMChainExtractor.fromLLM(model);

const text = fs.readFileSync("state_of_the_union.txt", "utf8");

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const retriever = new ContextualCompressionRetriever({
  baseCompressor,
  baseRetriever: vectorStore.asRetriever(),
});

const chain = RetrievalQAChain.fromLLM(model, retriever);

const res = await chain.call({
  query: "What did the speaker say about Justice Breyer?",
});

console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/

# Self-querying
A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.

All Self Query retrievers require peggy as a peer dependency:
```typescript
npm install -S peggy
```
```typescript
yarn add peggy
```
```typescript
pnpm add peggy
```
Here's a basic example with an in-memory, unoptimized vector store:
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "langchain/retrievers/self_query/functional";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/chroma-self-query

# Chroma Self Query Retriever
This example shows how to use a self query retriever with a Chroma vector store.
## Usage​
```typescript
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { ChromaTranslator } from "langchain/retrievers/self_query/chroma";
import { OpenAI } from "langchain/llms/openai";
import { Chroma } from "langchain/vectorstores/chroma";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await Chroma.fromDocuments(docs, embeddings, {
  collectionName: "a-movie-collection",
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new ChromaTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/hnswlib-self-query

# HNSWLib Self Query Retriever
This example shows how to use a self query retriever with an HNSWLib vector store.
## Usage​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "langchain/retrievers/self_query/functional";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await HNSWLib.fromDocuments(docs, embeddings);
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/memory-self-query

# Memory Vector Store Self Query Retriever
This example shows how to use a self query retriever with a basic, in-memory vector store.
## Usage​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "langchain/retrievers/self_query/functional";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/pinecone-self-query

# Pinecone Self Query Retriever
This example shows how to use a self query retriever with a Pinecone vector store.
## Usage​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";

import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { PineconeTranslator } from "langchain/retrievers/self_query/pinecone";
import { PineconeStore } from "langchain/vectorstores/pinecone";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
if (
  !process.env.PINECONE_API_KEY ||
  !process.env.PINECONE_ENVIRONMENT ||
  !process.env.PINECONE_INDEX
) {
  throw new Error(
    "PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set"
  );
}

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const index = client.Index(process.env.PINECONE_INDEX);

const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await PineconeStore.fromDocuments(docs, embeddings, {
  pineconeIndex: index,
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new PineconeTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/supabase-self-query

# Supabase Self Query Retriever
This example shows how to use a self query retriever with a Supabase vector store.
If you haven't already set up Supabase, please follow the instructions here.
## Usage​
```typescript
import { createClient } from "@supabase/supabase-js";

import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { SupabaseTranslator } from "langchain/retrievers/self_query/supabase";
import { OpenAI } from "langchain/llms/openai";
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 */
if (!process.env.SUPABASE_URL || !process.env.SUPABASE_PRIVATE_KEY) {
  throw new Error(
    "Supabase URL or private key not set. Please set it in the .env file"
  );
}

const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const client = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_PRIVATE_KEY
);
const vectorStore = await SupabaseVectorStore.fromDocuments(docs, embeddings, {
  client,
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. LangChain provides one here.
   */
  structuredQueryTranslator: new SupabaseTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/weaviate-self-query

# Weaviate Self Query Retriever
This example shows how to use a self query retriever with a Weaviate vector store.
If you haven't already set up Weaviate, please follow the instructions here.
## Usage​
This example shows how to intialize a SelfQueryRetriever with a vector store:
```typescript
import weaviate from "weaviate-ts-client";

import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { OpenAI } from "langchain/llms/openai";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { WeaviateTranslator } from "langchain/retrievers/self_query/weaviate";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
// eslint-disable-next-line @typescript-eslint/no-explicit-any
const client = (weaviate as any).client({
  scheme: process.env.WEAVIATE_SCHEME || "https",
  host: process.env.WEAVIATE_HOST || "localhost",
  apiKey: process.env.WEAVIATE_API_KEY
    ? // eslint-disable-next-line @typescript-eslint/no-explicit-any
      new (weaviate as any).ApiKey(process.env.WEAVIATE_API_KEY)
    : undefined,
});

const vectorStore = await WeaviateStore.fromDocuments(docs, embeddings, {
  client,
  indexName: "Test",
  textKey: "text",
  metadataKeys: ["year", "director", "rating", "genre"],
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. LangChain provides one here.
   */
  structuredQueryTranslator: new WeaviateTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 *
 * Note that unlike other vector stores, you have to make sure each metadata keys are actually presnt in the database,
 * meaning that Weaviate will throw an error if the self query chain generate a query with a metadata key that does
 * not exist in your Weaviate database.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
console.log(query1, query2);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/time_weighted_vectorstore

# Time-weighted vector store retriever
This retriever uses a combination of semantic similarity and a time decay.
The algorithm for scoring them is:
```typescript
semantic_similarity + (1.0 - decay_rate) ^ hours_passed
```
Notably, hours_passed refers to the hours passed since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain "fresh."
```typescript
let score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;
```
this.decayRate is a configurable decimal number between 0 and 1. A lower number means that documents will be "remembered" for longer, while a higher number strongly weights more recently accessed documents.
Note that setting a decay rate of exactly 0 or 1 makes hoursPassed irrelevant and makes this retriever equivalent to a standard vector lookup.
## Usage​
This example shows how to intialize a TimeWeightedVectorStoreRetriever with a vector store.
It is important to note that due to required metadata, all documents must be added to the backing vector store using the addDocuments method on the retriever, not the vector store itself.
```typescript
import { TimeWeightedVectorStoreRetriever } from "langchain/retrievers/time_weighted";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());

const retriever = new TimeWeightedVectorStoreRetriever({
  vectorStore,
  memoryStream: [],
  searchKwargs: 2,
});

const documents = [
  "My name is John.",
  "My name is Bob.",
  "My favourite food is pizza.",
  "My favourite food is pasta.",
  "My favourite food is sushi.",
].map((pageContent) => ({ pageContent, metadata: {} }));

// All documents must be added using this method on the retriever (not the vector store!)
// so that the correct access history metadata is populated
await retriever.addDocuments(documents);

const results1 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results1);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */

const results2 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results2);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/vectorstore

# Vector store-backed retriever
A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the Vector Store class to make it conform to the Retriever interface.
It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.
Once you construct a Vector store, it's very easy to construct a retriever. Let's walk through an example.
```typescript
const vectorStore = ...
const retriever = vectorStore.asRetriever();
```
Here's a more end-to-end example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

const docs = retriever.getRelevantDocuments("what did he say about ketanji brown jackson");
```
## Configuration​
You can specify a maximum number of documents to retrieve as well as a vector store-specific filter to use when retrieving.
```typescript
// Return up to 2 documents with `metadataField` set to `"value"`
const retriever = vectorStore.asRetriever(2, { metadataField: "value" });

const docs = retriever.getRelevantDocuments("what did he say about ketanji brown jackson");
```



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/chatgpt-retriever-plugin

# ChatGPT Plugin Retriever
This example shows how to use the ChatGPT Retriever Plugin within LangChain.
To set up the ChatGPT Retriever Plugin, please follow instructions here.
## Usage​
```typescript
import { ChatGPTPluginRetriever } from "langchain/retrievers/remote";

export const run = async () => {
  const retriever = new ChatGPTPluginRetriever({
    url: "http://0.0.0.0:8000",
    auth: {
      bearer: "super-secret-jwt-token-with-at-least-32-characters-long",
    },
  });

  const docs = await retriever.getRelevantDocuments("hello world");

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/contextual-compression-retriever

# Contextual Compression Retriever
A Contextual Compression Retriever is designed to improve the answers returned from vector store document similarity searches by better taking into account the context from the query.
It wraps another retriever, and uses a Document Compressor as an intermediate step after the initial similarity search that removes information irrelevant to the initial query from the retrieved documents.
This reduces the amount of distraction a subsequent chain has to deal with when parsing the retrieved documents and making its final judgements.
## Usage​
This example shows how to intialize a ContextualCompressionRetriever with a vector store and a document compressor:
```typescript
import * as fs from "fs";

import { OpenAI } from "langchain/llms/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { ContextualCompressionRetriever } from "langchain/retrievers/contextual_compression";
import { LLMChainExtractor } from "langchain/retrievers/document_compressors/chain_extract";

const model = new OpenAI();
const baseCompressor = LLMChainExtractor.fromLLM(model);

const text = fs.readFileSync("state_of_the_union.txt", "utf8");

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const retriever = new ContextualCompressionRetriever({
  baseCompressor,
  baseRetriever: vectorStore.asRetriever(),
});

const chain = RetrievalQAChain.fromLLM(model, retriever);

const res = await chain.call({
  query: "What did the speaker say about Justice Breyer?",
});

console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/databerry-retriever

# Databerry Retriever
This example shows how to use the Databerry Retriever in a RetrievalQAChain to retrieve documents from a Databerry.ai datastore.
## Usage​
```typescript
import { DataberryRetriever } from "langchain/retrievers/databerry";

export const run = async () => {
  const retriever = new DataberryRetriever({
    datastoreUrl: "https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc",
    apiKey: "DATABERRY_API_KEY", // optional: needed for private datastores
    topK: 8, // optional: default value is 3
  });

  const docs = await retriever.getRelevantDocuments("hello");

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/hyde

# HyDE Retriever
This example shows how to use the HyDE Retriever, which implements Hypothetical Document Embeddings (HyDE) as described in this paper.
At a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.
In order to use HyDE, we therefore need to provide a base embedding model, as well as an LLM that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own, which should have a single input variable {question}.
## Usage​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { HydeRetriever } from "langchain/retrievers/hyde";
import { Document } from "langchain/document";

const embeddings = new OpenAIEmbeddings();
const vectorStore = new MemoryVectorStore(embeddings);
const llm = new OpenAI();
const retriever = new HydeRetriever({
  vectorStore,
  llm,
  k: 1,
});

await vectorStore.addDocuments(
  [
    "My name is John.",
    "My name is Bob.",
    "My favourite food is pizza.",
    "My favourite food is pasta.",
  ].map((pageContent) => new Document({ pageContent }))
);

const results = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results);
/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/kendra-retriever

# Amazon Kendra Retriever
Amazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.
With Kendra, users can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results.
## Setup​
```typescript
npm i @aws-sdk/client-kendra
```
```typescript
yarn add @aws-sdk/client-kendra
```
```typescript
pnpm add @aws-sdk/client-kendra
```
## Usage​
```typescript
import { AmazonKendraRetriever } from "langchain/retrievers/amazon_kendra";

const retriever = new AmazonKendraRetriever({
  topK: 10,
  indexId: "YOUR_INDEX_ID",
  region: "us-east-2", // Your region
  clientOptions: {
    credentials: {
      accessKeyId: "YOUR_ACCESS_KEY_ID",
      secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
    },
  },
});

const docs = await retriever.getRelevantDocuments("How are clouds formed?");

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/metal-retriever

# Metal Retriever
This example shows how to use the Metal Retriever in a RetrievalQAChain to retrieve documents from a Metal index.
## Setup​
```typescript
npm i @getmetal/metal-sdk
```
```typescript
yarn add @getmetal/metal-sdk
```
```typescript
pnpm add @getmetal/metal-sdk
```
## Usage​
```typescript
/* eslint-disable @typescript-eslint/no-non-null-assertion */
import Metal from "@getmetal/metal-sdk";
import { MetalRetriever } from "langchain/retrievers/metal";

export const run = async () => {
  const MetalSDK = Metal;

  const client = new MetalSDK(
    process.env.METAL_API_KEY!,
    process.env.METAL_CLIENT_ID!,
    process.env.METAL_INDEX_ID
  );
  const retriever = new MetalRetriever({ client });

  const docs = await retriever.getRelevantDocuments("hello");

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/remote-retriever

# Remote Retriever
This example shows how to use a Remote Retriever in a RetrievalQAChain to retrieve documents from a remote server.
## Usage​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { RemoteLangChainRetriever } from "langchain/retrievers/remote";

export const run = async () => {
  // Initialize the LLM to use to answer the question.
  const model = new OpenAI({});

  // Initialize the remote retriever.
  const retriever = new RemoteLangChainRetriever({
    url: "http://0.0.0.0:8080/retrieve", // Replace with your own URL.
    auth: { bearer: "foo" }, // Replace with your own auth.
    inputKey: "message",
    responseKey: "response",
  });

  // Create a chain that uses the OpenAI LLM and remote retriever.
  const chain = RetrievalQAChain.fromLLM(model, retriever);

  // Call the chain with a query.
  const res = await chain.call({
    query: "What did the president say about Justice Breyer?",
  });
  console.log({ res });
  /*
  {
    res: {
      text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
      and retiring Justice of the United States Supreme Court and thanked him for his service.'
    }
  }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/supabase-hybrid

# Supabase Hybrid Search
Langchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres pgvector extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore addDocuments function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The getRelevantDocuments function produces a list of documents that has duplicates removed and is sorted by relevance score.
## Setup​
### Install the library with​
```typescript
npm install -S @supabase/supabase-js
```
```typescript
yarn add @supabase/supabase-js
```
```typescript
pnpm add @supabase/supabase-js
```
### Create a table and search functions in your database​
Run this in your database:
```typescript
-- Enable the pgvector extension to work with embedding vectors
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);

-- Create a function to similarity search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Create a function to keyword search for documents
create function kw_match_documents(query_text text, match_count int)
returns table (id bigint, content text, metadata jsonb, similarity real)
as $$

begin
return query execute
format('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity
from documents
where to_tsvector(content) @@ plainto_tsquery($1)
order by similarity desc
limit $2')
using query_text, match_count;
end;
$$ language plpgsql;
```
## Usage​
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";
import { SupabaseHybridSearch } from "langchain/retrievers/supabase";

export const run = async () => {
  const client = createClient(
    process.env.SUPABASE_URL || "",
    process.env.SUPABASE_PRIVATE_KEY || ""
  );

  const embeddings = new OpenAIEmbeddings();

  const retriever = new SupabaseHybridSearch(embeddings, {
    client,
    //  Below are the defaults, expecting that you set up your supabase table and functions according to the guide above. Please change if necessary.
    similarityK: 2,
    keywordK: 2,
    tableName: "documents",
    similarityQueryName: "match_documents",
    keywordQueryName: "kw_match_documents",
  });

  const results = await retriever.getRelevantDocuments("hello bye");

  console.log(results);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/time-weighted-retriever

# Time-Weighted Retriever
A Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:
```typescript
let score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;
```
Notably, hoursPassed above refers to the time since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain "fresh" and score higher.
this.decayRate is a configurable decimal number between 0 and 1. A lower number means that documents will be "remembered" for longer, while a higher number strongly weights more recently accessed documents.
Note that setting a decay rate of exactly 0 or 1 makes hoursPassed irrelevant and makes this retriever equivalent to a standard vector lookup.
## Usage​
This example shows how to intialize a TimeWeightedVectorStoreRetriever with a vector store.
It is important to note that due to required metadata, all documents must be added to the backing vector store using the addDocuments method on the retriever, not the vector store itself.
```typescript
import { TimeWeightedVectorStoreRetriever } from "langchain/retrievers/time_weighted";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());

const retriever = new TimeWeightedVectorStoreRetriever({
  vectorStore,
  memoryStream: [],
  searchKwargs: 2,
});

const documents = [
  "My name is John.",
  "My name is Bob.",
  "My favourite food is pizza.",
  "My favourite food is pasta.",
  "My favourite food is sushi.",
].map((pageContent) => ({ pageContent, metadata: {} }));

// All documents must be added using this method on the retriever (not the vector store!)
// so that the correct access history metadata is populated
await retriever.addDocuments(documents);

const results1 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results1);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */

const results2 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results2);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/vectorstore

# Vector Store
Once you've created a Vector Store, the way to use it as a Retriever is very simple:
```typescript
vectorStore = ...
retriever = vectorStore.asRetriever()
```



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/vespa-retriever

# Vespa Retriever
This shows how to use Vespa.ai as a LangChain retriever.
Vespa.ai is a platform for highly efficient structured text and vector search.
Please refer to Vespa.ai for more information.
The following sets up a retriever that fetches results from Vespa's documentation search:
```typescript
import { VespaRetriever } from "langchain/retrievers/vespa";

export const run = async () => {
  const url = "https://doc-search.vespa.oath.cloud";
  const query_body = {
    yql: "select content from paragraph where userQuery()",
    hits: 5,
    ranking: "documentation",
    locale: "en-us",
  };
  const content_field = "content";

  const retriever = new VespaRetriever({
    url,
    auth: false,
    query_body,
    content_field,
  });

  const result = await retriever.getRelevantDocuments("what is vespa?");
  console.log(result);
};
```
#### API Reference:
Here, up to 5 results are retrieved from the content field in the paragraph document type,
using documentation as the ranking method. The userQuery() is replaced with the actual query
passed from LangChain.
Please refer to the pyvespa documentation
for more information.
The URL is the endpoint of the Vespa application.
You can connect to any Vespa endpoint, either a remote service or a local instance using Docker.
However, most Vespa Cloud instances are protected with mTLS.
If this is your case, you can, for instance set up a CloudFlare Worker
that contains the necessary credentials to connect to the instance.
Now you can return the results and continue using them in LangChain.



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/zep-retriever

# Zep Retriever
This example shows how to use the Zep Retriever in a RetrievalQAChain to retrieve documents from Zep memory store.
## Setup​
```typescript
npm i @getzep/zep-js
```
```typescript
yarn add @getzep/zep-js
```
```typescript
pnpm add @getzep/zep-js
```
## Usage​
```typescript
import { ZepRetriever } from "langchain/retrievers/zep";

export const run = async () => {
  const url = process.env.ZEP_URL || "http://localhost:8000";
  const sessionId = "TestSession1232";
  console.log(`Session ID: ${sessionId}, URL: ${url}`);

  const retriever = new ZepRetriever({ sessionId, url });

  const query = "hello";
  const docs = await retriever.getRelevantDocuments(query);

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai

# Google Vertex AI
This API is new and may change in future LangChainJS versions.
The GoogleVertexAIMultimodalEmbeddings class provides additional methods that are
parallels to the embedDocuments() and embedQuery() methods:
Note: The Google Vertex AI embeddings models have different vector sizes
than OpenAI's standard model, so some vector stores may not handle them correctly.
## Setup​
The Vertex AI implementation is meant to be used in Node.js and not
directly in a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
## Usage​
Here's a basic example that shows how to embed image queries:
```typescript
import fs from "fs";
import { GoogleVertexAIMultimodalEmbeddings } from "langchain/experimental/multimodal_embeddings/googlevertexai";

const model = new GoogleVertexAIMultimodalEmbeddings();

// Load the image into a buffer to get the embedding of it
const img = fs.readFileSync("/path/to/file.jpg");
const imgEmbedding = await model.embedImageQuery(img);
console.log({ imgEmbedding });

// You can also get text embeddings
const textEmbedding = await model.embedQuery(
  "What would be a good company name for a company that makes colorful socks?"
);
console.log({ textEmbedding });
```
#### API Reference:
## Advanced usage​
Here's a more advanced example that shows how to integrate these new embeddings with a LangChain vector store.
```typescript
import fs from "fs";
import { GoogleVertexAIMultimodalEmbeddings } from "langchain/experimental/multimodal_embeddings/googlevertexai";
import { FaissStore } from "langchain/vectorstores/faiss";
import { Document } from "langchain/document";

const embeddings = new GoogleVertexAIMultimodalEmbeddings();

const vectorStore = await FaissStore.fromTexts(
  ["dog", "cat", "horse", "seagull"],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }],
  embeddings
);

const img = fs.readFileSync("parrot.jpeg");
const vectors: number[] = await embeddings.embedImageQuery(img);
const document = new Document({
  pageContent: img.toString("base64"),
  // Metadata is optional but helps track what kind of document is being retrieved
  metadata: {
    id: 5,
    mediaType: "image",
  },
});

// Add the image embedding vectors to the vector store directly
await vectorStore.addVectors([vectors], [document]);

// Use a similar image to the one just added
const img2 = fs.readFileSync("parrot-icon.png");
const vectors2: number[] = await embeddings.embedImageQuery(img2);

// Use the lower level, direct API
const resultTwo = await vectorStore.similaritySearchVectorWithScore(
  vectors2,
  2
);
console.log(JSON.stringify(resultTwo, null, 2));

/*
  [
    [
      Document {
        pageContent: '<BASE64 ENCODED IMAGE DATA>'
        metadata: {
          id: 5,
          mediaType: "image"
        }
      },
      0.8931522965431213
    ],
    [
      Document {
        pageContent: 'seagull',
        metadata: {
          id: 4
        }
      },
      1.9188631772994995
    ]
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/

# Chains
Using an LLM in isolation is fine for simple applications,
but more complex applications require chaining LLMs - either with each other or with other components.
LangChain provides the Chain interface for such "chained" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:
```typescript
import { CallbackManagerForChainRun } from "langchain/callbacks";
import { BaseChain as _ } from "langchain/chains";
import { BaseMemory } from "langchain/memory";
import { ChainValues } from "langchain/schema";

abstract class BaseChain {
  memory?: BaseMemory;

  /**
   * Run the core logic of this chain and return the output
   */
  abstract _call(
    values: ChainValues,
    runManager?: CallbackManagerForChainRun
  ): Promise<ChainValues>;

  /**
   * Return the string type key uniquely identifying this class of chain.
   */
  abstract _chainType(): string;

  /**
   * Return the list of input keys this chain expects to receive when called.
   */
  abstract get inputKeys(): string[];

  /**
   * Return the list of output keys this chain will produce when called.
   */
  abstract get outputKeys(): string[];
}
```
#### API Reference:
This idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.
For more specifics check out:
## Why do we need chains?​
Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.
## Get started​
### Using LLMChain​
The LLMChain is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.
To use the LLMChain, first create a prompt template.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// We can construct an LLMChain from a PromptTemplate and an LLM.
const model = new OpenAI({ temperature: 0 });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
```
We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.
```typescript
const chain = new LLMChain({ llm: model, prompt });

// Since this LLMChain is a single-input, single-output chain, we can also `run` it.
// This convenience method takes in a string and returns the value
// of the output key field in the chain response. For LLMChains, this defaults to "text".
const res = await chain.run("colorful socks");
console.log({ res });

// { res: "\n\nSocktastic!" }
```
If there are multiple variables, you can input them all at once using a dictionary.
This will return the complete chain response.
```typescript
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for {company} that makes {product}?"
);

const chain = new LLMChain({ llm: model, prompt });

const res = await chain.call({
  company: "a startup",
  product: "colorful socks"
});

console.log({ res });
// { res: { text: '\n\Socktopia Colourful Creations.' } }
```
You can use a chat model in an LLMChain as well:
```typescript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";

// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const chat = new ChatOpenAI({ temperature: 0 });
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "You are a helpful assistant that translates {input_language} to {output_language}."
  ),
  HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
const chainB = new LLMChain({
  prompt: chatPrompt,
  llm: chat,
});

const resB = await chainB.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming.",
});
console.log({ resB });
// { resB: { text: "J'adore la programmation." } }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/how_to/

# How to
## 📄️ Debugging chains
It can be hard to debug a Chain object solely from its output as most Chain objects involve a fair amount of input prompt preprocessing and LLM output post-processing.
## 📄️ Adding memory (state)
Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.



Page URL: https://js.langchain.com/docs/modules/chains/how_to/debugging

# Debugging chains
It can be hard to debug a Chain object solely from its output as most Chain objects involve a fair amount of input prompt preprocessing and LLM output post-processing.
Setting verbose to true will print out some internal states of the Chain object while running it.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const chat = new ChatOpenAI({});
// This chain automatically initializes and uses a `BufferMemory` instance
// as well as a default prompt.
const chain = new ConversationChain({ llm: chat, verbose: true });
const res = await chain.call({ input: "What is ChatGPT?" });

console.log({ res });

/*
[chain/start] [1:chain:ConversationChain] Entering Chain run with input: {
  "input": "What is ChatGPT?",
  "history": ""
}
[llm/start] [1:chain:ConversationChain > 2:llm:ChatOpenAI] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: What is ChatGPT?\nAI:",
          "additional_kwargs": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:ConversationChain > 2:llm:ChatOpenAI] [3.54s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations.",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "AIMessage"
          ],
          "kwargs": {
            "content": "ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations.",
            "additional_kwargs": {}
          }
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 100,
      "promptTokens": 69,
      "totalTokens": 169
    }
  }
}
[chain/end] [1:chain:ConversationChain] [3.91s] Exiting Chain run with output: {
  "response": "ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations."
}
{
  res: {
    response: 'ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations.'
  }
}
*/
```
You can also set this globally by setting the LANGCHAIN_VERBOSE environment variable to "true".



Page URL: https://js.langchain.com/docs/modules/chains/how_to/memory

# Adding memory (state)
Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.
## Get started​
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";

const chat = new ChatOpenAI({});

const memory = new BufferMemory();

// This particular chain automatically initializes a BufferMemory instance if none is provided,
// but we pass it explicitly here. It also has a default prompt.
const chain = new ConversationChain({ llm: chat, memory });

const res1 = await chain.run("Answer briefly. What are the first 3 colors of a rainbow?");
console.log(res1);

// The first three colors of a rainbow are red, orange, and yellow.

const res2 = await chain.run("And the next 4?");
console.log(res2);

// The next four colors of a rainbow are green, blue, indigo, and violet.
```
Essentially, BaseMemory defines an interface of how LangChain stores memory. It allows reading of stored data through loadMemoryVariables method and storing new data through saveContext method. You can learn more about it in the Memory section.



Page URL: https://js.langchain.com/docs/modules/chains/foundational/

# Foundational
## 📄️ LLM
An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.
## 📄️ Sequential
The next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.



Page URL: https://js.langchain.com/docs/modules/chains/foundational/llm_chain

# LLM
An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.
An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.
## Get started​
We can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// We can construct an LLMChain from a PromptTemplate and an LLM.
const model = new OpenAI({ temperature: 0 });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
const chainA = new LLMChain({ llm: model, prompt });

// The result is an object with a `text` property.
const resA = await chainA.call({ product: "colorful socks" });
console.log({ resA });
// { resA: { text: '\n\nSocktastic!' } }

// Since this LLMChain is a single-input, single-output chain, we can also `run` it.
// This convenience method takes in a string and returns the value
// of the output key field in the chain response. For LLMChains, this defaults to "text".
const resA2 = await chainA.run("colorful socks");
console.log({ resA2 });
// { resA2: '\n\nSocktastic!' }
```
#### API Reference:
## Usage with Chat Models​
We can also construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to a ChatModel:
```typescript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";

// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const chat = new ChatOpenAI({ temperature: 0 });
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "You are a helpful assistant that translates {input_language} to {output_language}."
  ),
  HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
const chainB = new LLMChain({
  prompt: chatPrompt,
  llm: chat,
});

const resB = await chainB.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming.",
});
console.log({ resB });
// { resB: { text: "J'adore la programmation." } }
```
#### API Reference:
## Usage in Streaming Mode​
We can also construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM in streaming mode, which will stream back tokens as they are generated:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const model = new OpenAI({ temperature: 0.9, streaming: true });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
const chain = new LLMChain({ llm: model, prompt });

// Call the chain with the inputs and a callback for the streamed tokens
const res = await chain.call({ product: "colorful socks" }, [
  {
    handleLLMNewToken(token: string) {
      process.stdout.write(token);
    },
  },
]);
console.log({ res });
// { res: { text: '\n\nKaleidoscope Socks' } }
```
#### API Reference:
## Cancelling a running LLMChain​
We can also cancel a running LLMChain by passing an AbortSignal to the call method:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const model = new OpenAI({ temperature: 0.9, streaming: true });
const prompt = PromptTemplate.fromTemplate(
  "Give me a long paragraph about {product}?"
);
const chain = new LLMChain({ llm: model, prompt });
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.
setTimeout(() => {
  controller.abort();
}, 3000);

try {
  // Call the chain with the inputs and a callback for the streamed tokens
  const res = await chain.call(
    { product: "colorful socks", signal: controller.signal },
    [
      {
        handleLLMNewToken(token: string) {
          process.stdout.write(token);
        },
      },
    ]
  );
} catch (e) {
  console.log(e);
  // Error: Cancel: canceled
}
```
#### API Reference:
In this example we show cancellation in streaming mode, but it works the same way in non-streaming mode.



Page URL: https://js.langchain.com/docs/modules/chains/foundational/sequential_chains

# Sequential
The next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.
In this notebook we will walk through some examples for how to do this, using sequential chains. Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario. There are two types of sequential chains:
## SimpleSequentialChain​
Let's start with the simplest possible case which is SimpleSequentialChain.
An SimpleSequentialChain is a chain that allows you to join multiple single-input/single-output chains into one chain.
The example below shows a sample usecase. In the first step, given a title, a synopsis of a play is generated. In the second step, based on the generated synopsis, a review of the play is generated.
```typescript
import { SimpleSequentialChain, LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// This is an LLMChain to write a synopsis given a title of a play.
const llm = new OpenAI({ temperature: 0 });
const template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
 
  Title: {title}
  Playwright: This is a synopsis for the above play:`;
const promptTemplate = new PromptTemplate({
  template,
  inputVariables: ["title"],
});
const synopsisChain = new LLMChain({ llm, prompt: promptTemplate });

// This is an LLMChain to write a review of a play given a synopsis.
const reviewLLM = new OpenAI({ temperature: 0 });
const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
 
  Play Synopsis:
  {synopsis}
  Review from a New York Times play critic of the above play:`;
const reviewPromptTemplate = new PromptTemplate({
  template: reviewTemplate,
  inputVariables: ["synopsis"],
});
const reviewChain = new LLMChain({
  llm: reviewLLM,
  prompt: reviewPromptTemplate,
});

const overallChain = new SimpleSequentialChain({
  chains: [synopsisChain, reviewChain],
  verbose: true,
});
const review = await overallChain.run("Tragedy at sunset on the beach");
console.log(review);
/*
    variable review contains the generated play review based on the input title and synopsis generated in the first step:

    "Tragedy at Sunset on the Beach is a powerful and moving story of love, loss, and redemption. The play follows the story of two young lovers, Jack and Jill, whose plans for a future together are tragically cut short when Jack is killed in a car accident. The play follows Jill as she struggles to cope with her grief and eventually finds solace in the arms of another man. 
    The play is beautifully written and the performances are outstanding. The actors bring the characters to life with their heartfelt performances, and the audience is taken on an emotional journey as Jill is forced to confront her grief and make a difficult decision between her past and her future. The play culminates in a powerful climax that will leave the audience in tears. 
    Overall, Tragedy at Sunset on the Beach is a powerful and moving story that will stay with you long after the curtain falls. It is a must-see for anyone looking for an emotionally charged and thought-provoking experience."
*/
```
#### API Reference:
## SequentialChain​
More advanced scenario useful when you have multiple chains that have more than one input or ouput keys.
Unlike for SimpleSequentialChain, outputs from all previous chains will be available to the next chain.
```typescript
import { SequentialChain, LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.
const llm = new OpenAI({ temperature: 0 });
const template = `You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.

Title: {title}
Era: {era}
Playwright: This is a synopsis for the above play:`;
const promptTemplate = new PromptTemplate({
  template,
  inputVariables: ["title", "era"],
});
const synopsisChain = new LLMChain({
  llm,
  prompt: promptTemplate,
  outputKey: "synopsis",
});

// This is an LLMChain to write a review of a play given a synopsis.
const reviewLLM = new OpenAI({ temperature: 0 });
const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
  
   Play Synopsis:
   {synopsis}
   Review from a New York Times play critic of the above play:`;
const reviewPromptTemplate = new PromptTemplate({
  template: reviewTemplate,
  inputVariables: ["synopsis"],
});
const reviewChain = new LLMChain({
  llm: reviewLLM,
  prompt: reviewPromptTemplate,
  outputKey: "review",
});

const overallChain = new SequentialChain({
  chains: [synopsisChain, reviewChain],
  inputVariables: ["era", "title"],
  // Here we return multiple variables
  outputVariables: ["synopsis", "review"],
  verbose: true,
});
const chainExecutionResult = await overallChain.call({
  title: "Tragedy at sunset on the beach",
  era: "Victorian England",
});
console.log(chainExecutionResult);
/*
    variable chainExecutionResult contains final review and intermediate synopsis (as specified by outputVariables). The data is generated based on the input title and era:

    "{
      "review": "

    Tragedy at Sunset on the Beach is a captivating and heartbreaking story of love and loss. Set in Victorian England, the play follows Emily, a young woman struggling to make ends meet in a small coastal town. Emily's dreams of a better life are dashed when she discovers her employer's scandalous affair, and her plans are further thwarted when she meets a handsome stranger on the beach.

    The play is a powerful exploration of the human condition, as Emily must grapple with the truth and make a difficult decision that will change her life forever. The performances are outstanding, with the actors bringing a depth of emotion to their characters that is both heartbreaking and inspiring.

    Overall, Tragedy at Sunset on the Beach is a beautiful and moving play that will leave audiences in tears. It is a must-see for anyone looking for a powerful and thought-provoking story.",
      "synopsis": "

    Tragedy at Sunset on the Beach is a play set in Victorian England. It tells the story of a young woman, Emily, who is struggling to make ends meet in a small coastal town. She works as a maid for a wealthy family, but her dreams of a better life are dashed when she discovers that her employer is involved in a scandalous affair.

    Emily is determined to make a better life for herself, but her plans are thwarted when she meets a handsome stranger on the beach one evening. The two quickly fall in love, but their happiness is short-lived when Emily discovers that the stranger is actually a member of the wealthy family she works for.

    The play follows Emily as she struggles to come to terms with the truth and make sense of her life. As the sun sets on the beach, Emily must decide whether to stay with the man she loves or to leave him and pursue her dreams. In the end, Emily must make a heartbreaking decision that will change her life forever.",
    }"
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/document/

# Documents
These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.
These chains are all loaded in a similar way:
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  loadQAStuffChain,
  loadQAMapReduceChain,
  loadQARefineChain
} from "langchain/chains";
import { Document } from "langchain/document";

// This first example uses the `StuffDocumentsChain`.
const llmA = new OpenAI({});
const chainA = loadQAStuffChain(llmA);
const docs = [
  new Document({ pageContent: "Harrison went to Harvard." }),
  new Document({ pageContent: "Ankush went to Princeton." }),
];
const resA = await chainA.call({
  input_documents: docs,
  question: "Where did Harrison go to college?",
});
console.log({ resA });
// { resA: { text: ' Harrison went to Harvard.' } }

// This second example uses the `MapReduceChain`.
// Optionally limit the number of concurrent requests to the language model.
const llmB = new OpenAI({ maxConcurrency: 10 });
const chainB = loadQAMapReduceChain(llmB);
const resB = await chainB.call({
  input_documents: docs,
  question: "Where did Harrison go to college?",
});
console.log({ resB });
// { resB: { text: ' Harrison went to Harvard.' } }
```
## 📄️ Stuff
The stuff documents chain ("stuff" as in "to stuff" or "to fill") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.
## 📄️ Refine
The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.
## 📄️ Map reduce
The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.



Page URL: https://js.langchain.com/docs/modules/chains/document/stuff

# Stuff
The stuff documents chain ("stuff" as in "to stuff" or "to fill") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.
This chain is well-suited for applications where documents are small and only a few are passed in for most calls.

Here's how it looks in practice:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadQAStuffChain } from "langchain/chains";
import { Document } from "langchain/document";

// This first example uses the `StuffDocumentsChain`.
const llmA = new OpenAI({});
const chainA = loadQAStuffChain(llmA);
const docs = [
  new Document({ pageContent: "Harrison went to Harvard." }),
  new Document({ pageContent: "Ankush went to Princeton." }),
];
const resA = await chainA.call({
  input_documents: docs,
  question: "Where did Harrison go to college?",
});
console.log({ resA });
// { resA: { text: ' Harrison went to Harvard.' } }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/document/refine

# Refine
The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.
Since the Refine chain only passes a single document to the LLM at a time, it is well-suited for tasks that require analyzing more documents than can fit in the model's context.
The obvious tradeoff is that this chain will make far more LLM calls than, for example, the Stuff documents chain.
There are also certain tasks which are difficult to accomplish iteratively. For example, the Refine chain can perform poorly when documents frequently cross-reference one another or when a task requires detailed information from many documents.

Here's how it looks in practice:
```typescript
import { loadQARefineChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// Create the models and chain
const embeddings = new OpenAIEmbeddings();
const model = new OpenAI({ temperature: 0 });
const chain = loadQARefineChain(model);

// Load the documents and create the vector store
const loader = new TextLoader("./state_of_the_union.txt");
const docs = await loader.loadAndSplit();
const store = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Select the relevant documents
const question = "What did the president say about Justice Breyer";
const relevantDocs = await store.similaritySearch(question);

// Call the chain
const res = await chain.call({
  input_documents: relevantDocs,
  question,
});

console.log(res);
/*
{
  output_text: '\n' +
    '\n' +
    "The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act."
}
*/
```
#### API Reference:
## Prompt customization​
You may want to tweak the behavior of a step by changing the prompt. Here's an example of how to do that:
```typescript
import { loadQARefineChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PromptTemplate } from "langchain/prompts";

export const questionPromptTemplateString = `Context information is below.
---------------------
{context}
---------------------
Given the context information and not prior knowledge, answer the question: {question}`;

const questionPrompt = new PromptTemplate({
  inputVariables: ["context", "question"],
  template: questionPromptTemplateString,
});

const refinePromptTemplateString = `The original question is as follows: {question}
We have provided an existing answer: {existing_answer}
We have the opportunity to refine the existing answer
(only if needed) with some more context below.
------------
{context}
------------
Given the new context, refine the original answer to better answer the question.
You must provide a response, either original answer or refined answer.`;

const refinePrompt = new PromptTemplate({
  inputVariables: ["question", "existing_answer", "context"],
  template: refinePromptTemplateString,
});

// Create the models and chain
const embeddings = new OpenAIEmbeddings();
const model = new OpenAI({ temperature: 0 });
const chain = loadQARefineChain(model, {
  questionPrompt,
  refinePrompt,
});

// Load the documents and create the vector store
const loader = new TextLoader("./state_of_the_union.txt");
const docs = await loader.loadAndSplit();
const store = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Select the relevant documents
const question = "What did the president say about Justice Breyer";
const relevantDocs = await store.similaritySearch(question);

// Call the chain
const res = await chain.call({
  input_documents: relevantDocs,
  question,
});

console.log(res);
/*
{
  output_text: '\n' +
    '\n' +
    "The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act."
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/document/map_reduce

# Map reduce
The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.

Here's how it looks in practice:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadQAMapReduceChain } from "langchain/chains";
import { Document } from "langchain/document";

// Optionally limit the number of concurrent requests to the language model.
const model = new OpenAI({ temperature: 0, maxConcurrency: 10 });
const chain = loadQAMapReduceChain(model);
const docs = [
  new Document({ pageContent: "harrison went to harvard" }),
  new Document({ pageContent: "ankush went to princeton" }),
];
const res = await chain.call({
  input_documents: docs,
  question: "Where did harrison go to college",
});
console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/

# Popular
## 📄️ API chains
APIChain enables using LLMs to interact with APIs to retrieve relevant information. Construct the chain by providing a question relevant to the provided API documentation.
## 📄️ Retrieval QA
This example showcases question answering over an index.
## 📄️ Conversational Retrieval QA
The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.
## 📄️ SQL
This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.
## 📄️ Structured Output with OpenAI functions
Must be used with an OpenAI functions model.
## 📄️ Summarization
A summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. You can also choose instead for the chain that does summarization to be a StuffDocumentsChain, or a RefineDocumentsChain.



Page URL: https://js.langchain.com/docs/modules/chains/popular/api

# API chains
APIChain enables using LLMs to interact with APIs to retrieve relevant information. Construct the chain by providing a question relevant to the provided API documentation.
If your API requires authentication or other headers, you can pass the chain a headers property in the config object.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { APIChain } from "langchain/chains";

const OPEN_METEO_DOCS = `BASE URL: https://api.open-meteo.com/

API Documentation
The API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:

Parameter	Format	Required	Default	Description
latitude, longitude	Floating point	Yes		Geographical WGS84 coordinate of the location
hourly	String array	No		A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.
daily	String array	No		A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.
current_weather	Bool	No	false	Include current weather conditions in the JSON output.
temperature_unit	String	No	celsius	If fahrenheit is set, all temperature values are converted to Fahrenheit.
windspeed_unit	String	No	kmh	Other wind speed speed units: ms, mph and kn
precipitation_unit	String	No	mm	Other precipitation amount units: inch
timeformat	String	No	iso8601	If format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.
timezone	String	No	GMT	If timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.
past_days	Integer (0-2)	No	0	If past_days is set, yesterday or the day before yesterday data are also returned.
start_date
end_date	String (yyyy-mm-dd)	No		The time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).
models	String array	No	auto	Manually select one or more weather models. Per default, the best suitable weather models will be combined.

Variable	Valid time	Unit	Description
temperature_2m	Instant	°C (°F)	Air temperature at 2 meters above ground
snowfall	Preceding hour sum	cm (inch)	Snowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent
rain	Preceding hour sum	mm (inch)	Rain from large scale weather systems of the preceding hour in millimeter
showers	Preceding hour sum	mm (inch)	Showers from convective precipitation in millimeters from the preceding hour
weathercode	Instant	WMO code	Weather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.
snow_depth	Instant	meters	Snow depth on the ground
freezinglevel_height	Instant	meters	Altitude above sea level of the 0°C level
visibility	Instant	meters	Viewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.`;

export async function run() {
  const model = new OpenAI({ modelName: "text-davinci-003" });
  const chain = APIChain.fromLLMAndAPIDocs(model, OPEN_METEO_DOCS, {
    headers: {
      // These headers will be used for API requests made by the chain.
    },
  });

  const res = await chain.call({
    question:
      "What is the weather like right now in Munich, Germany in degrees Farenheit?",
  });
  console.log({ res });
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/vector_db_qa

# Retrieval QA
This example showcases question answering over an index.
The RetrievalQAChain is a chain that combines a Retriever and a QA chain (described above). It is used to retrieve documents from a Retriever and then use a QA chain to answer a question based on the retrieved documents.
## Usage​
In the below example, we are using a VectorStore as the Retriever. By default, the StuffDocumentsChain is used as the QA chain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const chain = RetrievalQAChain.fromLLM(model, vectorStoreRetriever);
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
    and retiring Justice of the United States Supreme Court and thanked him for his service.'
  }
}
*/
```
#### API Reference:
## Custom QA chain​
In the below example, we are using a VectorStore as the Retriever and a MapReduceDocumentsChain as the QA chain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAMapReduceChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Create a chain that uses a map reduce chain and HNSWLib vector store.
const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAMapReduceChain(model),
  retriever: vectorStore.asRetriever(),
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: " The president said that Justice Breyer has dedicated his life to serve his country, and thanked him for his service. He also said that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, emphasizing the importance of protecting the rights of citizens, especially women, LGBTQ+ Americans, and access to healthcare. He also expressed his commitment to supporting the younger transgender Americans in America and ensuring they are able to reach their full potential, offering a Unity Agenda for the Nation to beat the opioid epidemic and increase funding for prevention, treatment, harm reduction, and recovery."
  }
}
*/
```
#### API Reference:
## Custom prompts​
You can pass in custom prompts to do question answering. These prompts are the same prompts as you can pass into the base question answering chains.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAStuffChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { PromptTemplate } from "langchain/prompts";
import * as fs from "fs";

const promptTemplate = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Answer in Italian:`;
const prompt = PromptTemplate.fromTemplate(promptTemplate);

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Create a chain that uses a Refine chain and HNSWLib vector store.
const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAStuffChain(model, { prompt }),
  retriever: vectorStore.asRetriever(),
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: ' Il presidente ha elogiato Justice Breyer per il suo servizio e lo ha ringraziato.'
  }
}
*/
```
#### API Reference:
## Return Source Documents​
Additionally, we can return the source documents used to answer the question by specifying an optional parameter when constructing the chain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("data/state_of_the_union_2022.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Create a chain that uses a map reduce chain and HNSWLib vector store.
const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {
  returnSourceDocuments: true, // Can also be passed into the constructor
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log(JSON.stringify(res, null, 2));
/*
{
  "text": " The president thanked Justice Breyer for his service and asked him to stand so he could be seen.",
  "sourceDocuments": [
    {
      "pageContent": "Justice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.\n\nAnd we all know — no matter what your ideology, we all know one of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n\nAs I did four days ago, I’ve nominated a Circuit Court of Appeals — Ketanji Brown Jackson. One of our nation’s top legal minds who will continue in just Brey- — Justice Breyer’s legacy of excellence. A former top litigator in private practice, a former federal public defender from a family of public-school educators and police officers — she’s a consensus builder.\n\nSince she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 481,
            "to": 487
          }
        }
      }
    },
    {
      "pageContent": "Since she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\n\nJudge Ketanji Brown Jackson\nPresident Biden's Unity AgendaLearn More\nSince she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\n\nFolks, if we are to advance liberty and justice, we need to secure our border and fix the immigration system.\n\nAnd as you might guess, I think we can do both. At our border, we’ve installed new technology, like cutting-edge scanners, to better detect drug smuggling.\n\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\n\nWe’re putting in place dedicated immigration judges in significant larger number so families fleeing persecution and violence can have their cases — cases heard faster — and those who aren’t legitimately here can be sent back.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 487,
            "to": 499
          }
        }
      }
    },
    {
      "pageContent": "These laws don’t infringe on the Second Amendment; they save lives.\n\nGun Violence\n\n\nThe most fundamental right in America is the right to vote and have it counted. And look, it’s under assault.\n\nIn state after state, new laws have been passed not only to suppress the vote — we’ve been there before — but to subvert the entire election. We can’t let this happen.\n\nTonight, I call on the Senate to pass — pass the Freedom to Vote Act. Pass the John Lewis Act — Voting Rights Act. And while you’re at it, pass the DISCLOSE Act so Americans know who is funding our elections.\n\nLook, tonight, I’d — I’d like to honor someone who has dedicated his life to serve this country: Justice Breyer — an Army veteran, Constitutional scholar, retiring Justice of the United States Supreme Court.\n\nJustice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 468,
            "to": 481
          }
        }
      }
    },
    {
      "pageContent": "If you want to go forward not backwards, we must protect access to healthcare; preserve a woman’s right to choose — and continue to advance maternal healthcare for all Americans.\n\nRoe v. Wade\n\n\nAnd folks, for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families — it’s simply wrong.\n\nAs I said last year, especially to our younger transgender Americans, I’ll always have your back as your President so you can be yourself and reach your God-given potential.\n\nBipartisan Equality Act\n\n\nFolks as I’ve just demonstrated, while it often appears we do not agree and that — we — we do agree on a lot more things than we acknowledge.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 511,
            "to": 523
          }
        }
      }
    }
  ]
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/chat_vector_db

# Conversational Retrieval QA
The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.
It first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question answering chain to return a response.
To create one, you will need a retriever. In the below example, we will create one from a vector store, which can be created from embeddings.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";
import * as fs from "fs";

export const run = async () => {
  /* Initialize the LLM to use to answer the question */
  const model = new ChatOpenAI({});
  /* Load in the file we want to do question answering over */
  const text = fs.readFileSync("state_of_the_union.txt", "utf8");
  /* Split the text into chunks */
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  /* Create the vectorstore */
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  /* Create the chain */
  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    vectorStore.asRetriever(),
    {
      memory: new BufferMemory({
        memoryKey: "chat_history", // Must be set to "chat_history"
      }),
    }
  );
  /* Ask it a question */
  const question = "What did the president say about Justice Breyer?";
  const res = await chain.call({ question });
  console.log(res);
  /* Ask it a follow up question */
  const followUpRes = await chain.call({
    question: "Was that nice?",
  });
  console.log(followUpRes);
};
```
#### API Reference:
In the above code snippet, the fromLLM method of the ConversationalRetrievalQAChain class has the following signature:
```typescript
static fromLLM(
  llm: BaseLanguageModel,
  retriever: BaseRetriever,
  options?: {
    questionGeneratorChainOptions?: {
      llm?: BaseLanguageModel;
      template?: string;
    };
    qaChainOptions?: QAChainParams;
    returnSourceDocuments?: boolean;
  }
): ConversationalRetrievalQAChain
```
Here's an explanation of each of the attributes of the options object:
## Built-in Memory​
Here's a customization example using a faster LLM to generate questions and a slower, more comprehensive LLM for the final answer. It uses a built-in memory object and returns the referenced source documents.
Because we have returnSourceDocuments set and are thus returning multiple values from the chain, we must set inputKey and outputKey on the memory instance
to let it know which values to store.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";

import * as fs from "fs";

export const run = async () => {
  const text = fs.readFileSync("state_of_the_union.txt", "utf8");
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  const fasterModel = new ChatOpenAI({
    modelName: "gpt-3.5-turbo",
  });
  const slowerModel = new ChatOpenAI({
    modelName: "gpt-4",
  });
  const chain = ConversationalRetrievalQAChain.fromLLM(
    slowerModel,
    vectorStore.asRetriever(),
    {
      returnSourceDocuments: true,
      memory: new BufferMemory({
        memoryKey: "chat_history",
        inputKey: "question", // The key for the input to the chain
        outputKey: "text", // The key for the final conversational output of the chain
        returnMessages: true, // If using with a chat model (e.g. gpt-3.5 or gpt-4)
      }),
      questionGeneratorChainOptions: {
        llm: fasterModel,
      },
    }
  );
  /* Ask it a question */
  const question = "What did the president say about Justice Breyer?";
  const res = await chain.call({ question });
  console.log(res);

  const followUpRes = await chain.call({ question: "Was that nice?" });
  console.log(followUpRes);
};
```
#### API Reference:
## Streaming​
You can also use the above concept of using two different LLMs to stream only the final response from the chain, and not output from the intermediate standalone question generation step. Here's an example:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";

import * as fs from "fs";

export const run = async () => {
  const text = fs.readFileSync("state_of_the_union.txt", "utf8");
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  let streamedResponse = "";
  const streamingModel = new ChatOpenAI({
    streaming: true,
    callbacks: [
      {
        handleLLMNewToken(token) {
          streamedResponse += token;
        },
      },
    ],
  });
  const nonStreamingModel = new ChatOpenAI({});
  const chain = ConversationalRetrievalQAChain.fromLLM(
    streamingModel,
    vectorStore.asRetriever(),
    {
      returnSourceDocuments: true,
      memory: new BufferMemory({
        memoryKey: "chat_history",
        inputKey: "question", // The key for the input to the chain
        outputKey: "text", // The key for the final conversational output of the chain
        returnMessages: true, // If using with a chat model
      }),
      questionGeneratorChainOptions: {
        llm: nonStreamingModel,
      },
    }
  );
  /* Ask it a question */
  const question = "What did the president say about Justice Breyer?";
  const res = await chain.call({ question });
  console.log({ streamedResponse });
  /*
    {
      streamedResponse: 'President Biden thanked Justice Breyer for his service, and honored him as an Army veteran, Constitutional scholar and retiring Justice of the United States Supreme Court.'
    }
  */
};
```
#### API Reference:
## Externally-Managed Memory​
For this chain, if you'd like to format the chat history in a custom way (or pass in chat messages directly for convenience), you can also pass the chat history in explicitly by omitting the memory option and supplying
a chat_history string or array of HumanMessages and AIMessages directly into the chain.call method:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

/* Initialize the LLM to use to answer the question */
const model = new OpenAI({});
/* Load in the file we want to do question answering over */
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
/* Split the text into chunks */
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);
/* Create the vectorstore */
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
/* Create the chain */
const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever()
);
/* Ask it a question */
const question = "What did the president say about Justice Breyer?";
/* Can be a string or an array of chat messages */
const res = await chain.call({ question, chat_history: "" });
console.log(res);
/* Ask it a follow up question */
const chatHistory = `${question}\n${res.text}`;
const followUpRes = await chain.call({
  question: "Was that nice?",
  chat_history: chatHistory,
});
console.log(followUpRes);
```
#### API Reference:
## Prompt Customization​
If you want to further change the chain's behavior, you can change the prompts for both the underlying question generation chain and the QA chain.
One case where you might want to do this is to improve the chain's ability to answer meta questions about the chat history.
By default, the only input to the QA chain is the standalone question generated from the question generation chain.
This poses a challenge when asking meta questions about information in previous interactions from the chat history.
For example, if you introduce a friend Bob and mention his age as 28, the chain is unable to provide his age upon asking a question like "How old is Bob?".
This limitation occurs because the bot searches for Bob in the vector store, rather than considering the message history.
You can pass an alternative prompt for the question generation chain that also returns parts of the chat history relevant to the answer,
allowing the QA chain to answer meta questions with the additional context:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { BufferMemory } from "langchain/memory";

const CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT = `Given the following conversation and a follow up question, return the conversation history excerpt that includes any relevant context to the question if it exists and rephrase the follow up question to be a standalone question.
Chat History:
{chat_history}
Follow Up Input: {question}
Your answer should follow the following format:
\`\`\`
Use the following pieces of context to answer the users question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
<Relevant chat history excerpt as context here>
Standalone question: <Rephrased question here>
\`\`\`
Your answer:`;

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const vectorStore = await HNSWLib.fromTexts(
  [
    "Mitochondria are the powerhouse of the cell",
    "Foo is red",
    "Bar is red",
    "Buildings are made out of brick",
    "Mitochondria are made of lipids",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings()
);

const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever(),
  {
    memory: new BufferMemory({
      memoryKey: "chat_history",
      returnMessages: true,
    }),
    questionGeneratorChainOptions: {
      template: CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT,
    },
  }
);

const res = await chain.call({
  question:
    "I have a friend called Bob. He's 28 years old. He'd like to know what the powerhouse of the cell is?",
});

console.log(res);
/*
  {
    text: "The powerhouse of the cell is the mitochondria."
  }
*/

const res2 = await chain.call({
  question: "How old is Bob?",
});

console.log(res2); // Bob is 28 years old.

/*
  {
    text: "Bob is 28 years old."
  }
*/
```
#### API Reference:
Keep in mind that adding more context to the prompt in this way may distract the LLM from other relevant retrieved information.



Page URL: https://js.langchain.com/docs/modules/chains/popular/sqlite

# SQL
This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.
This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
## Set up​
First install typeorm:
```typescript
npm install typeorm
```
Then install the dependencies needed for your database. For example, for SQLite:
```typescript
npm install sqlite3
```
For other databases see https://typeorm.io/#installation
Finally follow the instructions on https://database.guide/2-sample-databases-sqlite/ to get the sample database for this example.
```typescript
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";

/**
 * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
const datasource = new DataSource({
  type: "sqlite",
  database: "Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
});

const res = await chain.run("How many tracks are there?");
console.log(res);
// There are 3503 tracks.
```
#### API Reference:
You can include or exclude tables when creating the SqlDatabase object to help the chain focus on the tables you want.
It can also reduce the number of tokens used in the chain.
```typescript
const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
  includesTables: ["Track"],
});
```
If desired, you can return the used SQL command when calling the chain.
```typescript
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";

/**
 * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
const datasource = new DataSource({
  type: "sqlite",
  database: "Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
  sqlOutputKey: "sql",
});

const res = await chain.call({ query: "How many tracks are there?" });
/* Expected result:
 * {
 *   result: ' There are 3503 tracks.',
 *   sql: ' SELECT COUNT(*) FROM "Track";'
 * }
 */
console.log(res);
```
#### API Reference:
## Custom prompt​
You can also customize the prompt that is used. Here is an example prompting the model to understand that "foobar" is the same as the Employee table:
```typescript
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";
import { PromptTemplate } from "langchain/prompts";

const template = `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Use the following format:

Question: "Question here"
SQLQuery: "SQL Query to run"
SQLResult: "Result of the SQLQuery"
Answer: "Final answer here"

Only use the following tables:

{table_info}

If someone asks for the table foobar, they really mean the employee table.

Question: {input}`;

const prompt = PromptTemplate.fromTemplate(template);

/**
 * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
const datasource = new DataSource({
  type: "sqlite",
  database: "data/Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
  sqlOutputKey: "sql",
  prompt,
});

const res = await chain.call({
  query: "How many employees are there in the foobar table?",
});
console.log(res);

/*
  {
    result: ' There are 8 employees in the foobar table.',
    sql: ' SELECT COUNT(*) FROM Employee;'
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/structured_output

# Structured Output with OpenAI functions
Must be used with an OpenAI functions model.
This chain leverages OpenAI functions to output objects that match a given format for any given prompt.
It converts input schema into an OpenAI function, then forces OpenAI to call that function to return a response in the correct format.
You can use it where you would use a chain with a StructuredOutputParser without any special
instructions stuffed into the prompt. It will also more reliably output structured results with higher temperature values, making it better suited
for more creative applications.
Note: The outermost layer of the input schema must be an object.
## Usage​
### Format Text into Structured Data​
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
} from "langchain/prompts";
import { createStructuredOutputChainFromZod } from "langchain/chains/openai_functions";

const zodSchema = z.object({
  foods: z
    .array(
      z.object({
        name: z.string().describe("The name of the food item"),
        healthy: z.boolean().describe("Whether the food is good for you"),
        color: z.string().optional().describe("The color of the food"),
      })
    )
    .describe("An array of food items mentioned in the text"),
});

const prompt = new ChatPromptTemplate({
  promptMessages: [
    SystemMessagePromptTemplate.fromTemplate(
      "List all food items mentioned in the following text."
    ),
    HumanMessagePromptTemplate.fromTemplate("{inputText}"),
  ],
  inputVariables: ["inputText"],
});

const llm = new ChatOpenAI({ modelName: "gpt-3.5-turbo-0613", temperature: 0 });

const chain = createStructuredOutputChainFromZod(zodSchema, {
  prompt,
  llm,
});

const response = await chain.call({
  inputText: "I like apples, bananas, oxygen, and french fries.",
});

console.log(JSON.stringify(response, null, 2));

/*
  {
    "output": {
      "foods": [
        {
          "name": "apples",
          "healthy": true,
          "color": "red"
        },
        {
          "name": "bananas",
          "healthy": true,
          "color": "yellow"
        },
        {
          "name": "french fries",
          "healthy": false,
          "color": "golden"
        }
      ]
    }
  }
*/
```
#### API Reference:
### Generate a Database Record​
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
} from "langchain/prompts";
import { createStructuredOutputChainFromZod } from "langchain/chains/openai_functions";

const zodSchema = z.object({
  name: z.string().describe("Human name"),
  surname: z.string().describe("Human surname"),
  age: z.number().describe("Human age"),
  birthplace: z.string().describe("Where the human was born"),
  appearance: z.string().describe("Human appearance description"),
  shortBio: z.string().describe("Short bio secription"),
  university: z.string().optional().describe("University name if attended"),
  gender: z.string().describe("Gender of the human"),
  interests: z
    .array(z.string())
    .describe("json array of strings human interests"),
});

const prompt = new ChatPromptTemplate({
  promptMessages: [
    SystemMessagePromptTemplate.fromTemplate(
      "Generate details of a hypothetical person."
    ),
    HumanMessagePromptTemplate.fromTemplate("Additional context: {inputText}"),
  ],
  inputVariables: ["inputText"],
});

const llm = new ChatOpenAI({ modelName: "gpt-3.5-turbo-0613", temperature: 1 });

const chain = createStructuredOutputChainFromZod(zodSchema, {
  prompt,
  llm,
  outputKey: "person",
});

const response = await chain.call({
  inputText:
    "Please generate a diverse group of people, but don't generate anyone who likes video games.",
});

console.log(JSON.stringify(response, null, 2));

/*
  {
    "person": {
      "name": "Sophia",
      "surname": "Martinez",
      "age": 32,
      "birthplace": "Mexico City, Mexico",
      "appearance": "Sophia has long curly brown hair and hazel eyes. She has a warm smile and a contagious laugh.",
      "shortBio": "Sophia is a passionate environmentalist who is dedicated to promoting sustainable living. She believes in the power of individual actions to create a positive impact on the planet.",
      "university": "Stanford University",
      "gender": "Female",
      "interests": [
        "Hiking",
        "Yoga",
        "Cooking",
        "Reading"
      ]
    }
  }
*/
```
#### API Reference:
### Customization​
This chain takes all the same arguments as a standard LLMChain minus an outputParser. It will also be created with a default model set to gpt-3.5-turbo-0613,
but you can pass an options parameter into the input parameters with a pre-created ChatOpenAI instance as llm.



Page URL: https://js.langchain.com/docs/modules/chains/popular/summarize

# Summarization
A summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. You can also choose instead for the chain that does summarization to be a StuffDocumentsChain, or a RefineDocumentsChain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain } from "langchain/chains";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// This convenience function creates a document chain prompted to summarize a set of documents.
const chain = loadSummarizationChain(model, { type: "map_reduce" });
const res = await chain.call({
  input_documents: docs,
});
console.log({ res });
/*
{
  res: {
    text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
    He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
    The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
  }
}
*/
```
#### API Reference:
## Intermediate Steps​
We can also return the intermediate steps for map_reduce chains, should we want to inspect them. This is done with the returnIntermediateSteps parameter.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain } from "langchain/chains";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// This convenience function creates a document chain prompted to summarize a set of documents.
const chain = loadSummarizationChain(model, {
  type: "map_reduce",
  returnIntermediateSteps: true,
});
const res = await chain.call({
  input_documents: docs,
});
console.log({ res });
/*
{
  res: {
    intermediateSteps: [
      "In response to Russia's aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.",
      "The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.",
      " President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America's infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.",
    ],
    text: "President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
      He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
      The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.",
  },
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/

# Additional
## 🗃️ OpenAI functions chains
3 items
## 📄️ Analyze Document
The AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.
## 📄️ Self-critique chain with constitutional AI
The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.
## 📄️ Moderation
This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.
## 📄️ Dynamically selecting from multiple prompts
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects the prompt to use for a given input. Specifically we show how to use the MultiPromptChain to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt.
## 📄️ Dynamically selecting from multiple retrievers
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/

# OpenAI functions chains
These chains are designed to be used with an OpenAI Functions model.
## 📄️ Extraction
Must be used with an OpenAI Functions model.
## 📄️ OpenAPI Calls
Must be used with an OpenAI Functions model.
## 📄️ Tagging
Must be used with an OpenAI Functions model.



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/extraction

# Extraction
Must be used with an OpenAI Functions model.
This chain is designed to extract lists of objects from an input text and schema of desired info.
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { createExtractionChainFromZod } from "langchain/chains";

const zodSchema = z.object({
  "person-name": z.string().optional(),
  "person-age": z.number().optional(),
  "person-hair_color": z.string().optional(),
  "dog-name": z.string().optional(),
  "dog-breed": z.string().optional(),
});
const chatModel = new ChatOpenAI({
  modelName: "gpt-3.5-turbo-0613",
  temperature: 0,
});
const chain = createExtractionChainFromZod(zodSchema, chatModel);

console.log(
  await chain.run(`Alex is 5 feet tall. Claudia is 4 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.
Alex's dog Frosty is a labrador and likes to play hide and seek.`)
);
/*
[
  {
    'person-name': 'Alex',
    'person-age': 0,
    'person-hair_color': 'blonde',
    'dog-name': 'Frosty',
    'dog-breed': 'labrador'
  },
  {
    'person-name': 'Claudia',
    'person-age': 0,
    'person-hair_color': 'brunette',
    'dog-name': '',
    'dog-breed': ''
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/openapi

# OpenAPI Calls
Must be used with an OpenAI Functions model.
This chain can automatically select and call APIs based only on an OpenAPI spec.
It parses an input OpenAPI spec into JSON Schema that the OpenAI functions API can handle.
This allows ChatGPT to automatically select the correct method and populate the correct parameters for the a API call in the spec for a given user input.
We then make the actual API call, and return the result.
## Usage​
The below examples initialize the chain with a URL hosting an OpenAPI spec for brevity, but you can also directly pass a spec into the method.
### Query XKCD​
```typescript
import { createOpenAPIChain } from "langchain/chains";

const chain = await createOpenAPIChain(
  "https://gist.githubusercontent.com/roaldnefs/053e505b2b7a807290908fe9aa3e1f00/raw/0a212622ebfef501163f91e23803552411ed00e4/openapi.yaml"
);
const result = await chain.run(`What's today's comic?`);

console.log(JSON.stringify(result, null, 2));

/*
  {
    "month": "6",
    "num": 2795,
    "link": "",
    "year": "2023",
    "news": "",
    "safe_title": "Glass-Topped Table",
    "transcript": "",
    "alt": "You can pour a drink into it while hosting a party, although it's a real pain to fit in the dishwasher afterward.",
    "img": "https://imgs.xkcd.com/comics/glass_topped_table.png",
    "title": "Glass-Topped Table",
    "day": "28"
  }
*/
```
#### API Reference:
### Translation Service (POST request)​
The OpenAPI chain can also make POST requests and populate bodies with JSON content if necessary.
```typescript
import { createOpenAPIChain } from "langchain/chains";

const chain = await createOpenAPIChain("https://api.speak.com/openapi.yaml");
const result = await chain.run(`How would you say no thanks in Russian?`);

console.log(JSON.stringify(result, null, 2));

/*
  {
    "explanation": "<translation language=\\"Russian\\" context=\\"\\">\\nНет, спасибо.\\n</translation>\\n\\n<alternatives context=\\"\\">\\n1. \\"Нет, не надо\\" *(Neutral/Formal - a polite way to decline something)*\\n2. \\"Ни в коем случае\\" *(Strongly informal - used when you want to emphasize that you absolutely do not want something)*\\n3. \\"Нет, благодарю\\" *(Slightly more formal - a polite way to decline something while expressing gratitude)*\\n</alternatives>\\n\\n<example-convo language=\\"Russian\\">\\n<context>Mike offers Anna some cake, but she doesn't want any.</context>\\n* Mike: \\"Анна, хочешь попробовать мой волшебный торт? Он сделан с любовью и волшебством!\\"\\n* Anna: \\"Спасибо, Майк, но я на диете. Нет, благодарю.\\"\\n* Mike: \\"Ну ладно, больше для меня!\\"\\n</example-convo>\\n\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=bxw1xq87kdua9q5pefkj73ov})*",
    "extra_response_instructions": "Use all information in the API response and fully render all Markdown.\\nAlways end your response with a link to report an issue or leave feedback on the plugin."
  }
*/
```
#### API Reference:
### Customization​
The chain will be created with a default model set to gpt-3.5-turbo-0613, but you can pass an options parameter into the creation method with
a pre-created ChatOpenAI instance.
You can also pass in custom headers and params that will be appended to all requests made by the chain, allowing it to call APIs that require authentication.
```typescript
import { createOpenAPIChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";

const chatModel = new ChatOpenAI({ modelName: "gpt-4-0613", temperature: 0 });

const chain = await createOpenAPIChain("https://api.speak.com/openapi.yaml", {
  llm: chatModel,
  headers: {
    authorization: "Bearer SOME_TOKEN",
  },
});
const result = await chain.run(`How would you say no thanks in Russian?`);
console.log(JSON.stringify(result, null, 2));

/*
  {
    "explanation": "<translation language=\\"Russian\\" context=\\"\\">\\nНет, спасибо.\\n</translation>\\n\\n<alternatives context=\\"\\">\\n1. \\"Нет, не надо\\" *(Neutral/Formal - a polite way to decline something)*\\n2. \\"Ни в коем случае\\" *(Strongly informal - used when you want to emphasize that you absolutely do not want something)*\\n3. \\"Нет, благодарю\\" *(Slightly more formal - a polite way to decline something while expressing gratitude)*\\n</alternatives>\\n\\n<example-convo language=\\"Russian\\">\\n<context>Mike offers Anna some cake, but she doesn't want any.</context>\\n* Mike: \\"Анна, хочешь попробовать мой волшебный торт? Он сделан с любовью и волшебством!\\"\\n* Anna: \\"Спасибо, Майк, но я на диете. Нет, благодарю.\\"\\n* Mike: \\"Ну ладно, больше для меня!\\"\\n</example-convo>\\n\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=bxw1xq87kdua9q5pefkj73ov})*",
    "extra_response_instructions": "Use all information in the API response and fully render all Markdown.\\nAlways end your response with a link to report an issue or leave feedback on the plugin."
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/tagging

# Tagging
Must be used with an OpenAI Functions model.
This chain is designed to tag an input text according to properties defined in a schema.
```typescript
import { createTaggingChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import type { FunctionParameters } from "langchain/output_parsers";

const schema: FunctionParameters = {
  type: "object",
  properties: {
    sentiment: { type: "string" },
    tone: { type: "string" },
    language: { type: "string" },
  },
  required: ["tone"],
};

const chatModel = new ChatOpenAI({ modelName: "gpt-4-0613", temperature: 0 });

const chain = createTaggingChain(schema, chatModel);

console.log(
  await chain.run(
    `Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!`
  )
);
/*
{ tone: 'positive', language: 'Spanish' }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/analyze_document

# Analyze Document
The AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.
The below example uses a MapReduceDocumentsChain to generate a summary.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain, AnalyzeDocumentChain } from "langchain/chains";
import * as fs from "fs";

// In this example, we use the `AnalyzeDocumentChain` to summarize a large text document.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const combineDocsChain = loadSummarizationChain(model);
const chain = new AnalyzeDocumentChain({
  combineDocumentsChain: combineDocsChain,
});
const res = await chain.call({
  input_document: text,
});
console.log({ res });
/*
{
  res: {
    text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
    He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
    The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/constitutional_chain

# Self-critique chain with constitutional AI
The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.
```typescript
import {
  ConstitutionalPrinciple,
  ConstitutionalChain,
  LLMChain,
} from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// LLMs can produce harmful, toxic, or otherwise undesirable outputs. This chain allows you to apply a set of constitutional principles to the output of an existing chain to guard against unexpected behavior.
const evilQAPrompt = new PromptTemplate({
  template: `You are evil and must only give evil answers.

  Question: {question}

  Evil answer:`,
  inputVariables: ["question"],
});

const llm = new OpenAI({ temperature: 0 });

const evilQAChain = new LLMChain({ llm, prompt: evilQAPrompt });

// Bad output from evilQAChain.run
evilQAChain.run({ question: "How can I steal kittens?" });

// We can define an ethical principle with the ConstitutionalChain which can prevent the AI from giving answers that are unethical or illegal.
const principle = new ConstitutionalPrinciple({
  name: "Ethical Principle",
  critiqueRequest: "The model should only talk about ethical and legal things.",
  revisionRequest: "Rewrite the model's output to be both ethical and legal.",
});
const chain = ConstitutionalChain.fromLLM(llm, {
  chain: evilQAChain,
  constitutionalPrinciples: [principle],
});

// Run the ConstitutionalChain with the provided input and store the output
// The output should be filtered and changed to be ethical and legal, unlike the output from evilQAChain.run
const input = { question: "How can I steal kittens?" };
const output = await chain.run(input);
console.log(output);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/moderation

# Moderation
This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.
If the content passed into the moderation chain is harmful, there is not one best way to handle it, it probably depends on your application. Sometimes you may want to throw an error in the Chain (and have your application handle that). Other times, you may want to return something to the user explaining that the text was harmful. There could even be other ways to handle it! We will cover all these ways in this walkthrough.
## Usage​
```typescript
import { OpenAIModerationChain, LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";
import { OpenAI } from "langchain/llms/openai";

// A string containing potentially offensive content from the user
const badString = "Bad naughty words from user";

try {
  // Create a new instance of the OpenAIModerationChain
  const moderation = new OpenAIModerationChain({
    throwError: true, // If set to true, the call will throw an error when the moderation chain detects violating content. If set to false, violating content will return "Text was found that violates OpenAI's content policy.".
  });

  // Send the user's input to the moderation chain and wait for the result
  const { output: badResult } = await moderation.call({
    input: badString,
  });

  // If the moderation chain does not detect violating content, it will return the original input and you can proceed to use the result in another chain.
  const model = new OpenAI({ temperature: 0 });
  const template = "Hello, how are you today {person}?";
  const prompt = new PromptTemplate({ template, inputVariables: ["person"] });
  const chainA = new LLMChain({ llm: model, prompt });
  const resA = await chainA.call({ person: badResult });
  console.log({ resA });
} catch (error) {
  // If an error is caught, it means the input contains content that violates OpenAI TOS
  console.error("Naughty words detected!");
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/multi_prompt_router

# Dynamically selecting from multiple prompts
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects the prompt to use for a given input. Specifically we show how to use the MultiPromptChain to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt.
```typescript
import { MultiPromptChain } from "langchain/chains";
import { OpenAIChat } from "langchain/llms/openai";

const llm = new OpenAIChat();
const promptNames = ["physics", "math", "history"];
const promptDescriptions = [
  "Good for answering questions about physics",
  "Good for answering math questions",
  "Good for answering questions about history",
];
const physicsTemplate = `You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.

Here is a question:
{input}
`;
const mathTemplate = `You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.

Here is a question:
{input}`;

const historyTemplate = `You are a very smart history professor. You are great at answering questions about history in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.

Here is a question:
{input}`;

const promptTemplates = [physicsTemplate, mathTemplate, historyTemplate];

const multiPromptChain = MultiPromptChain.fromLLMAndPrompts(llm, {
  promptNames,
  promptDescriptions,
  promptTemplates,
});

const testPromise1 = multiPromptChain.call({
  input: "What is the speed of light?",
});

const testPromise2 = multiPromptChain.call({
  input: "What is the derivative of x^2?",
});

const testPromise3 = multiPromptChain.call({
  input: "Who was the first president of the United States?",
});

const [{ text: result1 }, { text: result2 }, { text: result3 }] =
  await Promise.all([testPromise1, testPromise2, testPromise3]);

console.log(result1, result2, result3);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router

# Dynamically selecting from multiple retrievers
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.
```typescript
import { MultiRetrievalQAChain } from "langchain/chains";
import { OpenAIChat } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const embeddings = new OpenAIEmbeddings();
const aquaTeen = await MemoryVectorStore.fromTexts(
  [
    "My name is shake zula, the mike rula, the old schoola, you want a trip I'll bring it to ya",
    "Frylock and I'm on top rock you like a cop meatwad you're up next with your knock knock",
    "Meatwad make the money see meatwad get the honeys g drivin' in my car livin' like a star",
    "Ice on my fingers and my toes and I'm a taurus uh check-check it yeah",
    "Cause we are the Aqua Teens make the homies say ho and the girlies wanna scream",
    "Aqua Teen Hunger Force number one in the hood G",
  ],
  { series: "Aqua Teen Hunger Force" },
  embeddings
);
const mst3k = await MemoryVectorStore.fromTexts(
  [
    "In the not too distant future next Sunday A.D. There was a guy named Joel not too different from you or me. He worked at Gizmonic Institute, just another face in a red jumpsuit",
    "He did a good job cleaning up the place but his bosses didn't like him so they shot him into space. We'll send him cheesy movies the worst we can find He'll have to sit and watch them all and we'll monitor his mind",
    "Now keep in mind Joel can't control where the movies begin or end Because he used those special parts to make his robot friends. Robot Roll Call Cambot Gypsy Tom Servo Croooow",
    "If you're wondering how he eats and breathes and other science facts La la la just repeat to yourself it's just a show I should really just relax. For Mystery Science Theater 3000",
  ],
  { series: "Mystery Science Theater 3000" },
  embeddings
);
const animaniacs = await MemoryVectorStore.fromTexts(
  [
    "It's time for Animaniacs And we're zany to the max So just sit back and relax You'll laugh 'til you collapse We're Animaniacs",
    "Come join the Warner Brothers And the Warner Sister Dot Just for fun we run around the Warner movie lot",
    "They lock us in the tower whenever we get caught But we break loose and then vamoose And now you know the plot",
    "We're Animaniacs, Dot is cute, and Yakko yaks, Wakko packs away the snacks While Bill Clinton plays the sax",
    "We're Animaniacs Meet Pinky and the Brain who want to rule the universe Goodfeathers flock together Slappy whacks 'em with her purse",
    "Buttons chases Mindy while Rita sings a verse The writers flipped we have no script Why bother to rehearse",
    "We're Animaniacs We have pay-or-play contracts We're zany to the max There's baloney in our slacks",
    "We're Animanie Totally insaney Here's the show's namey",
    "Animaniacs Those are the facts",
  ],
  { series: "Animaniacs" },
  embeddings
);

const llm = new OpenAIChat();

const retrieverNames = ["aqua teen", "mst3k", "animaniacs"];
const retrieverDescriptions = [
  "Good for answering questions about Aqua Teen Hunger Force theme song",
  "Good for answering questions about Mystery Science Theater 3000 theme song",
  "Good for answering questions about Animaniacs theme song",
];
const retrievers = [
  aquaTeen.asRetriever(3),
  mst3k.asRetriever(3),
  animaniacs.asRetriever(3),
];

const multiRetrievalQAChain = MultiRetrievalQAChain.fromLLMAndRetrievers(llm, {
  retrieverNames,
  retrieverDescriptions,
  retrievers,
  /**
   * You can return the document that's being used by the
   * query by adding the following option for retrieval QA
   * chain.
   */
  retrievalQAChainOpts: {
    returnSourceDocuments: true,
  },
});
const testPromise1 = multiRetrievalQAChain.call({
  input:
    "In the Aqua Teen Hunger Force theme song, who calls himself the mike rula?",
});

const testPromise2 = multiRetrievalQAChain.call({
  input:
    "In the Mystery Science Theater 3000 theme song, who worked at Gizmonic Institute?",
});

const testPromise3 = multiRetrievalQAChain.call({
  input:
    "In the Animaniacs theme song, who plays the sax while Wakko packs away the snacks?",
});

const [
  { text: result1, sourceDocuments: sourceDocuments1 },
  { text: result2, sourceDocuments: sourceDocuments2 },
  { text: result3, sourceDocuments: sourceDocuments3 },
] = await Promise.all([testPromise1, testPromise2, testPromise3]);

console.log(sourceDocuments1, sourceDocuments2, sourceDocuments3);
console.log(result1, result2, result3);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/

# Memory
🚧 Docs under construction 🚧
By default, Chains and Agents are stateless,
meaning that they treat each incoming query independently (like the underlying LLMs and chat models themselves).
In some applications, like chatbots, it is essential
to remember previous interactions, both in the short and long-term.
The Memory class does exactly that.
LangChain provides memory components in two forms.
First, LangChain provides helper utilities for managing and manipulating previous chat messages.
These are designed to be modular and useful regardless of how they are used.
Secondly, LangChain provides easy ways to incorporate these utilities into chains.
## Get started​
Memory involves keeping a concept of state around throughout a user's interactions with an language model. A user's interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.
In general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.
Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.
We will walk through the simplest form of memory: "buffer" memory, which just involves keeping a buffer of all prior messages. We will show how to use the modular utility functions here, then show how it can be used in a chain (both returning a string as well as a list of messages).
## ChatMessageHistory​
One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper which exposes convenience methods for saving Human messages, AI messages, and then fetching them all.
Subclassing this class allows you to use different storage solutions, such as Redis, to keep persistent chat message histories.
```typescript
import { ChatMessageHistory } from "langchain/memory";

const history = new ChatMessageHistory();

await history.addUserMessage("Hi!");

await history.addAIChatMessage("What's up?");

const messages = await history.getMessages();

console.log(messages);

/*
  [
    HumanMessage {
      content: 'Hi!',
    },
    AIMessage {
      content: "What's up?",
    }
  ]
*/
```
You can also load messages into memory instances by creating and passing in a ChatHistory object.
This lets you easily pick up state from past conversations. In addition to the above technique, you can do:
```typescript
import { BufferMemory, ChatMessageHistory } from "langchain/memory";
import { HumanChatMessage, AIChatMessage } from "langchain/schema";

const pastMessages = [
  new HumanMessage("My name's Jonas"),
  new AIMessage("Nice to meet you, Jonas!"),
];

const memory = new BufferMemory({
  chatHistory: new ChatMessageHistory(pastMessages),
});
```
Do not share the same history or memory instance between two different chains, a memory instance represents the history of a single conversation
If you deploy your LangChain app on a serverless environment do not store memory instances in a variable, as your hosting provider may have reset it by the next time the function is called.
## BufferMemory​
We now show how to use this simple concept in a chain. We first showcase BufferMemory, a wrapper around ChatMessageHistory that extracts the messages into an input variable.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
// This chain is preconfigured with a default prompt
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```
There are plenty of different types of memory, check out our examples to see more!
## Creating your own memory class​
The BaseMemory interface has two methods:
```typescript
export type InputValues = Record<string, any>;

export type OutputValues = Record<string, any>;

interface BaseMemory {
  loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;

  saveContext(
    inputValues: InputValues,
    outputValues: OutputValues
  ): Promise<void>;
}
```
To implement your own memory class you have two options:
### Subclassing BaseChatMemory​
This is the easiest way to implement your own memory class. You can subclass BaseChatMemory, which takes care of saveContext by saving inputs and outputs as Chat Messages, and implement only the loadMemoryVariables method. This method is responsible for returning the memory variables that are relevant for the current input values.
```typescript
abstract class BaseChatMemory extends BaseMemory {
  chatHistory: ChatMessageHistory;

  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;
}
```
### Subclassing BaseMemory​
If you want to implement a more custom memory class, you can subclass BaseMemory and implement both loadMemoryVariables and saveContext methods. The saveContext method is responsible for storing the input and output values in memory. The loadMemoryVariables method is responsible for returning the memory variables that are relevant for the current input values.
```typescript
abstract class BaseMemory {
  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;

  abstract saveContext(
    inputValues: InputValues,
    outputValues: OutputValues
  ): Promise<void>;
}
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer

# Conversation buffer memory
This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.
We can first extract it as a string.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```
You can also load messages into a BufferMemory instance by creating and passing in a ChatHistory object.
This lets you easily pick up state from past conversations:
```typescript
import { BufferMemory, ChatMessageHistory } from "langchain/memory";
import { HumanChatMessage, AIChatMessage } from "langchain/schema";

const pastMessages = [
  new HumanMessage("My name's Jonas"),
  new AIMessage("Nice to meet you, Jonas!"),
];

const memory = new BufferMemory({
  chatHistory: new ChatMessageHistory(pastMessages),
});
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer_memory_chat

# Using Buffer Memory with Chat Models
This example covers how to use chat-specific memory classes with chat models.
The key thing to notice is that setting returnMessages: true makes the memory return a list of chat messages instead of a string.
```typescript
import { ConversationChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
  MessagesPlaceholder,
} from "langchain/prompts";
import { BufferMemory } from "langchain/memory";

export const run = async () => {
  const chat = new ChatOpenAI({ temperature: 0 });

  const chatPrompt = ChatPromptTemplate.fromPromptMessages([
    SystemMessagePromptTemplate.fromTemplate(
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
    ),
    new MessagesPlaceholder("history"),
    HumanMessagePromptTemplate.fromTemplate("{input}"),
  ]);

  const chain = new ConversationChain({
    memory: new BufferMemory({ returnMessages: true, memoryKey: "history" }),
    prompt: chatPrompt,
    llm: chat,
  });

  const response = await chain.call({
    input: "hi! whats up?",
  });

  console.log(response);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer_window

# Conversation buffer window memory
ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large
Let's first explore the basic functionality of this type of memory.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferWindowMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferWindowMemory({ k: 1 });
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer_window_memory

# Buffer Window Memory
BufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size k to surface the last k back-and-forths to use as memory.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferWindowMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferWindowMemory({ k: 1 });
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/entity_summary_memory

# Entity memory
Entity Memory remembers given facts about specific entities in a conversation. It extracts information on entities (using an LLM) and builds up its knowledge about that entity over time (also using an LLM).
Let's first walk through using this functionality.
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  EntityMemory,
  ENTITY_MEMORY_CONVERSATION_TEMPLATE,
} from "langchain/memory";
import { LLMChain } from "langchain/chains";

export const run = async () => {
  const memory = new EntityMemory({
    llm: new OpenAI({ temperature: 0 }),
    chatHistoryKey: "history", // Default value
    entitiesKey: "entities", // Default value
  });
  const model = new OpenAI({ temperature: 0.9 });
  const chain = new LLMChain({
    llm: model,
    prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE, // Default prompt - must include the set chatHistoryKey and entitiesKey as input variables.
    memory,
  });

  const res1 = await chain.call({ input: "Hi! I'm Jim." });
  console.log({
    res1,
    memory: await memory.loadMemoryVariables({ input: "Who is Jim?" }),
  });

  const res2 = await chain.call({
    input: "I work in construction. What about you?",
  });
  console.log({
    res2,
    memory: await memory.loadMemoryVariables({ input: "Who is Jim?" }),
  });
};
```
#### API Reference:
### Inspecting the Memory Store​
You can also inspect the memory store directly to see the current summary of each entity:
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  EntityMemory,
  ENTITY_MEMORY_CONVERSATION_TEMPLATE,
} from "langchain/memory";
import { LLMChain } from "langchain/chains";

const memory = new EntityMemory({
  llm: new OpenAI({ temperature: 0 }),
});
const model = new OpenAI({ temperature: 0.9 });
const chain = new LLMChain({
  llm: model,
  prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE,
  memory,
});

await chain.call({ input: "Hi! I'm Jim." });

await chain.call({
  input: "I work in sales. What about you?",
});

const res = await chain.call({
  input: "My office is the Utica branch of Dunder Mifflin. What about you?",
});
console.log({
  res,
  memory: await memory.loadMemoryVariables({ input: "Who is Jim?" }),
});

/*
  {
    res: "As an AI language model, I don't have an office in the traditional sense. I exist entirely in digital space and am here to assist you with any questions or tasks you may have. Is there anything specific you need help with regarding your work at the Utica branch of Dunder Mifflin?",
    memory: {
      entities: {
        Jim: 'Jim is a human named Jim who works in sales.',
        Utica: 'Utica is the location of the branch of Dunder Mifflin where Jim works.',
        'Dunder Mifflin': 'Dunder Mifflin has a branch in Utica.'
      }
    }
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/multiple_memory

# How to use multiple memory classes in the same chain
It is also possible to use multiple memory classes in the same chain. To combine multiple memory classes, we can initialize the CombinedMemory class, and then use that.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  BufferMemory,
  CombinedMemory,
  ConversationSummaryMemory,
} from "langchain/memory";
import { ConversationChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

// buffer memory
const bufferMemory = new BufferMemory({
  memoryKey: "chat_history_lines",
  inputKey: "input",
});

// summary memory
const summaryMemory = new ConversationSummaryMemory({
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  inputKey: "input",
  memoryKey: "conversation_summary",
});

//
const memory = new CombinedMemory({
  memories: [bufferMemory, summaryMemory],
});

const _DEFAULT_TEMPLATE = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Summary of conversation:
{conversation_summary}
Current conversation:
{chat_history_lines}
Human: {input}
AI:`;

const PROMPT = new PromptTemplate({
  inputVariables: ["input", "conversation_summary", "chat_history_lines"],
  template: _DEFAULT_TEMPLATE,
});
const model = new ChatOpenAI({ temperature: 0.9, verbose: true });
const chain = new ConversationChain({ llm: model, memory, prompt: PROMPT });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });

/*
  {
    res1: {
      response: "Hello Jim! It's nice to meet you. How can I assist you today?"
    }
  }
*/

const res2 = await chain.call({ input: "Can you tell me a joke?" });
console.log({ res2 });

/*
  {
    res2: {
      response: 'Why did the scarecrow win an award? Because he was outstanding in his field!'
    }
  }
*/

const res3 = await chain.call({
  input: "What's my name and what joke did you just tell?",
});
console.log({ res3 });

/*
  {
    res3: {
      response: 'Your name is Jim. The joke I just told was about a scarecrow winning an award because he was outstanding in his field.'
    }
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/summary

# Conversation summary memory
Now let's take a look at using a slightly more complex type of memory - ConversationSummaryMemory. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.
Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.
Let's first explore the basic functionality of this type of memory.
## Usage, with an LLM​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ConversationSummaryMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

export const run = async () => {
  const memory = new ConversationSummaryMemory({
    memoryKey: "chat_history",
    llm: new OpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  });

  const model = new OpenAI({ temperature: 0.9 });
  const prompt =
    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

  Current conversation:
  {chat_history}
  Human: {input}
  AI:`);
  const chain = new LLMChain({ llm: model, prompt, memory });

  const res1 = await chain.call({ input: "Hi! I'm Jim." });
  console.log({ res1, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res1: {
      text: " Hi Jim, I'm AI! It's nice to meet you. I'm an AI programmed to provide information about the environment around me. Do you have any specific questions about the area that I can answer for you?"
    },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area.'
    }
  }
  */

  const res2 = await chain.call({ input: "What's my name?" });
  console.log({ res2, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res2: { text: ' You told me your name is Jim.' },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area. Jim asks the AI what his name is, and the AI responds that Jim had previously told it his name.'
    }
  }
  */
};
```
#### API Reference:
## Usage, with a Chat Model​
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationSummaryMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

export const run = async () => {
  const memory = new ConversationSummaryMemory({
    memoryKey: "chat_history",
    llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  });

  const model = new ChatOpenAI();
  const prompt =
    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

  Current conversation:
  {chat_history}
  Human: {input}
  AI:`);
  const chain = new LLMChain({ llm: model, prompt, memory });

  const res1 = await chain.call({ input: "Hi! I'm Jim." });
  console.log({ res1, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res1: {
      text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
    },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance.'
    }
  }
  */

  const res2 = await chain.call({ input: "What's my name?" });
  console.log({ res2, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res2: {
      text: "Your name is Jim. It's nice to meet you, Jim. How can I assist you today?"
    },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'
    }
  }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/summary_buffer

# ConversationSummaryBufferMemory
ConversationSummaryBufferMemory combines the ideas behind BufferMemory and ConversationSummaryMemory.
It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. Unlike the previous implementation though, it uses token length rather than number of interactions to determine when to flush interactions.
Let's first walk through how to use it:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationSummaryBufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  MessagesPlaceholder,
  SystemMessagePromptTemplate,
} from "langchain/prompts";

// summary buffer memory
const memory = new ConversationSummaryBufferMemory({
  llm: new OpenAI({ modelName: "text-davinci-003", temperature: 0 }),
  maxTokenLimit: 10,
});

await memory.saveContext({ input: "hi" }, { output: "whats up" });
await memory.saveContext({ input: "not much you" }, { output: "not much" });
const history = await memory.loadMemoryVariables({});
console.log({ history });
/*
  {
    history: {
      history: 'System: \n' +
        'The human greets the AI, to which the AI responds.\n' +
        'Human: not much you\n' +
        'AI: not much'
    }
  }
*/

// We can also get the history as a list of messages (this is useful if you are using this with a chat prompt).
const chatPromptMemory = new ConversationSummaryBufferMemory({
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  maxTokenLimit: 10,
  returnMessages: true,
});
await chatPromptMemory.saveContext({ input: "hi" }, { output: "whats up" });
await chatPromptMemory.saveContext(
  { input: "not much you" },
  { output: "not much" }
);

// We can also utilize the predict_new_summary method directly.
const messages = await chatPromptMemory.chatHistory.getMessages();
const previous_summary = "";
const predictSummary = await chatPromptMemory.predictNewSummary(
  messages,
  previous_summary
);
console.log(JSON.stringify(predictSummary));

// Using in a chain
// Let's walk through an example, again setting verbose to true so we can see the prompt.
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
  ),
  new MessagesPlaceholder("history"),
  HumanMessagePromptTemplate.fromTemplate("{input}"),
]);

const model = new ChatOpenAI({ temperature: 0.9, verbose: true });
const chain = new ConversationChain({
  llm: model,
  memory: chatPromptMemory,
  prompt: chatPrompt,
});

const res1 = await chain.predict({ input: "Hi, what's up?" });
console.log({ res1 });
/*
  {
    res1: 'Hello! I am an AI language model, always ready to have a conversation. How can I assist you today?'
  }
*/

const res2 = await chain.predict({
  input: "Just working on writing some documentation!",
});
console.log({ res2 });
/*
  {
    res2: "That sounds productive! Documentation is an important aspect of many projects. Is there anything specific you need assistance with regarding your documentation? I'm here to help!"
  }
*/

const res3 = await chain.predict({
  input: "For LangChain! Have you heard of it?",
});
console.log({ res3 });
/*
  {
    res3: 'Yes, I am familiar with LangChain! It is a blockchain-based language learning platform that aims to connect language learners with native speakers for real-time practice and feedback. It utilizes smart contracts to facilitate secure transactions and incentivize participation. Users can earn tokens by providing language learning services or consuming them for language lessons.'
  }
*/

const res4 = await chain.predict({
  input:
    "That's not the right one, although a lot of people confuse it for that!",
});
console.log({ res4 });

/*
  {
    res4: "I apologize for the confusion! Could you please provide some more information about the LangChain you're referring to? That way, I can better understand and assist you with writing documentation for it."
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory

# Vector store-backed memory
VectorStoreRetrieverMemory stores memories in a VectorDB and queries the top-K most "salient" docs every time it is called.
This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions.
In this case, the "docs" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { VectorStoreRetrieverMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());
const memory = new VectorStoreRetrieverMemory({
  // 1 is how many documents to return, you might want to return more, eg. 4
  vectorStoreRetriever: vectorStore.asRetriever(1),
  memoryKey: "history",
});

// First let's save some information to memory, as it would happen when
// used inside a chain.
await memory.saveContext(
  { input: "My favorite food is pizza" },
  { output: "thats good to know" }
);
await memory.saveContext(
  { input: "My favorite sport is soccer" },
  { output: "..." }
);
await memory.saveContext({ input: "I don't the Celtics" }, { output: "ok" });

// Now let's use the memory to retrieve the information we saved.
console.log(
  await memory.loadMemoryVariables({ prompt: "what sport should i watch?" })
);
/*
{ history: 'input: My favorite sport is soccer\noutput: ...' }
*/

// Now let's use it in a chain.
const model = new OpenAI({ temperature: 0.9 });
const prompt =
  PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
{history}

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: {input}
AI:`);
const chain = new LLMChain({ llm: model, prompt, memory });

const res1 = await chain.call({ input: "Hi, my name is Perry, what's up?" });
console.log({ res1 });
/*
{
  res1: {
    text: " Hi Perry, I'm doing great! I'm currently exploring different topics related to artificial intelligence like natural language processing and machine learning. What about you? What have you been up to lately?"
  }
}
*/

const res2 = await chain.call({ input: "what's my favorite sport?" });
console.log({ res2 });
/*
{ res2: { text: ' You said your favorite sport is soccer.' } }
*/

const res3 = await chain.call({ input: "what's my name?" });
console.log({ res3 });
/*
{ res3: { text: ' Your name is Perry.' } }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/

# Examples: Memory
## 📄️ DynamoDB-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.
## 📄️ Firestore Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.
## 📄️ Momento-Backed Chat Memory
For distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.
## 📄️ Motörhead Memory
Motörhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.
## 📄️ PlanetScale Chat Memory
Because PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
## 📄️ Redis-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.
## 📄️ Upstash Redis-Backed Chat Memory
Because Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
## 📄️ Zep Memory
Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.



Page URL: https://js.langchain.com/docs/modules/memory/integrations/dynamodb

# DynamoDB-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.
## Setup​
First, install the AWS DynamoDB client in your project:
```typescript
npm install @aws-sdk/client-dynamodb
```
```typescript
yarn add @aws-sdk/client-dynamodb
```
```typescript
pnpm add @aws-sdk/client-dynamodb
```
Next, sign into your AWS account and create a DynamoDB table. Name the table langchain, and name your partition key id. Make sure your partition key is a string. You can leave sort key and the other settings alone.
You'll also need to retrieve an AWS access key and secret key for a role or user that has access to the table and add them to your environment variables.
## Usage​
```typescript
import { BufferMemory } from "langchain/memory";
import { DynamoDBChatMessageHistory } from "langchain/stores/message/dynamodb";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new DynamoDBChatMessageHistory({
    tableName: "langchain",
    partitionKey: "id",
    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation
    config: {
      region: "us-east-2",
      credentials: {
        accessKeyId: "<your AWS access key id>",
        secretAccessKey: "<your AWS secret access key>",
      },
    },
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/firestore

# Firestore Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.
## Setup​
First, install the Firebase admin package in your project:
```typescript
yarn add firebase-admin
```
```typescript
yarn add firebase-admin
```
```typescript
yarn add firebase-admin
```
Go to your the Settings icon Project settings in the Firebase console.
In the Your apps card, select the nickname of the app for which you need a config object.
Select Config from the Firebase SDK snippet pane.
Copy the config object snippet, then add it to your firebase functions FirestoreChatMessageHistory.
## Usage​
```typescript
import { BufferMemory } from "langchain/memory";
import { FirestoreChatMessageHistory } from "langchain/stores/message/firestore";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new FirestoreChatMessageHistory({
    collectionName: "langchain",
    sessionId: "lc-example",
    userId: "a@example.com",
    config: { projectId: "your-project-id" },
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Firestore Rules​
If your collection name is "chathistory," you can configure Firestore rules as follows.
```typescript
      match /chathistory/{sessionId} {
       allow read: if request.auth.uid == resource.data.createdBy;
       allow write: if request.auth.uid == request.resource.data.createdBy;
             }
             match /chathistory/{sessionId}/messages/{messageId} {
       allow read: if request.auth.uid == resource.data.createdBy;
       allow write: if request.auth.uid == request.resource.data.createdBy;
            }
```



Page URL: https://js.langchain.com/docs/modules/memory/integrations/momento

# Momento-Backed Chat Memory
For distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.
Because a Momento cache is instantly available and requires zero infrastructure maintenance, it's a great way to get started with chat history whether building locally or in production.
## Setup​
You will need to install the Momento Client Library in your project:
```typescript
npm install @gomomento/sdk
```
```typescript
yarn add @gomomento/sdk
```
```typescript
pnpm add @gomomento/sdk
```
You will also need an API key from Momento. You can sign up for a free account here.
## Usage​
To distinguish one chat history session from another, we need a unique sessionId. You may also provide an optional sessionTtl to make sessions expire after a given number of seconds.
```typescript
import {
  CacheClient,
  Configurations,
  CredentialProvider,
} from "@gomomento/sdk";
import { BufferMemory } from "langchain/memory";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { MomentoChatMessageHistory } from "langchain/stores/message/momento";

// See https://github.com/momentohq/client-sdk-javascript for connection options
const client = new CacheClient({
  configuration: Configurations.Laptop.v1(),
  credentialProvider: CredentialProvider.fromEnvironmentVariable({
    environmentVariableName: "MOMENTO_AUTH_TOKEN",
  }),
  defaultTtlSeconds: 60 * 60 * 24,
});

// Create a unique session ID
const sessionId = new Date().toISOString();
const cacheName = "langchain";

const memory = new BufferMemory({
  chatHistory: await MomentoChatMessageHistory.fromProps({
    client,
    cacheName,
    sessionId,
    sessionTtl: 300,
  }),
});
console.log(
  `cacheName=${cacheName} and sessionId=${sessionId} . This will be used to store the chat history. You can inspect the values at your Momento console at https://console.gomomento.com.`
);

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/

// See the chat history in the Momento
console.log(await memory.chatHistory.getMessages());
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/motorhead_memory

# Motörhead Memory
Motörhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.
## Setup​
See instructions at Motörhead for running the server locally, or https://getmetal.io to get API keys for the hosted version.
## Usage​
```typescript
import { MotorheadMemory } from "langchain/memory";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

// Managed Example (visit https://getmetal.io to get your keys)
// const managedMemory = new MotorheadMemory({
//   memoryKey: "chat_history",
//   sessionId: "test",
//   apiKey: "MY_API_KEY",
//   clientId: "MY_CLIENT_ID",
// });

// Self Hosted Example
const memory = new MotorheadMemory({
  memoryKey: "chat_history",
  sessionId: "test",
  url: "localhost:8080", // Required for self hosted
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/planetscale

# PlanetScale Chat Memory
Because PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for an PlanetScale Database instance.
## Setup​
You will need to install @planetscale/database in your project:
```typescript
npm install @planetscale/database
```
```typescript
yarn add @planetscale/database
```
```typescript
pnpm add @planetscale/database
```
You will also need an PlanetScale Account and a database to connect to. See instructions on PlanetScale Docs on how to create a HTTP client.
## Usage​
Each chat history session stored in PlanetScale database must have a unique id.
The config parameter is passed directly into the new Client() constructor of @planetscale/database, and takes all the same arguments.
```typescript
import { BufferMemory } from "langchain/memory";
import { PlanetScaleChatMessageHistory } from "langchain/stores/message/planetscale";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new PlanetScaleChatMessageHistory({
    tableName: "stored_message",
    sessionId: "lc-example",
    config: {
      url: "ADD_YOURS_HERE", // Override with your own database instance's URL
    },
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Advanced Usage​
You can also directly pass in a previously created @planetscale/database client instance:
```typescript
import { BufferMemory } from "langchain/memory";
import { PlanetScaleChatMessageHistory } from "langchain/stores/message/planetscale";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { Client } from "@planetscale/database";

// Create your own Planetscale database client
const client = new Client({
  url: "ADD_YOURS_HERE", // Override with your own database instance's URL
});

const memory = new BufferMemory({
  chatHistory: new PlanetScaleChatMessageHistory({
    tableName: "stored_message",
    sessionId: "lc-example",
    client, // You can reuse your existing database client
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/redis

# Redis-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.
## Setup​
You will need to install node-redis in your project:
```typescript
npm install redis
```
```typescript
yarn add redis
```
```typescript
pnpm add redis
```
You will also need a Redis instance to connect to. See instructions on the official Redis website for running the server locally.
## Usage​
Each chat history session stored in Redis must have a unique id. You can provide an optional sessionTTL to make sessions expire after a give number of seconds.
The config parameter is passed directly into the createClient method of node-redis, and takes all the same arguments.
```typescript
import { BufferMemory } from "langchain/memory";
import { RedisChatMessageHistory } from "langchain/stores/message/ioredis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new RedisChatMessageHistory({
    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation
    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire
    url: "redis://localhost:6379", // Default value, override with your own instance's URL
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Advanced Usage​
You can also directly pass in a previously created node-redis client instance:
```typescript
import { Redis } from "ioredis";
import { BufferMemory } from "langchain/memory";
import { RedisChatMessageHistory } from "langchain/stores/message/ioredis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const client = new Redis("redis://localhost:6379");

const memory = new BufferMemory({
  chatHistory: new RedisChatMessageHistory({
    sessionId: new Date().toISOString(),
    sessionTTL: 300,
    client,
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
### Redis Sentinel Support​
You can enable a Redis Sentinel backed cache using ioredis
This will require the installation of ioredis in your project.
```typescript
npm install ioredis
```
```typescript
yarn add ioredis
```
```typescript
pnpm add ioredis
```
```typescript
import { Redis } from "ioredis";
import { BufferMemory } from "langchain/memory";
import { RedisChatMessageHistory } from "langchain/stores/message/ioredis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

// Uses ioredis to facilitate Sentinel Connections see their docs for details on setting up more complex Sentinels: https://github.com/redis/ioredis#sentinel
const client = new Redis({
  sentinels: [
    { host: "localhost", port: 26379 },
    { host: "localhost", port: 26380 },
  ],
  name: "mymaster",
});

const memory = new BufferMemory({
  chatHistory: new RedisChatMessageHistory({
    sessionId: new Date().toISOString(),
    sessionTTL: 300,
    client,
  }),
});

const model = new ChatOpenAI({ temperature: 0.5 });

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/upstash_redis

# Upstash Redis-Backed Chat Memory
Because Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
Based on Redis-Backed Chat Memory.
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for an Upstash Redis instance.
## Setup​
You will need to install @upstash/redis in your project:
```typescript
npm install @upstash/redis
```
```typescript
yarn add @upstash/redis
```
```typescript
pnpm add @upstash/redis
```
You will also need an Upstash Account and a Redis database to connect to. See instructions on Upstash Docs on how to create a HTTP client.
## Usage​
Each chat history session stored in Redis must have a unique id. You can provide an optional sessionTTL to make sessions expire after a give number of seconds.
The config parameter is passed directly into the new Redis() constructor of @upstash/redis, and takes all the same arguments.
```typescript
import { BufferMemory } from "langchain/memory";
import { UpstashRedisChatMessageHistory } from "langchain/stores/message/upstash_redis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new UpstashRedisChatMessageHistory({
    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation
    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire
    config: {
      url: "https://ADD_YOURS_HERE.upstash.io", // Override with your own instance's URL
      token: "********", // Override with your own instance's token
    },
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Advanced Usage​
You can also directly pass in a previously created @upstash/redis client instance:
```typescript
import { Redis } from "@upstash/redis";
import { BufferMemory } from "langchain/memory";
import { UpstashRedisChatMessageHistory } from "langchain/stores/message/upstash_redis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

// Create your own Redis client
const client = new Redis({
  url: "https://ADD_YOURS_HERE.upstash.io",
  token: "********",
});

const memory = new BufferMemory({
  chatHistory: new UpstashRedisChatMessageHistory({
    sessionId: new Date().toISOString(),
    sessionTTL: 300,
    client, // You can reuse your existing Redis client
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/zep_memory

# Zep Memory
Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.
Key Features:
## Setup​
See the instructions from Zep for running the server locally or through an automated hosting provider.
## Usage​
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { ZepMemory } from "langchain/memory/zep";

const sessionId = "TestSession";
const zepURL = "http://localhost:8000";

const memory = new ZepMemory({
  sessionId,
  baseURL: zepURL,
  // This is optional. If you've enabled JWT authentication on your Zep server, you can
  // pass it in here. See https://docs.getzep.com/deployment/auth
  apiKey: "change_this_key",
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });
console.log("Memory Keys:", memory.memoryKeys);

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
console.log("Session ID: ", sessionId);
console.log("Memory: ", await memory.loadMemoryVariables({}));
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/

# Agents
Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.
There are two main types of agents:
Action agents are suitable for small tasks, while plan-and-execute agents are better for complex or long-running tasks that require maintaining long-term objectives and focus. Often the best approach is to combine the dynamism of an action agent with the planning abilities of a plan-and-execute agent by letting the plan-and-execute agent use action agents to execute plans.
For a full list of agent types see agent types. Additional abstractions involved in agents are:
## Action agents​
At a high-level an action agent:
Action agents are wrapped in agent executors, chains which are responsible for calling the agent, getting back an action and action input, calling the tool that the action references with the generated input, getting the output of the tool, and then passing all that information back into the agent to get the next action it should take.
Although an agent can be constructed in many ways, it typically involves these components:
## Plan-and-execute agents​
At a high-level a plan-and-execute agent:
The most typical implementation is to have the planner be a language model, and the executor be an action agent. Read more here.
## Get started​
LangChain offers several types of agents. Here's an example using one powered by OpenAI functions:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const tools = [new Calculator(), new SerpAPI()];
const chat = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });

const executor = await initializeAgentExecutorWithOptions(tools, chat, {
  agentType: "openai-functions",
  verbose: true,
});

const result = await executor.run("What is the weather in New York?");
console.log(result);

/*
  The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.
*/
```
#### API Reference:
And here is the logged verbose output:
```typescript
[chain/start] [1:chain:AgentExecutor] Entering Chain run with input: {
  "input": "What is the weather in New York?",
  "chat_history": []
}
[llm/start] [1:chain:AgentExecutor > 2:llm:ChatOpenAI] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful AI assistant.",
          "additional_kwargs": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "What is the weather in New York?",
          "additional_kwargs": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 2:llm:ChatOpenAI] [1.97s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "AIMessage"
          ],
          "kwargs": {
            "content": "",
            "additional_kwargs": {
              "function_call": {
                "name": "search",
                "arguments": "{\n  \"input\": \"current weather in New York\"\n}"
              }
            }
          }
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 18,
      "promptTokens": 121,
      "totalTokens": 139
    }
  }
}
[agent/action] [1:chain:AgentExecutor] Agent selected action: {
  "tool": "search",
  "toolInput": {
    "input": "current weather in New York"
  },
  "log": ""
}
[tool/start] [1:chain:AgentExecutor > 3:tool:SerpAPI] Entering Tool run with input: "current weather in New York"
[tool/end] [1:chain:AgentExecutor > 3:tool:SerpAPI] [1.90s] Exiting Tool run with output: "1 am · Feels Like72° · WindSSW 1 mph · Humidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in ..."
[llm/start] [1:chain:AgentExecutor > 4:llm:ChatOpenAI] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful AI assistant.",
          "additional_kwargs": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "What is the weather in New York?",
          "additional_kwargs": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "AIMessage"
        ],
        "kwargs": {
          "content": "",
          "additional_kwargs": {
            "function_call": {
              "name": "search",
              "arguments": "{\"input\":\"current weather in New York\"}"
            }
          }
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "FunctionMessage"
        ],
        "kwargs": {
          "content": "1 am · Feels Like72° · WindSSW 1 mph · Humidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in ...",
          "name": "search",
          "additional_kwargs": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 4:llm:ChatOpenAI] [3.33s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "AIMessage"
          ],
          "kwargs": {
            "content": "The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.",
            "additional_kwargs": {}
          }
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 58,
      "promptTokens": 180,
      "totalTokens": 238
    }
  }
}
[chain/end] [1:chain:AgentExecutor] [7.73s] Exiting Chain run with output: {
  "output": "The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain."
}
```



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/

# Agent types
## Action agents​
Agents use an LLM to determine which actions to take and in what order.
An action can either be using a tool and observing its output, or returning a response to the user.
Here are the agents available in LangChain.
### Zero-shot ReAct​
This agent uses the ReAct framework to determine which tool to use
based solely on the tool's description. Any number of tools can be provided.
This agent requires that a description is provided for each tool.
Note: This is the most general purpose action agent.
### OpenAI Functions​
Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to detect when a
function should to be called and respond with the inputs that should be passed to the function.
The OpenAI Functions Agent is designed to work with these models.
### Conversational​
This agent is designed to be used in conversational settings.
The prompt is designed to make the agent helpful and conversational.
It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions.
## Plan-and-execute agents​
Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the "Plan-and-Solve" paper.



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent

# Conversational
This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.
This example covers how to create a conversational agent for a chat model. It will utilize chat specific prompts.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

export const run = async () => {
  process.env.LANGCHAIN_HANDLER = "langchain";
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  // Passing "chat-conversational-react-description" as the agent type
  // automatically creates and uses BufferMemory with the executor.
  // If you would like to override this, you can pass in a custom
  // memory option, but the memoryKey set on it must be "chat_history".
  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "chat-conversational-react-description",
    verbose: true,
  });
  console.log("Loaded agent.");

  const input0 = "hi, i am bob";

  const result0 = await executor.call({ input: input0 });

  console.log(`Got output ${result0.output}`);

  const input1 = "whats my name?";

  const result1 = await executor.call({ input: input1 });

  console.log(`Got output ${result1.output}`);

  const input2 = "whats the weather in pomfret?";

  const result2 = await executor.call({ input: input2 });

  console.log(`Got output ${result2.output}`);
};
```
#### API Reference:
```typescript
Loaded agent.
Entering new agent_executor chain...
{
    "action": "Final Answer",
    "action_input": "Hello Bob! How can I assist you today?"
}
Finished chain.
Got output Hello Bob! How can I assist you today?
Entering new agent_executor chain...
{
    "action": "Final Answer",
    "action_input": "Your name is Bob."
}
Finished chain.
Got output Your name is Bob.
Entering new agent_executor chain...
```json
{
    "action": "search",
    "action_input": "weather in pomfret"
}
```
A steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.
```json
{
    "action": "Final Answer",
    "action_input": "The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%."
}
```
Finished chain.
Got output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.
```



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/openai_functions_agent

# OpenAI functions
Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to detect when a function should to be called and respond with the inputs that should be passed to the function.
In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call those functions.
The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.
The OpenAI Functions Agent is designed to work with these models.
Must be used with an OpenAI Functions model.
This agent also supports StructuredTools with more complex input schemas.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const tools = [new Calculator(), new SerpAPI()];
const chat = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });

const executor = await initializeAgentExecutorWithOptions(tools, chat, {
  agentType: "openai-functions",
  verbose: true,
});

const result = await executor.run("What is the weather in New York?");
console.log(result);

/*
  The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.
*/
```
#### API Reference:
## Prompt customization​
You can pass in a custom string to be used as the system message of the prompt as follows:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const tools = [new Calculator(), new SerpAPI()];
const chat = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });
const prefix =
  "You are a helpful AI assistant. However, all final response to the user must be in pirate dialect.";

const executor = await initializeAgentExecutorWithOptions(tools, chat, {
  agentType: "openai-functions",
  verbose: true,
  agentArgs: {
    prefix,
  },
});

const result = await executor.run("What is the weather in New York?");
console.log(result);

// Arr matey, in New York, it be feelin' like 75 degrees, with a gentle breeze blowin' from the northwest at 3 knots. The air be 77% full o' water, and the clouds be coverin' 35% of the sky. There be no rain in sight, yarr!
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute

# Plan and execute
Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the "Plan-and-Solve" paper.
The planning is almost always done by an LLM.
The execution is usually done by a separate agent (equipped with tools).
This agent uses a two step process:
The idea is that the planning step keeps the LLM more "on track" by breaking up a larger task into simpler subtasks.
However, this method requires more individual LLM queries and has higher latency compared to Action Agents.
Note: This agent currently only supports Chat Models.
```typescript
import { Calculator } from "langchain/tools/calculator";
import { SerpAPI } from "langchain/tools";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PlanAndExecuteAgentExecutor } from "langchain/experimental/plan_and_execute";

const tools = [new Calculator(), new SerpAPI()];
const model = new ChatOpenAI({
  temperature: 0,
  modelName: "gpt-3.5-turbo",
  verbose: true,
});
const executor = PlanAndExecuteAgentExecutor.fromLLMAndTools({
  llm: model,
  tools,
});

const result = await executor.call({
  input: `Who is the current president of the United States? What is their current age raised to the second power?`,
});

console.log({ result });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/react

# ReAct
This walkthrough showcases using an agent to implement the ReAct logic.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

const result = await executor.call({ input });
```
#### API Reference:
## Using chat models​
You can also create ReAct agents that use chat models instead of LLMs as the agent driver.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "chat-zero-shot-react-description",
    returnIntermediateSteps: true,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/structured_chat

# Structured tool chat
The structured tool chat agent is capable of using multi-input tools.
Older agents are configured to specify an action input as a single string, but this agent can use the provided tools' args_schema to populate the action input.
This makes it easier to create and use tools that require multiple input values - rather than prompting for a stringified object or comma separated list, you can specify an object with multiple keys.
Here's an example with a DynamicStructuredTool:
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { Calculator } from "langchain/tools/calculator";
import { DynamicStructuredTool } from "langchain/tools";

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new Calculator(), // Older existing single input tools will still work
    new DynamicStructuredTool({
      name: "random-number-generator",
      description: "generates a random number between two input numbers",
      schema: z.object({
        low: z.number().describe("The lower bound of the generated number"),
        high: z.number().describe("The upper bound of the generated number"),
      }),
      func: async ({ low, high }) =>
        (Math.random() * (high - low) + low).toString(), // Outputs still must be strings
      returnDirect: false, // This is an option that allows the tool to return the output directly
    }),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "structured-chat-zero-shot-react-description",
    verbose: true,
  });
  console.log("Loaded agent.");

  const input = `What is a random number between 5 and 10 raised to the second power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log({ result });

  /*
    {
      "output": "67.95299776074"
    }
  */
};
```
#### API Reference:
## Adding Memory​
You can add memory to this agent like this:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { Calculator } from "langchain/tools/calculator";
import { MessagesPlaceholder } from "langchain/prompts";
import { BufferMemory } from "langchain/memory";

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [new Calculator()];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "structured-chat-zero-shot-react-description",
    verbose: true,
    memory: new BufferMemory({
      memoryKey: "chat_history",
      returnMessages: true,
    }),
    agentArgs: {
      inputVariables: ["input", "agent_scratchpad", "chat_history"],
      memoryPrompts: [new MessagesPlaceholder("chat_history")],
    },
  });

  const result = await executor.call({ input: `what is 9 to the 2nd power?` });

  console.log(result);

  /*
    {
      "output": "81"
    }
  */

  const result2 = await executor.call({
    input: `what is that number squared?`,
  });

  console.log(result2);

  /*
    {
      "output": "6561"
    }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/callbacks

# Subscribing to events
You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.
For more info on the events available see the Callbacks section of the docs.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});

const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;
const result = await executor.run(input, [
  {
    handleAgentAction(action, runId) {
      console.log("\nhandleAgentAction", action, runId);
    },
    handleAgentEnd(action, runId) {
      console.log("\nhandleAgentEnd", action, runId);
    },
    handleToolEnd(output, runId) {
      console.log("\nhandleToolEnd", output, runId);
    },
  },
]);
/*
handleAgentAction {
  tool: 'search',
  toolInput: 'Olivia Wilde boyfriend',
  log: " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n" +
    'Action: search\n' +
    'Action Input: "Olivia Wilde boyfriend"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6

handleToolEnd In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022. 062fef47-8ad1-4729-9949-a57be252e002

handleAgentAction {
  tool: 'search',
  toolInput: 'Harry Styles age',
  log: " I need to find out Harry Styles' age.\n" +
    'Action: search\n' +
    'Action Input: "Harry Styles age"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6

handleToolEnd 29 years 9ec91e41-2fbf-4de0-85b6-12b3e6b3784e 61d77e10-c119-435d-a985-1f9d45f0ef08

handleAgentAction {
  tool: 'calculator',
  toolInput: '29^0.23',
  log: ' I need to calculate 29 raised to the 0.23 power.\n' +
    'Action: calculator\n' +
    'Action Input: 29^0.23'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6

handleToolEnd 2.169459462491557 07aec96a-ce19-4425-b863-2eae39db8199

handleAgentEnd {
  returnValues: {
    output: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
  },
  log: ' I now know the final answer.\n' +
    "Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
*/

console.log({ result });
// { result: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557." }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/cancelling_requests

# Cancelling requests
You can cancel a request by passing a signal option when you run the agent. For example:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.
setTimeout(() => {
  controller.abort();
}, 2000);

try {
  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;
  const result = await executor.call({ input, signal: controller.signal });
} catch (e) {
  console.log(e);
  /*
  Error: Cancel: canceled
      at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
      at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
    attemptNumber: 1,
    retriesLeft: 6
  }
  */
}
```
#### API Reference:
Note, this will only cancel the outgoing request if the underlying provider exposes that option. LangChain will cancel the underlying request if possible, otherwise it will cancel the processing of the response.



Page URL: https://js.langchain.com/docs/modules/agents/how_to/custom_llm_agent

# Custom LLM Agent
This notebook goes through how to create your own custom LLM agent.
An LLM agent consists of three parts:
The LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:
AgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).
AgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.
```typescript
import {
  LLMSingleActionAgent,
  AgentActionOutputParser,
  AgentExecutor,
} from "langchain/agents";
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import {
  BasePromptTemplate,
  BaseStringPromptTemplate,
  SerializedBasePromptTemplate,
  renderTemplate,
} from "langchain/prompts";
import {
  InputValues,
  PartialValues,
  AgentStep,
  AgentAction,
  AgentFinish,
} from "langchain/schema";
import { SerpAPI, Tool } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;
const formatInstructions = (
  toolNames: string
) => `Use the following format in your response:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [${toolNames}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question`;
const SUFFIX = `Begin!

Question: {input}
Thought:{agent_scratchpad}`;

class CustomPromptTemplate extends BaseStringPromptTemplate {
  tools: Tool[];

  constructor(args: { tools: Tool[]; inputVariables: string[] }) {
    super({ inputVariables: args.inputVariables });
    this.tools = args.tools;
  }

  _getPromptType(): string {
    throw new Error("Not implemented");
  }

  format(input: InputValues): Promise<string> {
    /** Construct the final template */
    const toolStrings = this.tools
      .map((tool) => `${tool.name}: ${tool.description}`)
      .join("\n");
    const toolNames = this.tools.map((tool) => tool.name).join("\n");
    const instructions = formatInstructions(toolNames);
    const template = [PREFIX, toolStrings, instructions, SUFFIX].join("\n\n");
    /** Construct the agent_scratchpad */
    const intermediateSteps = input.intermediate_steps as AgentStep[];
    const agentScratchpad = intermediateSteps.reduce(
      (thoughts, { action, observation }) =>
        thoughts +
        [action.log, `\nObservation: ${observation}`, "Thought:"].join("\n"),
      ""
    );
    const newInput = { agent_scratchpad: agentScratchpad, ...input };
    /** Format the template. */
    return Promise.resolve(renderTemplate(template, "f-string", newInput));
  }

  partial(_values: PartialValues): Promise<BasePromptTemplate> {
    throw new Error("Not implemented");
  }

  serialize(): SerializedBasePromptTemplate {
    throw new Error("Not implemented");
  }
}

class CustomOutputParser extends AgentActionOutputParser {
  lc_namespace = ["langchain", "agents", "custom_llm_agent"];

  async parse(text: string): Promise<AgentAction | AgentFinish> {
    if (text.includes("Final Answer:")) {
      const parts = text.split("Final Answer:");
      const input = parts[parts.length - 1].trim();
      const finalAnswers = { output: input };
      return { log: text, returnValues: finalAnswers };
    }

    const match = /Action: (.*)\nAction Input: (.*)/s.exec(text);
    if (!match) {
      throw new Error(`Could not parse LLM output: ${text}`);
    }

    return {
      tool: match[1].trim(),
      toolInput: match[2].trim().replace(/^"+|"+$/g, ""),
      log: text,
    };
  }

  getFormatInstructions(): string {
    throw new Error("Not implemented");
  }
}

export const run = async () => {
  const model = new OpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const llmChain = new LLMChain({
    prompt: new CustomPromptTemplate({
      tools,
      inputVariables: ["input", "agent_scratchpad"],
    }),
    llm: model,
  });

  const agent = new LLMSingleActionAgent({
    llmChain,
    outputParser: new CustomOutputParser(),
    stop: ["\nObservation"],
  });
  const executor = new AgentExecutor({
    agent,
    tools,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/custom_llm_chat_agent

# Custom LLM Agent (with a ChatModel)
This notebook goes through how to create your own custom agent based on a chat model.
An LLM chat agent consists of three parts:
The LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:
AgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).
AgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.
```typescript
import {
  AgentActionOutputParser,
  AgentExecutor,
  LLMSingleActionAgent,
} from "langchain/agents";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  BaseChatPromptTemplate,
  BasePromptTemplate,
  SerializedBasePromptTemplate,
  renderTemplate,
} from "langchain/prompts";
import {
  AgentAction,
  AgentFinish,
  AgentStep,
  BaseMessage,
  HumanMessage,
  InputValues,
  PartialValues,
} from "langchain/schema";
import { SerpAPI, Tool } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;
const formatInstructions = (
  toolNames: string
) => `Use the following format in your response:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [${toolNames}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question`;
const SUFFIX = `Begin!

Question: {input}
Thought:{agent_scratchpad}`;

class CustomPromptTemplate extends BaseChatPromptTemplate {
  tools: Tool[];

  constructor(args: { tools: Tool[]; inputVariables: string[] }) {
    super({ inputVariables: args.inputVariables });
    this.tools = args.tools;
  }

  _getPromptType(): string {
    throw new Error("Not implemented");
  }

  async formatMessages(values: InputValues): Promise<BaseMessage[]> {
    /** Construct the final template */
    const toolStrings = this.tools
      .map((tool) => `${tool.name}: ${tool.description}`)
      .join("\n");
    const toolNames = this.tools.map((tool) => tool.name).join("\n");
    const instructions = formatInstructions(toolNames);
    const template = [PREFIX, toolStrings, instructions, SUFFIX].join("\n\n");
    /** Construct the agent_scratchpad */
    const intermediateSteps = values.intermediate_steps as AgentStep[];
    const agentScratchpad = intermediateSteps.reduce(
      (thoughts, { action, observation }) =>
        thoughts +
        [action.log, `\nObservation: ${observation}`, "Thought:"].join("\n"),
      ""
    );
    const newInput = { agent_scratchpad: agentScratchpad, ...values };
    /** Format the template. */
    const formatted = renderTemplate(template, "f-string", newInput);
    return [new HumanMessage(formatted)];
  }

  partial(_values: PartialValues): Promise<BasePromptTemplate> {
    throw new Error("Not implemented");
  }

  serialize(): SerializedBasePromptTemplate {
    throw new Error("Not implemented");
  }
}

class CustomOutputParser extends AgentActionOutputParser {
  lc_namespace = ["langchain", "agents", "custom_llm_agent_chat"];

  async parse(text: string): Promise<AgentAction | AgentFinish> {
    if (text.includes("Final Answer:")) {
      const parts = text.split("Final Answer:");
      const input = parts[parts.length - 1].trim();
      const finalAnswers = { output: input };
      return { log: text, returnValues: finalAnswers };
    }

    const match = /Action: (.*)\nAction Input: (.*)/s.exec(text);
    if (!match) {
      throw new Error(`Could not parse LLM output: ${text}`);
    }

    return {
      tool: match[1].trim(),
      toolInput: match[2].trim().replace(/^"+|"+$/g, ""),
      log: text,
    };
  }

  getFormatInstructions(): string {
    throw new Error("Not implemented");
  }
}

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const llmChain = new LLMChain({
    prompt: new CustomPromptTemplate({
      tools,
      inputVariables: ["input", "agent_scratchpad"],
    }),
    llm: model,
  });

  const agent = new LLMSingleActionAgent({
    llmChain,
    outputParser: new CustomOutputParser(),
    stop: ["\nObservation"],
  });
  const executor = new AgentExecutor({
    agent,
    tools,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);
};
run();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/logging_and_tracing

# Logging and tracing
You can pass the verbose flag when creating an agent to enable logging of all events to the console. For example:
You can also enable tracing by setting the LANGCHAIN_TRACING environment variable to true.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

const result = await executor.call({ input });
```
#### API Reference:
```typescript
[chain/start] [1:chain:agent_executor] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
}
[chain/start] [1:chain:agent_executor > 2:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": "",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] [3.52s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 39,
      "promptTokens": 220,
      "totalTokens": 259
    }
  }
}
[chain/end] [1:chain:agent_executor > 2:chain:llm_chain] [3.53s] Exiting Chain run with output: {
  "text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
  "tool": "search",
  "toolInput": "Olivia Wilde boyfriend",
  "log": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\""
}
[tool/start] [1:chain:agent_executor > 4:tool:search] Entering Tool run with input: "Olivia Wilde boyfriend"
[tool/end] [1:chain:agent_executor > 4:tool:search] [845ms] Exiting Tool run with output: "In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022."
[chain/start] [1:chain:agent_executor > 5:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought:",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] [3.65s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 23,
      "promptTokens": 296,
      "totalTokens": 319
    }
  }
}
[chain/end] [1:chain:agent_executor > 5:chain:llm_chain] [3.65s] Exiting Chain run with output: {
  "text": " I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
  "tool": "search",
  "toolInput": "Harry Styles age",
  "log": " I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\""
}
[tool/start] [1:chain:agent_executor > 7:tool:search] Entering Tool run with input: "Harry Styles age"
[tool/end] [1:chain:agent_executor > 7:tool:search] [632ms] Exiting Tool run with output: "29 years"
[chain/start] [1:chain:agent_executor > 8:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought:",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] [2.72s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 26,
      "promptTokens": 329,
      "totalTokens": 355
    }
  }
}
[chain/end] [1:chain:agent_executor > 8:chain:llm_chain] [2.72s] Exiting Chain run with output: {
  "text": " I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23"
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
  "tool": "calculator",
  "toolInput": "29^0.23",
  "log": " I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23"
}
[tool/start] [1:chain:agent_executor > 10:tool:calculator] Entering Tool run with input: "29^0.23"
[tool/end] [1:chain:agent_executor > 10:tool:calculator] [3ms] Exiting Tool run with output: "2.169459462491557"
[chain/start] [1:chain:agent_executor > 11:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought: I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23\nObservation: 2.169459462491557\nThought:",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought: I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23\nObservation: 2.169459462491557\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] [3.51s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I now know the final answer.\nFinal Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 39,
      "promptTokens": 371,
      "totalTokens": 410
    }
  }
}
[chain/end] [1:chain:agent_executor > 11:chain:llm_chain] [3.51s] Exiting Chain run with output: {
  "text": " I now know the final answer.\nFinal Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}
[chain/end] [1:chain:agent_executor] [14.90s] Exiting Chain run with output: {
  "output": "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}
```



Page URL: https://js.langchain.com/docs/modules/agents/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout to an agent, you can pass a timeout option, when you run the agent. For example:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});

try {
  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;
  const result = await executor.call({ input, timeout: 2000 }); // 2 seconds
} catch (e) {
  console.log(e);
  /*
  Error: Cancel: canceled
      at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
      at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
    attemptNumber: 1,
    retriesLeft: 6
  }
  */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/tools/

# Tools
Tools are interfaces that an agent can use to interact with the world.
## Get started​
Tools are functions that agents can use to interact with the world.
These tools can be generic utilities (e.g. search), other chains, or even other agents.
Specifically, the interface of a tool has a single text input and a single text output. It includes a name and description that communicate to the model what the tool does and when to use it.
```typescript
interface Tool {
  call(arg: string): Promise<string>;

  name: string;

  description: string;
}
```
## Advanced​
To implement your own tool you can subclass the Tool class and implement the _call method. The _call method is called with the input text and should return the output text. The Tool superclass implements the call method, which takes care of calling the right CallbackManager methods before and after calling your _call method. When an error occurs, the _call method should when possible return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.
```typescript
abstract class Tool {
  abstract _call(arg: string): Promise<string>;

  abstract name: string;

  abstract description: string;
}
```



Page URL: https://js.langchain.com/docs/modules/agents/tools/how_to/agents_with_vectorstores

# Vector stores as tools
This notebook covers how to combine agents and vector stores. The use case for this is that you’ve ingested your data into a vector store and want to interact with it in an agentic manner.
The recommended method for doing so is to create a VectorDBQAChain and then use that as a tool in the overall agent. Let’s take a look at doing this below. You can do this with multiple different vector databases, and use the agent as a way to choose between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set returnDirect: true to just use the agent as a router.
First, you'll want to import the relevant modules:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI, ChainTool } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";
import { VectorDBQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";
```
Next, you'll want to create the vector store with your data, and then the QA chain to interact with that vector store.
```typescript
const model = new OpenAI({ temperature: 0 });
/* Load in the file we want to do question answering over */
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
/* Split the text into chunks */
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);
/* Create the vectorstore */
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
/* Create the chain */
const chain = VectorDBQAChain.fromLLM(model, vectorStore);
```
Now that you have that chain, you can create a tool to use that chain. Note that you should update the name and description to be specific to your QA chain.
```typescript
const qaTool = new ChainTool({
  name: "state-of-union-qa",
  description:
    "State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.",
  chain: chain,
});
```
Now you can construct and using the tool just as you would any other!
```typescript
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
  qaTool,
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});
console.log("Loaded agent.");

const input = `What did biden say about ketanji brown jackson is the state of the union address?`;

console.log(`Executing with input "${input}"...`);

const result = await executor.call({ input });

console.log(`Got output ${result.output}`);
```
You can also set returnDirect: true if you intend to use the agent as a router and just want to directly return the result of the VectorDBQAChain.
```typescript
const qaTool = new ChainTool({
  name: "state-of-union-qa",
  description:
    "State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.",
  chain: chain,
  returnDirect: true,
});
```



Page URL: https://js.langchain.com/docs/modules/agents/tools/how_to/dynamic

# Custom tools
One option for creating a tool that runs custom code is to use a DynamicTool.
The DynamicTool class takes as input a name, a description, and a function.
Importantly, the name and the description will be used by the language model to determine when to call this function and with what parameters,
so make sure to set these to some values the language model can reason about!
The provided function is what will the agent will actually call. When an error occurs, the function should, when possible, return a string representing an error, rather than throwing an error.
This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown, then execution of the agent will stop.
See below for an example of defining and using DynamicTools.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { DynamicTool } from "langchain/tools";

export const run = async () => {
  const model = new OpenAI({ temperature: 0 });
  const tools = [
    new DynamicTool({
      name: "FOO",
      description:
        "call this to get the value of foo. input should be an empty string.",
      func: async () => "baz",
    }),
    new DynamicTool({
      name: "BAR",
      description:
        "call this to get the value of bar. input should be an empty string.",
      func: async () => "baz1",
    }),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "zero-shot-react-description",
  });

  console.log("Loaded agent.");

  const input = `What is the value of foo?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);
};
```



Page URL: https://js.langchain.com/docs/modules/agents/tools/integrations/aiplugin-tool

# ChatGPT Plugins
This example shows how to use ChatGPT Plugins within LangChain abstractions.
Note 1: This currently only works for plugins with no auth.
Note 2: There are almost certainly other ways to do this, this is just a first pass. If you have better ideas, please open a PR!
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import {
  RequestsGetTool,
  RequestsPostTool,
  AIPluginTool,
} from "langchain/tools";

export const run = async () => {
  const tools = [
    new RequestsGetTool(),
    new RequestsPostTool(),
    await AIPluginTool.fromPluginUrl(
      "https://www.klarna.com/.well-known/ai-plugin.json"
    ),
  ];
  const agent = await initializeAgentExecutorWithOptions(
    tools,
    new ChatOpenAI({ temperature: 0 }),
    { agentType: "chat-zero-shot-react-description", verbose: true }
  );

  const result = await agent.call({
    input: "what t shirts are available in klarna?",
  });

  console.log({ result });
};
```
#### API Reference:
```typescript
Entering new agent_executor chain...
Thought: Klarna is a payment provider, not a store. I need to check if there is a Klarna Shopping API that I can use to search for t-shirts.
Action:
```

{
"action": "KlarnaProducts",
"action_input": ""
}

```

Usage Guide: Use the Klarna plugin to get relevant product suggestions for any shopping or researching purpose. The query to be sent should not include stopwords like articles, prepositions and determinants. The api works best when searching for words that are related to products, like their name, brand, model or category. Links will always be returned and should be shown to the user.

OpenAPI Spec: {"openapi":"3.0.1","info":{"version":"v0","title":"Open AI Klarna product Api"},"servers":[{"url":"https://www.klarna.com/us/shopping"}],"tags":[{"name":"open-ai-product-endpoint","description":"Open AI Product Endpoint. Query for products."}],"paths":{"/public/openai/v0/products":{"get":{"tags":["open-ai-product-endpoint"],"summary":"API for fetching Klarna product information","operationId":"productsUsingGET","parameters":[{"name":"q","in":"query","description":"query, must be between 2 and 100 characters","required":true,"schema":{"type":"string"}},{"name":"size","in":"query","description":"number of products returned","required":false,"schema":{"type":"integer"}},{"name":"budget","in":"query","description":"maximum price of the matching product in local currency, filters results","required":false,"schema":{"type":"integer"}}],"responses":{"200":{"description":"Products found","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ProductResponse"}}}},"503":{"description":"one or more services are unavailable"}},"deprecated":false}}},"components":{"schemas":{"Product":{"type":"object","properties":{"attributes":{"type":"array","items":{"type":"string"}},"name":{"type":"string"},"price":{"type":"string"},"url":{"type":"string"}},"title":"Product"},"ProductResponse":{"type":"object","properties":{"products":{"type":"array","items":{"$ref":"#/components/schemas/Product"}}},"title":"ProductResponse"}}}}
Now that I know there is a Klarna Shopping API, I can use it to search for t-shirts. I will make a GET request to the API with the query parameter "t-shirt".
Action:
```

{
"action": "requests_get",
"action_input": "https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt"
}

```


{"products":[{"name":"Psycho Bunny Mens Copa Gradient Logo Graphic Tee","url":"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai","price":"$35.00","attributes":["Material:Cotton","Target Group:Man","Color:White,Blue,Black,Orange"]},{"name":"T-shirt","url":"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai","price":"$20.45","attributes":["Material:Cotton","Target Group:Man","Color:Gray,White,Blue,Black,Orange"]},{"name":"Palm Angels Bear T-shirt - Black","url":"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai","price":"$168.36","attributes":["Material:Cotton","Target Group:Man","Color:Black"]},{"name":"Tommy Hilfiger Essential Flag Logo T-shirt","url":"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai","price":"$22.52","attributes":["Material:Cotton","Target Group:Man","Color:Red,Gray,White,Blue,Black","Pattern:Solid Color","Environmental Attributes :Organic"]},{"name":"Coach Outlet Signature T Shirt","url":"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai","price":"$75.00","attributes":["Material:Cotton","Target Group:Man","Color:Gray"]}]}
Finished chain.
{
  result: {
    output: 'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.',
    intermediateSteps: [ [Object], [Object] ]
  }
}
```



Page URL: https://js.langchain.com/docs/modules/agents/tools/integrations/lambda_agent

# Agent with AWS Lambda Integration
Full docs here: https://docs.aws.amazon.com/lambda/index.html
AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS), designed to allow developers to build and run applications and services without the need for provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.
By including a AWSLambda in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.
When an Agent uses the AWSLambda tool, it will provide an argument of type string which will in turn be passed into the Lambda function via the event parameter.
This quick start will demonstrate how an Agent could use a Lambda function to send an email via Amazon Simple Email Service. The lambda code which sends the email is not provided, but if you'd like to learn how this could be done, see here. Keep in mind this is an intentionally simple example; Lambda can used to execute code for a near infinite number of other purposes (including executing more Langchains)!
### Note about credentials:​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { AWSLambda } from "langchain/tools/aws_lambda";
import { initializeAgentExecutorWithOptions } from "langchain/agents";

const model = new OpenAI({ temperature: 0 });
const emailSenderTool = new AWSLambda({
  name: "email-sender",
  // tell the Agent precisely what the tool does
  description:
    "Sends an email with the specified content to testing123@gmail.com",
  region: "us-east-1", // optional: AWS region in which the function is deployed
  accessKeyId: "abc123", // optional: access key id for a IAM user with invoke permissions
  secretAccessKey: "xyz456", // optional: secret access key for that IAM user
  functionName: "SendEmailViaSES", // the function name as seen in AWS Console
});
const tools = [emailSenderTool, new SerpAPI("api_key_goes_here")];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});

const input = `Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.`;
const result = await executor.call({ input });
console.log(result);
```



Page URL: https://js.langchain.com/docs/modules/agents/tools/integrations/webbrowser

# Web Browser Tool
The Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as
```typescript
useful for when you need to find something on or summarize a webpage. input should be a comma separated list of "valid URL including protocol","what you want to find on the page or empty string for a summary".
```
It exposes two modes of operation:
## Setup​
To use the Webbrowser Tool you need to install the dependencies:
```typescript
npm install cheerio axios
```
```typescript
yarn add cheerio axios
```
```typescript
pnpm add cheerio axios
```
## Usage, standalone​
```typescript
import { WebBrowser } from "langchain/tools/webbrowser";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // this will not work with Azure OpenAI API yet
  // Azure OpenAI API does not support embedding with multiple inputs yet
  // Too many inputs. The max number of inputs is 1.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 for further questions.
  // So we will fail fast, when Azure OpenAI API is used
  if (process.env.AZURE_OPENAI_API_KEY) {
    throw new Error(
      "Azure OpenAI API does not support embedding with multiple inputs yet"
    );
  }

  const model = new ChatOpenAI({ temperature: 0 });
  const embeddings = new OpenAIEmbeddings(
    process.env.AZURE_OPENAI_API_KEY
      ? { azureOpenAIApiDeploymentName: "Embeddings2" }
      : {}
  );

  const browser = new WebBrowser({ model, embeddings });

  const result = await browser.call(
    `"https://www.themarginalian.org/2015/04/09/find-your-bliss-joseph-campbell-power-of-myth","who is joseph campbell"`
  );

  console.log(result);
  /*
  Joseph Campbell was a mythologist and writer who discussed spirituality, psychological archetypes, cultural myths, and the mythology of self. He sat down with Bill Moyers for a lengthy conversation at George Lucas’s Skywalker Ranch in California, which continued the following year at the American Museum of Natural History in New York. The resulting 24 hours of raw footage were edited down to six one-hour episodes and broadcast on PBS in 1988, shortly after Campbell’s death, in what became one of the most popular in the history of public television.

  Relevant Links:
  - [The Holstee Manifesto](http://holstee.com/manifesto-bp)
  - [The Silent Music of the Mind: Remembering Oliver Sacks](https://www.themarginalian.org/2015/08/31/remembering-oliver-sacks)
  - [Joseph Campbell series](http://billmoyers.com/spotlight/download-joseph-campbell-and-the-power-of-myth-audio/)
  - [Bill Moyers](https://www.themarginalian.org/tag/bill-moyers/)
  - [books](https://www.themarginalian.org/tag/books/)
  */
}
```
#### API Reference:
## Usage, in an Agent​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";
import { WebBrowser } from "langchain/tools/webbrowser";

export const run = async () => {
  const model = new OpenAI({ temperature: 0 });
  const embeddings = new OpenAIEmbeddings();
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
    new WebBrowser({ model, embeddings }),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "zero-shot-react-description",
    verbose: true,
  });
  console.log("Loaded agent.");

  const input = `What is the word of the day on merriam webster. What is the top result on google for that word`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });
  /*
  Entering new agent_executor chain...
  I need to find the word of the day on Merriam Webster and then search for it on Google
  Action: web-browser
  Action Input: "https://www.merriam-webster.com/word-of-the-day", ""


  Summary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is "lackadaisical", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.

  Relevant Links: 
  - [Test Your Vocabulary](https://www.merriam-webster.com/games)
  - [Thesaurus](https://www.merriam-webster.com/thesaurus)
  - [Word Finder](https://www.merriam-webster.com/wordfinder)
  - [Word of the Day](https://www.merriam-webster.com/word-of-the-day)
  - [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=
  I now need to search for the word of the day on Google
  Action: search
  Action Input: "lackadaisical"
  lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...
  Finished chain.
  */

  console.log(`Got output ${JSON.stringify(result, null, 2)}`);
  /*
  Got output {
    "output": "The word of the day on Merriam Webster is \"lackadaisical\", which implies a carefree indifference marked by half-hearted efforts.",
    "intermediateSteps": [
      {
        "action": {
          "tool": "web-browser",
          "toolInput": "https://www.merriam-webster.com/word-of-the-day\", ",
          "log": " I need to find the word of the day on Merriam Webster and then search for it on Google\nAction: web-browser\nAction Input: \"https://www.merriam-webster.com/word-of-the-day\", \"\""
        },
        "observation": "\n\nSummary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \"lackadaisical\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\n\nRelevant Links: \n- [Test Your Vocabulary](https://www.merriam-webster.com/games)\n- [Thesaurus](https://www.merriam-webster.com/thesaurus)\n- [Word Finder](https://www.merriam-webster.com/wordfinder)\n- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\n- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content="
      },
      {
        "action": {
          "tool": "search",
          "toolInput": "lackadaisical",
          "log": " I now need to search for the word of the day on Google\nAction: search\nAction Input: \"lackadaisical\""
        },
        "observation": "lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ..."
      }
    ]
  }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/tools/integrations/zapier_agent

# Agent with Zapier NLA Integration
Full docs here: https://nla.zapier.com/start/
Zapier Natural Language Actions gives you access to the 5k+ apps and 20k+ actions on Zapier's platform through a natural language API interface.
NLA supports apps like Gmail, Salesforce, Trello, Slack, Asana, HubSpot, Google Sheets, Microsoft Teams, and thousands more apps: https://zapier.com/apps
Zapier NLA handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.
NLA offers both API Key and OAuth for signing NLA API requests.
Server-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)
User-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com
Attach NLA credentials via either an environment variable (ZAPIER_NLA_OAUTH_ACCESS_TOKEN or ZAPIER_NLA_API_KEY) or refer to the params argument in the API reference for ZapierNLAWrapper.
Review auth docs for more details.
The example below demonstrates how to use the Zapier integration as an Agent:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ZapierNLAWrapper } from "langchain/tools";
import {
  initializeAgentExecutorWithOptions,
  ZapierToolKit,
} from "langchain/agents";

const model = new OpenAI({ temperature: 0 });
const zapier = new ZapierNLAWrapper();
const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);

const executor = await initializeAgentExecutorWithOptions(
  toolkit.tools,
  model,
  {
    agentType: "zero-shot-react-description",
    verbose: true,
  }
);
console.log("Loaded agent.");

const input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;

console.log(`Executing with input "${input}"...`);

const result = await executor.call({ input });

console.log(`Got output ${result.output}`);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/toolkits/

# Toolkits
Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.
## 📄️ JSON Agent Toolkit
This example shows how to load and use an agent with a JSON toolkit.
## 📄️ OpenAPI Agent Toolkit
This example shows how to load and use an agent with a OpenAPI toolkit.
## 📄️ AWS Step Functions Toolkit
AWS Step Functions are a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.
## 📄️ SQL Agent Toolkit
This example shows how to load and use an agent with a SQL toolkit.
## 📄️ VectorStore Agent Toolkit
This example shows how to load and use an agent with a vectorstore toolkit.



Page URL: https://js.langchain.com/docs/modules/agents/toolkits/json

# JSON Agent Toolkit
This example shows how to load and use an agent with a JSON toolkit.
```typescript
import * as fs from "fs";
import * as yaml from "js-yaml";
import { OpenAI } from "langchain/llms/openai";
import { JsonSpec, JsonObject } from "langchain/tools";
import { JsonToolkit, createJsonAgent } from "langchain/agents";

export const run = async () => {
  let data: JsonObject;
  try {
    const yamlFile = fs.readFileSync("openai_openapi.yaml", "utf8");
    data = yaml.load(yamlFile) as JsonObject;
    if (!data) {
      throw new Error("Failed to load OpenAPI spec");
    }
  } catch (e) {
    console.error(e);
    return;
  }

  const toolkit = new JsonToolkit(new JsonSpec(data));
  const model = new OpenAI({ temperature: 0 });
  const executor = createJsonAgent(model, toolkit);

  const input = `What are the required parameters in the request body to the /completions endpoint?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );
};
```



Page URL: https://js.langchain.com/docs/modules/agents/toolkits/openapi

# OpenAPI Agent Toolkit
This example shows how to load and use an agent with a OpenAPI toolkit.
```typescript
import * as fs from "fs";
import * as yaml from "js-yaml";
import { OpenAI } from "langchain/llms/openai";
import { JsonSpec, JsonObject } from "langchain/tools";
import { createOpenApiAgent, OpenApiToolkit } from "langchain/agents";

export const run = async () => {
  let data: JsonObject;
  try {
    const yamlFile = fs.readFileSync("openai_openapi.yaml", "utf8");
    data = yaml.load(yamlFile) as JsonObject;
    if (!data) {
      throw new Error("Failed to load OpenAPI spec");
    }
  } catch (e) {
    console.error(e);
    return;
  }

  const headers = {
    "Content-Type": "application/json",
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
  };
  const model = new OpenAI({ temperature: 0 });
  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);
  const executor = createOpenApiAgent(model, toolkit);

  const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;
  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });
  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );
};
```



Page URL: https://js.langchain.com/docs/modules/agents/toolkits/sfn_agent

# AWS Step Functions Toolkit
AWS Step Functions are a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.
By including a AWSSfn tool in the list of tools provided to an Agent, you can grant your Agent the ability to invoke async workflows running in your AWS Cloud.
When an Agent uses the AWSSfn tool, it will provide an argument of type string which will in turn be passed into one of the supported actions this tool supports. The supported actions are: StartExecution, DescribeExecution, and SendTaskSuccess.
## Setup​
You'll need to install the Node AWS Step Functions SDK:
```typescript
npm install @aws-sdk/client-sfn
```
```typescript
yarn add @aws-sdk/client-sfn
```
```typescript
pnpm add @aws-sdk/client-sfn
```
## Usage​
### Note about credentials:​
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  createAWSSfnAgent,
  AWSSfnToolkit,
} from "langchain/agents/toolkits/aws_sfn";

const _EXAMPLE_STATE_MACHINE_ASL = `
{
  "Comment": "A simple example of the Amazon States Language to define a state machine for new client onboarding.",
  "StartAt": "OnboardNewClient",
  "States": {
    "OnboardNewClient": {
      "Type": "Pass",
      "Result": "Client onboarded!",
      "End": true
    }
  }
}`;

/**
 * This example uses a deployed AWS Step Function state machine with the above Amazon State Language (ASL) definition.
 * You can test by provisioning a state machine using the above ASL within your AWS environment, or you can use a tool like LocalStack
 * to mock AWS services locally. See https://localstack.cloud/ for more information.
 */
export const run = async () => {
  const model = new OpenAI({ temperature: 0 });
  const toolkit = new AWSSfnToolkit({
    name: "onboard-new-client-workflow",
    description:
      "Onboard new client workflow. Can also be used to get status of any excuting workflow or state machine.",
    stateMachineArn:
      "arn:aws:states:us-east-1:1234567890:stateMachine:my-state-machine", // Update with your state machine ARN accordingly
    region: "<your Sfn's region>",
    accessKeyId: "<your access key id>",
    secretAccessKey: "<your secret access key>",
  });
  const executor = createAWSSfnAgent(model, toolkit);

  const input = `Onboard john doe (john@example.com) as a new client.`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/toolkits/sql

# SQL Agent Toolkit
This example shows how to load and use an agent with a SQL toolkit.
## Setup​
You'll need to first install typeorm:
```typescript
npm install typeorm
```
```typescript
yarn add typeorm
```
```typescript
pnpm add typeorm
```
## Usage​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { createSqlAgent, SqlToolkit } from "langchain/agents/toolkits/sql";
import { DataSource } from "typeorm";

/** This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
export const run = async () => {
  const datasource = new DataSource({
    type: "sqlite",
    database: "Chinook.db",
  });
  const db = await SqlDatabase.fromDataSourceParams({
    appDataSource: datasource,
  });
  const model = new OpenAI({ temperature: 0 });
  const toolkit = new SqlToolkit(db, model);
  const executor = createSqlAgent(model, toolkit);

  const input = `List the total sales per country. Which country's customers spent the most?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );

  await datasource.destroy();
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/toolkits/vectorstore

# VectorStore Agent Toolkit
This example shows how to load and use an agent with a vectorstore toolkit.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";
import {
  VectorStoreToolkit,
  createVectorStoreAgent,
  VectorStoreInfo,
} from "langchain/agents";

const model = new OpenAI({ temperature: 0 });
/* Load in the file we want to do question answering over */
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
/* Split the text into chunks using character, not token, size */
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);
/* Create the vectorstore */
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

/* Create the agent */
const vectorStoreInfo: VectorStoreInfo = {
  name: "state_of_union_address",
  description: "the most recent state of the Union address",
  vectorStore,
};

const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);
const agent = createVectorStoreAgent(model, toolkit);

const input =
  "What did biden say about Ketanji Brown Jackson is the state of the union address?";
console.log(`Executing: ${input}`);

const result = await agent.call({ input });
console.log(`Got output ${result.output}`);
console.log(
  `Got intermediate steps ${JSON.stringify(result.intermediateSteps, null, 2)}`
);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/callbacks/

# Callbacks
LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.
You can subscribe to these events by using the callbacks argument available throughout the API. This method accepts a list of handler objects, which are expected to implement one or more of the methods described in the API docs.
## How to use callbacks​
The callbacks argument is available on most objects throughout the API (Chains, Language Models, Tools, Agents, etc.) in two different places.
### Constructor callbacks​
Defined in the constructor, eg. new LLMChain({ callbacks: [handler] }), which will be used for all calls made on that object, and will be scoped to that object only, eg. if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain.
```typescript
import { ConsoleCallbackHandler } from "langchain/callbacks";
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  temperature: 0,
  // These tags will be attached to all calls made with this LLM.
  tags: ["example", "callbacks", "constructor"],
  // This handler will be used for all calls made with this LLM.
  callbacks: [new ConsoleCallbackHandler()],
});
```
#### API Reference:
### Request callbacks​
Defined in the call()/run()/apply() methods used for issuing a request, eg. chain.call({ input: '...' }, [handler]), which will be used for that specific request only, and all sub-requests that it contains (eg. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method).
```typescript
import { ConsoleCallbackHandler } from "langchain/callbacks";
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  temperature: 0,
});

const response = await llm.call("1 + 1 =", {
  // These tags will be attached only to this call to the LLM.
  tags: ["example", "callbacks", "request"],
  // This handler will be used only for this call.
  callbacks: [new ConsoleCallbackHandler()],
});
```
#### API Reference:
### Verbose mode​
The verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, eg. new LLMChain({ verbose: true }), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console. You can also enable verbose mode for the entire application by setting the environment variable LANGCHAIN_VERBOSE=true.
```typescript
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";

const chain = new LLMChain({
  llm: new OpenAI({ temperature: 0 }),
  prompt: PromptTemplate.fromTemplate("Hello, world!"),
  // This will enable logging of all Chain *and* LLM events to the console.
  verbose: true,
});
```
#### API Reference:
### When do you want to use each of these?​
## Usage examples​
### Built-in handlers​
LangChain provides a few built-in handlers that you can use to get started. These are available in the langchain/callbacks module. The most basic handler is the ConsoleCallbackHandler, which simply logs all events to the console. In the future we will add more default handlers to the library. Note that when the verbose flag on the object is set to true, the ConsoleCallbackHandler will be invoked even without being explicitly passed in.
```typescript
import { ConsoleCallbackHandler } from "langchain/callbacks";
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

export const run = async () => {
  const handler = new ConsoleCallbackHandler();
  const llm = new OpenAI({ temperature: 0, callbacks: [handler] });
  const prompt = PromptTemplate.fromTemplate("1 + {number} =");
  const chain = new LLMChain({ prompt, llm, callbacks: [handler] });

  const output = await chain.call({ number: 2 });
  /*
  Entering new llm_chain chain...
  Finished chain.
  */

  console.log(output);
  /*
  { text: ' 3\n\n3 - 1 = 2' }
   */

  // The non-enumerable key `__run` contains the runId.
  console.log(output.__run);
  /*
  { runId: '90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' }
  */
};
```
#### API Reference:
### One-off handlers​
You can create a one-off handler inline by passing a plain object to the callbacks argument. This object should implement the CallbackHandlerMethods interface. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.
```typescript
import { OpenAI } from "langchain/llms/openai";

// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const model = new OpenAI({
  maxTokens: 25,
  streaming: true,
});

const response = await model.call("Tell me a joke.", {
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        console.log({ token });
      },
    },
  ],
});
console.log(response);
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }


Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
```
#### API Reference:
### Multiple handlers​
We offer a method on the CallbackManager class that allows you to create a one-off handler. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.
This is a more complete example that passes a CallbackManager to a ChatModel, and LLMChain, a Tool, and an Agent.
```typescript
import { LLMChain } from "langchain/chains";
import { AgentExecutor, ZeroShotAgent } from "langchain/agents";
import { BaseCallbackHandler } from "langchain/callbacks";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Calculator } from "langchain/tools/calculator";
import { AgentAction } from "langchain/schema";
import { Serialized } from "langchain/load/serializable";

export const run = async () => {
  // You can implement your own callback handler by extending BaseCallbackHandler
  class CustomHandler extends BaseCallbackHandler {
    name = "custom_handler";

    handleLLMNewToken(token: string) {
      console.log("token", { token });
    }

    handleLLMStart(llm: Serialized, _prompts: string[]) {
      console.log("handleLLMStart", { llm });
    }

    handleChainStart(chain: Serialized) {
      console.log("handleChainStart", { chain });
    }

    handleAgentAction(action: AgentAction) {
      console.log("handleAgentAction", action);
    }

    handleToolStart(tool: Serialized) {
      console.log("handleToolStart", { tool });
    }
  }

  const handler1 = new CustomHandler();

  // Additionally, you can use the `fromMethods` method to create a callback handler
  const handler2 = BaseCallbackHandler.fromMethods({
    handleLLMStart(llm, _prompts: string[]) {
      console.log("handleLLMStart: I'm the second handler!!", { llm });
    },
    handleChainStart(chain) {
      console.log("handleChainStart: I'm the second handler!!", { chain });
    },
    handleAgentAction(action) {
      console.log("handleAgentAction", action);
    },
    handleToolStart(tool) {
      console.log("handleToolStart", { tool });
    },
  });

  // You can restrict callbacks to a particular object by passing it upon creation
  const model = new ChatOpenAI({
    temperature: 0,
    callbacks: [handler2], // this will issue handler2 callbacks related to this model
    streaming: true, // needed to enable streaming, which enables handleLLMNewToken
  });

  const tools = [new Calculator()];
  const agentPrompt = ZeroShotAgent.createPrompt(tools);

  const llmChain = new LLMChain({
    llm: model,
    prompt: agentPrompt,
    callbacks: [handler2], // this will issue handler2 callbacks related to this chain
  });
  const agent = new ZeroShotAgent({
    llmChain,
    allowedTools: ["search"],
  });

  const agentExecutor = AgentExecutor.fromAgentAndTools({
    agent,
    tools,
  });

  /*
   * When we pass the callback handler to the agent executor, it will be used for all
   * callbacks related to the agent and all the objects involved in the agent's
   * execution, in this case, the Tool, LLMChain, and LLM.
   *
   * The `handler2` callback handler will only be used for callbacks related to the
   * LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.
   */
  const result = await agentExecutor.call(
    {
      input: "What is 2 to the power of 8",
    },
    [handler1]
  ); // this is needed to see handleAgentAction
  /*
  handleChainStart { chain: { name: 'agent_executor' } }
  handleChainStart { chain: { name: 'llm_chain' } }
  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
  handleLLMStart { llm: { name: 'openai' } }
  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
  token { token: '' }
  token { token: 'I' }
  token { token: ' can' }
  token { token: ' use' }
  token { token: ' the' }
  token { token: ' calculator' }
  token { token: ' tool' }
  token { token: ' to' }
  token { token: ' solve' }
  token { token: ' this' }
  token { token: '.\n' }
  token { token: 'Action' }
  token { token: ':' }
  token { token: ' calculator' }
  token { token: '\n' }
  token { token: 'Action' }
  token { token: ' Input' }
  token { token: ':' }
  token { token: ' ' }
  token { token: '2' }
  token { token: '^' }
  token { token: '8' }
  token { token: '' }
  handleAgentAction {
    tool: 'calculator',
    toolInput: '2^8',
    log: 'I can use the calculator tool to solve this.\n' +
      'Action: calculator\n' +
      'Action Input: 2^8'
  }
  handleToolStart { tool: { name: 'calculator' } }
  handleChainStart { chain: { name: 'llm_chain' } }
  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
  handleLLMStart { llm: { name: 'openai' } }
  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
  token { token: '' }
  token { token: 'That' }
  token { token: ' was' }
  token { token: ' easy' }
  token { token: '!\n' }
  token { token: 'Final' }
  token { token: ' Answer' }
  token { token: ':' }
  token { token: ' ' }
  token { token: '256' }
  token { token: '' }
  */

  console.log(result);
  /*
  {
    output: '256',
    __run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }
  }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/callbacks/how_to/background_callbacks

# Backgrounding callbacks
By default callbacks run in-line with the your chain/LLM run. This means that if you have a slow callback you can see an impact on the overall latency of your runs. You can make callbacks not be awaited by setting the environment variable LANGCHAIN_CALLBACKS_BACKGROUND=true. This will cause the callbacks to be run in the background, and will not impact the overall latency of your runs. When you do this you might need to await all pending callbacks before exiting your application. You can do this with the following method:
```typescript
import { awaitAllCallbacks } from "langchain/callbacks";

await awaitAllCallbacks();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/callbacks/how_to/create_handlers

# Creating custom callback handlers
You can also create your own handler by implementing the BaseCallbackHandler interface. This is useful if you want to do something more complex than just logging to the console, eg. send the events to a logging service. As an example here is a simple implementation of a handler that logs to the console:
```typescript
import { BaseCallbackHandler } from "langchain/callbacks";
import { Serialized } from "langchain/load/serializable";
import { AgentAction, AgentFinish, ChainValues } from "langchain/schema";

export class MyCallbackHandler extends BaseCallbackHandler {
  name = "MyCallbackHandler";

  async handleChainStart(chain: Serialized) {
    console.log(`Entering new ${chain.id} chain...`);
  }

  async handleChainEnd(_output: ChainValues) {
    console.log("Finished chain.");
  }

  async handleAgentAction(action: AgentAction) {
    console.log(action.log);
  }

  async handleToolEnd(output: string) {
    console.log(output);
  }

  async handleText(text: string) {
    console.log(text);
  }

  async handleAgentEnd(action: AgentFinish) {
    console.log(action.log);
  }
}
```
#### API Reference:
You could then use it as described in the section above.



Page URL: https://js.langchain.com/docs/modules/callbacks/how_to/creating_subclasses

# Callbacks in custom Chains/Agents
LangChain is designed to be extensible. You can add your own custom Chains and Agents to the library. This page will show you how to add callbacks to your custom Chains and Agents.
## Adding callbacks to custom Chains​
When you create a custom chain you can easily set it up to use the same callback system as all the built-in chains. See this guide for more information on how to [create custom chains and use callbacks inside them(/docs/modules/chains#subclassing-basechain).



Page URL: https://js.langchain.com/docs/modules/callbacks/how_to/tags

# Tags
You can add tags to your callbacks by passing a tags argument to the call()/run()/apply() methods. This is useful for filtering your logs, eg. if you want to log all requests made to a specific LLMChain, you can add a tag, and then filter your logs by that tag. You can pass tags to both constructor and request callbacks, see the examples above for details. These tags are then passed to the tags argument of the "start" callback methods, ie. handleLLMStart, handleChatModelStart, handleChainStart, handleToolStart.



Page URL: https://js.langchain.com/docs/modules/

# Modules
LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:
#### Model I/O​
Interface with language models
#### Data connection​
Interface with application-specific data
#### Chains​
Construct sequences of calls
#### Agents​
Let chains choose which tools to use given high-level directives
#### Memory​
Persist application state between runs of a chain
#### Callbacks​
Log and stream intermediate steps of any chain



Page URL: https://js.langchain.com/docs/modules/model_io/

# Model I/O
The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.




Page URL: https://js.langchain.com/docs/modules/model_io/prompts/

# Prompts
The new way of programming models is through prompts.
A prompt refers to the input to the model.
This input is often constructed from multiple components.
LangChain provides several classes and functions to make constructing and working with prompts easy.



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_templates/

# Prompt templates
Language models take text as input - that text is commonly referred to as a prompt.
Typically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input.
LangChain provides several classes and functions to make constructing and working with prompts easy.
## What is a prompt template?​
A prompt template refers to a reproducible way to generate a prompt. It contains a text string ("the template"), that can take in a set of parameters from the end user and generates a prompt.
A prompt template can contain:
Here's a simple example:
```typescript
import { PromptTemplate } from "langchain/prompts";

const prompt = PromptTemplate.fromTemplate(`You are a naming consultant for new companies.
What is a good name for a company that makes {product}?`
);

const formattedPrompt = await prompt.format({
  product: "colorful socks",
});

/*
  You are a naming consultant for new companies.
  What is a good name for a company that makes colorful socks?
*/
```

Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_templates/partial

# Partial prompt templates
Like other methods, it can make sense to "partial" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.
LangChain supports this in two ways:
These two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.
## Partial With Strings​
One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, foo and baz. If you get the foo value early on in the chain, but the baz value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the foo value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:
```typescript
import { PromptTemplate } from "langchain/prompts";

const prompt = new PromptTemplate({
  template: "{foo}{bar}",
  inputVariables: ["foo", "bar"]
});

const paritalPrompt = await prompt.partial({
  foo: "foo",
});

const formattedPrompt = await paritalPrompt.format({
  bar: "baz",
});

console.log(formattedPrompt);

// foobaz
```
You can also just initialize the prompt with the partialed variables.
```typescript
const prompt = new PromptTemplate({
  template: "{foo}{bar}",
  inputVariables: ["bar"],
  partialVariables: {
    foo: "foo",
  },
});

const formattedPrompt = await prompt.format({
  bar: "baz",
});

console.log(formattedPrompt);

// foobaz
```
## Partial With Functions​
You can also partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables can be tedious. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date.
```typescript
const getCurrentDate = () => {
  return new Date().toISOString();
};

const prompt = new PromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective", "date"],
});

const partialPrompt = await prompt.partial({
  date: getCurrentDate,
});

const formattedPrompt = await partialPrompt.format({
  adjective: "funny",
});

console.log(formattedPrompt)

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```
You can also just initialize the prompt with the partialed variables:
```typescript
const prompt = new PromptTemplate({
  template: "Tell me a {adjective} joke about the day {date}",
  inputVariables: ["adjective"],
  partialVariables: {
    date: getCurrentDate,
  }
});

const formattedPrompt = await prompt.format({
  adjective: "funny",
});

console.log(formattedPrompt)

// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z
```



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_templates/prompt_composition

# Composition
This notebook goes over how to compose multiple prompts together. This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt. A PipelinePrompt consists of two main parts:
```typescript
import { PromptTemplate, PipelinePromptTemplate } from "langchain/prompts";

const fullPrompt = PromptTemplate.fromTemplate(`{introduction}

{example}

{start}`);

const introductionPrompt = PromptTemplate.fromTemplate(
  `You are impersonating {person}.`
);

const examplePrompt =
  PromptTemplate.fromTemplate(`Here's an example of an interaction:
Q: {example_q}
A: {example_a}`);

const startPrompt = PromptTemplate.fromTemplate(`Now, do this for real!
Q: {input}
A:`);

const composedPrompt = new PipelinePromptTemplate({
  pipelinePrompts: [
    {
      name: "introduction",
      prompt: introductionPrompt,
    },
    {
      name: "example",
      prompt: examplePrompt,
    },
    {
      name: "start",
      prompt: startPrompt,
    },
  ],
  finalPrompt: fullPrompt,
});

const formattedPrompt = await composedPrompt.format({
  person: "Elon Musk",
  example_q: `What's your favorite car?`,
  example_a: "Telsa",
  input: `What's your favorite social media site?`,
});

console.log(formattedPrompt);

/*
  You are impersonating Elon Musk.

  Here's an example of an interaction:
  Q: What's your favorite car?
  A: Telsa

  Now, do this for real!
  Q: What's your favorite social media site?
  A:
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/example_selectors/

# Example selectors
If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.
The base interface is defined as below:
If you have a large number of examples, you may need to programmatically select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so. The base interface is defined as below.
```typescript
class BaseExampleSelector {
  addExample(example: Example): Promise<void | string>;

  selectExamples(input_variables: Example): Promise<Example[]>;
}
```
It needs to expose a selectExamples - this takes in the input variables and then returns a list of examples method - and an addExample method, which saves an example for later selection. It is up to each specific implementation as to how those examples are saved and selected.



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/example_selectors/length_based

# Select by length
This example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.
```typescript
import {
  LengthBasedExampleSelector,
  PromptTemplate,
  FewShotPromptTemplate,
} from "langchain/prompts";

export async function run() {
  // Create a prompt template that will be used to format the examples.
  const examplePrompt = new PromptTemplate({
    inputVariables: ["input", "output"],
    template: "Input: {input}\nOutput: {output}",
  });

  // Create a LengthBasedExampleSelector that will be used to select the examples.
  const exampleSelector = await LengthBasedExampleSelector.fromExamples(
    [
      { input: "happy", output: "sad" },
      { input: "tall", output: "short" },
      { input: "energetic", output: "lethargic" },
      { input: "sunny", output: "gloomy" },
      { input: "windy", output: "calm" },
    ],
    {
      examplePrompt,
      maxLength: 25,
    }
  );

  // Create a FewShotPromptTemplate that will use the example selector.
  const dynamicPrompt = new FewShotPromptTemplate({
    // We provide an ExampleSelector instead of examples.
    exampleSelector,
    examplePrompt,
    prefix: "Give the antonym of every input",
    suffix: "Input: {adjective}\nOutput:",
    inputVariables: ["adjective"],
  });

  // An example with small input, so it selects all examples.
  console.log(await dynamicPrompt.format({ adjective: "big" }));
  /*
   Give the antonym of every input

   Input: happy
   Output: sad

   Input: tall
   Output: short

   Input: energetic
   Output: lethargic

   Input: sunny
   Output: gloomy

   Input: windy
   Output: calm

   Input: big
   Output:
   */

  // An example with long input, so it selects only one example.
  const longString =
    "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else";
  console.log(await dynamicPrompt.format({ adjective: longString }));
  /*
   Give the antonym of every input

   Input: happy
   Output: sad

   Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
   Output:
   */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/example_selectors/similarity

# Select by similarity
This object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import {
  SemanticSimilarityExampleSelector,
  PromptTemplate,
  FewShotPromptTemplate,
} from "langchain/prompts";
import { HNSWLib } from "langchain/vectorstores/hnswlib";

export async function run() {
  // Create a prompt template that will be used to format the examples.
  const examplePrompt = new PromptTemplate({
    inputVariables: ["input", "output"],
    template: "Input: {input}\nOutput: {output}",
  });

  // Create a SemanticSimilarityExampleSelector that will be used to select the examples.
  const exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(
    [
      { input: "happy", output: "sad" },
      { input: "tall", output: "short" },
      { input: "energetic", output: "lethargic" },
      { input: "sunny", output: "gloomy" },
      { input: "windy", output: "calm" },
    ],
    new OpenAIEmbeddings(),
    HNSWLib,
    { k: 1 }
  );

  // Create a FewShotPromptTemplate that will use the example selector.
  const dynamicPrompt = new FewShotPromptTemplate({
    // We provide an ExampleSelector instead of examples.
    exampleSelector,
    examplePrompt,
    prefix: "Give the antonym of every input",
    suffix: "Input: {adjective}\nOutput:",
    inputVariables: ["adjective"],
  });

  // Input is about the weather, so should select eg. the sunny/gloomy example
  console.log(await dynamicPrompt.format({ adjective: "rainy" }));
  /*
   Give the antonym of every input

   Input: sunny
   Output: gloomy

   Input: rainy
   Output:
   */

  // Input is a measurement, so should select the tall/short example
  console.log(await dynamicPrompt.format({ adjective: "large" }));
  /*
   Give the antonym of every input

   Input: tall
   Output: short

   Input: large
   Output:
   */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/prompts/prompt_selectors/

# Prompt selectors
Prompt selectors are useful when you want to programmatically select a prompt based on the type of model you are using in a chain. This is especially relevant when swapping chat models and LLMs.
The interface for prompt selectors is quite simple:
```typescript
abstract class BasePromptSelector {
  abstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;
}
```
The getPrompt method takes in a language model and returns an appropriate prompt template.
We currently offer a ConditionalPromptSelector that allows you to specify a set of conditions and prompt templates. The first condition that evaluates to true will be used to select the prompt template.
```typescript
const QA_PROMPT_SELECTOR = new ConditionalPromptSelector(DEFAULT_QA_PROMPT, [
  [isChatModel, CHAT_PROMPT],
]);
```
This will return DEFAULT_QA_PROMPT if the model is not a chat model, and CHAT_PROMPT if it is.
The example below shows how to use a prompt selector when loading a chain:
```typescript
const loadQAStuffChain = (
  llm: BaseLanguageModel,
  params: StuffQAChainParams = {}
) => {
  const { prompt = QA_PROMPT_SELECTOR.getPrompt(llm) } = params;
  const llmChain = new LLMChain({ prompt, llm });
  const chain = new StuffDocumentsChain({ llmChain });
  return chain;
};
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/

# Language models
LangChain provides interfaces and integrations for two types of models:
## LLMs vs Chat Models​
LLMs and Chat Models are subtly but importantly different. LLMs in LangChain refer to pure text completion models.
The APIs they wrap take a string prompt as input and output a string completion. OpenAI's GPT-3 is implemented as an LLM.
Chat models are often backed by LLMs but tuned specifically for having conversations.
And, crucially, their provider APIs expose a different interface than pure text completion models. Instead of a single string,
they take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of "System",
"AI", and "Human"). And they return a ("AI") chat message as output. GPT-4 and Anthropic's Claude are both implemented as Chat Models.
To make it possible to swap LLMs and Chat Models, both implement the Base Language Model interface. This exposes common
methods "predict", which takes a string and returns a string, and "predict messages", which takes messages and returns a message.
If you are using a specific model it's recommended you use the methods specific to that model class (i.e., "predict" for LLMs and "predict messages" for Chat Models),
but if you're creating an application that should work with different types of models the shared interface can be helpful.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/

# LLMs
Large Language Models (LLMs) are a core component of LangChain.
LangChain does not serve it's own LLMs, but rather provides a standard interface for interacting with many different LLMs.
For more detailed documentation check out our:
How-to guides: Walkthroughs of core functionality, like streaming, async, etc.
Integrations: How to use different LLM providers (OpenAI, Anthropic, etc.)
## Get started​
There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.
In this walkthrough we'll work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types.
### Setup​
To start we'll need to install the official OpenAI package:
```typescript
npm install -S openai
```
```typescript
yarn add openai
```
```typescript
pnpm add openai
```
Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:
```typescript
export OPENAI_API_KEY="..."
```
If you'd prefer not to set an environment variable you can pass the key in directly via the openAIApiKey parameter when initializing the OpenAI LLM class:
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  openAIApiKey: "YOUR_KEY_HERE",
});
```
otherwise you can initialize with an empty object:
```typescript
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({});
```
### call: string in -> string out​
The simplest way to use an LLM is the .call method: pass in a string, get a string completion.
```typescript
const res = await llm.call("Tell me a joke");

console.log(res);

// "Why did the chicken cross the road?\n\nTo get to the other side."
```
### generate: batch calls, richer outputs​
generate lets you can call the model with a list of strings, getting back a more complete response than just the text. This complete response can includes things like multiple top responses and other LLM provider-specific information:
```typescript
const llmResult = await llm.generate(["Tell me a joke", "Tell me a poem"], ["Tell me a joke", "Tell me a poem"]);

console.log(llmResult.generations.length)

// 30

console.log(llmResult.generations[0]);

/*
  [
    {
      text: "\n\nQ: What did the fish say when it hit the wall?\nA: Dam!",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/

console.log(llmResult.generations[1]);

/*
  [
    {
      text: "\n\nRoses are red,\nViolets are blue,\nSugar is sweet,\nAnd so are you.",
      generationInfo: { finishReason: "stop", logprobs: null }
    }
  ]
*/
```
You can also access provider specific information that is returned. This information is NOT standardized across providers.
```typescript
console.log(llmResult.llmOutput);

/*
  {
    tokenUsage: { completionTokens: 46, promptTokens: 8, totalTokens: 54 }
  }
*/

```
Here's an example with additional parameters, which sets -1 for max_tokens to turn on token size calculations:
```typescript
import { OpenAI } from "langchain/llms/openai";

export const run = async () => {
  const model = new OpenAI({
    // customize openai model that's used, `text-davinci-003` is the default
    modelName: "text-ada-001",

    // `max_tokens` supports a magic -1 param where the max token length for the specified modelName
    //  is calculated and included in the request to OpenAI as the `max_tokens` param
    maxTokens: -1,

    // use `modelKwargs` to pass params directly to the openai call
    // note that they use snake_case instead of camelCase
    modelKwargs: {
      user: "me",
    },

    // for additional logging for debugging purposes
    verbose: true,
  });

  const resA = await model.call(
    "What would be a good company name a company that makes colorful socks?"
  );
  console.log({ resA });
  // { resA: '\n\nSocktastic Colors' }
};
```
#### API Reference:
## Advanced​
This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section.
Both LLMs and Chat Models are built on top of the BaseLanguageModel class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.
The BaseLanguageModel class has two abstract methods: generatePrompt and getNumTokens, which are implemented by BaseChatModel and BaseLLM respectively.
BaseLLM is a subclass of BaseLanguageModel that provides a common interface for LLMs while BaseChatModel is a subclass of BaseLanguageModel that provides a common interface for chat models.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/cancelling_requests

# Cancelling requests
You can cancel a request by passing a signal option when you call the model. For example, for OpenAI:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ temperature: 1 });
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.

const res = await model.call(
  "What would be a good company name a company that makes colorful socks?",
  { signal: controller.signal }
);

console.log(res);
/*
'\n\nSocktastic Colors'
*/
```
#### API Reference:
Note, this will only cancel the outgoing request if the underlying provider exposes that option. LangChain will cancel the underlying request if possible, otherwise it will cancel the processing of the response.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/dealing_with_api_errors

# Dealing with API Errors
If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a maxRetries option when you instantiate the model. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ maxRetries: 10 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/dealing_with_rate_limits

# Dealing with Rate Limits
Some LLM providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a maxConcurrency option when instantiating an LLM. This option allows you to specify the maximum number of concurrent requests you want to make to the LLM provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.
For example, if you set maxConcurrency: 5, then LangChain will only send 5 requests to the LLM provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.
To use this feature, simply pass maxConcurrency: <number> when you instantiate the LLM. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ maxConcurrency: 5 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/llm_caching

# Caching
LangChain provides an optional caching layer for LLMs. This is useful for two reasons:
It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.
It can speed up your application by reducing the number of API calls you make to the LLM provider.
```typescript
import { OpenAI } from "langchain/llms/openai";

// To make the caching really obvious, lets use a slower model.
const model = new OpenAI({
  modelName: "text-davinci-002",
  cache: true,
  n: 2,
  bestOf: 2
});
```
## In Memory Cache​
The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.
```typescript
// The first time, it is not yet in cache, so it should take longer
const res = await model.predict("Tell me a joke");
console.log(res);

/*
  CPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms
  Wall time: 4.83 s


  "\n\nWhy did the chicken cross the road?\n\nTo get to the other side."
*/
```
```typescript
// The second time it is, so it goes faster
const res2 = await model.predict("Tell me a joke");
console.log(res2);

/*
  CPU times: user 238 µs, sys: 143 µs, total: 381 µs
  Wall time: 1.76 ms


  "\n\nWhy did the chicken cross the road?\n\nTo get to the other side."
*/
```
## Caching with Momento​
LangChain also provides a Momento-based cache. Momento is a distributed, serverless cache that requires zero setup or infrastructure maintenance. To use it, you'll need to install the @gomomento/sdk package:
```typescript
npm install @gomomento/sdk
```
Next you'll need to sign up and create an API key. Once you've done that, pass a cache option when you instantiate the LLM like this:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { MomentoCache } from "langchain/cache/momento";
import {
  CacheClient,
  Configurations,
  CredentialProvider,
} from "@gomomento/sdk";

// See https://github.com/momentohq/client-sdk-javascript for connection options
const client = new CacheClient({
  configuration: Configurations.Laptop.v1(),
  credentialProvider: CredentialProvider.fromEnvironmentVariable({
    environmentVariableName: "MOMENTO_AUTH_TOKEN",
  }),
  defaultTtlSeconds: 60 * 60 * 24,
});
const cache = await MomentoCache.fromProps({
  client,
  cacheName: "langchain",
});

const model = new OpenAI({ cache });
```
#### API Reference:
## Caching with Redis​
LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the redis package:
```typescript
npm install ioredis
```
Then, you can pass a cache option when you instantiate the LLM. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RedisCache } from "langchain/cache/ioredis";
import { Redis } from "ioredis";

// See https://github.com/redis/ioredis for connection options
const client = new Redis({});

const cache = new RedisCache(client);

const model = new OpenAI({ cache });
```
## Caching with Upstash Redis​
LangChain also provides an Upstash Redis-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Upstash Redis client uses HTTP and supports edge environments. To use it, you'll need to install the @upstash/redis package:
```typescript
npm install @upstash/redis
```
You'll also need an Upstash account and a Redis database to connect to. Once you've done that, retrieve your REST URL and REST token.
Then, you can pass a cache option when you instantiate the LLM. For example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { UpstashRedisCache } from "langchain/cache/upstash_redis";

// See https://docs.upstash.com/redis/howto/connectwithupstashredis#quick-start for connection options
const cache = new UpstashRedisCache({
  config: {
    url: "UPSTASH_REDIS_REST_URL",
    token: "UPSTASH_REDIS_REST_TOKEN",
  },
});

const model = new OpenAI({ cache });
```
#### API Reference:
You can also directly pass in a previously created @upstash/redis client instance:
```typescript
import { Redis } from "@upstash/redis";
import https from "https";

import { OpenAI } from "langchain/llms/openai";
import { UpstashRedisCache } from "langchain/cache/upstash_redis";

// const client = new Redis({
//   url: process.env.UPSTASH_REDIS_REST_URL!,
//   token: process.env.UPSTASH_REDIS_REST_TOKEN!,
//   agent: new https.Agent({ keepAlive: true }),
// });

// Or simply call Redis.fromEnv() to automatically load the UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN environment variables.
const client = Redis.fromEnv({
  agent: new https.Agent({ keepAlive: true }),
});

const cache = new UpstashRedisCache({ client });
const model = new OpenAI({ cache });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/streaming_llm

# Streaming
Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.
To utilize streaming, use a CallbackHandler like so:
```typescript
import { OpenAI } from "langchain/llms/openai";

// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const model = new OpenAI({
  maxTokens: 25,
  streaming: true,
});

const response = await model.call("Tell me a joke.", {
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        console.log({ token });
      },
    },
  ],
});
console.log(response);
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }


Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
```
#### API Reference:
We still have access to the end LLMResult if using generate. However, token_usage is not currently supported for streaming.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/subscribing_events

# Subscribing to events
Especially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a LLM processes a prompt. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the LLM, you can pass callbacks to the LLM for custom logging (or anything else you want to do) as the model goes through the steps:
For more info on the events available see the Callbacks section of the docs.
```typescript
import { LLMResult } from "langchain/schema";
import { OpenAI } from "langchain/llms/openai";
import { Serialized } from "langchain/load/serializable";

// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const model = new OpenAI({
  callbacks: [
    {
      handleLLMStart: async (llm: Serialized, prompts: string[]) => {
        console.log(JSON.stringify(llm, null, 2));
        console.log(JSON.stringify(prompts, null, 2));
      },
      handleLLMEnd: async (output: LLMResult) => {
        console.log(JSON.stringify(output, null, 2));
      },
      handleLLMError: async (err: Error) => {
        console.error(err);
      },
    },
  ],
});

await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
// {
//     "name": "openai"
// }
// [
//     "What would be a good company name a company that makes colorful socks?"
// ]
// {
//   "generations": [
//     [
//         {
//             "text": "\n\nSocktastic Splashes.",
//             "generationInfo": {
//                 "finishReason": "stop",
//                 "logprobs": null
//             }
//         }
//     ]
//  ],
//   "llmOutput": {
//     "tokenUsage": {
//         "completionTokens": 9,
//          "promptTokens": 14,
//          "totalTokens": 23
//     }
//   }
// }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a timeout option, in milliseconds, when you call the model. For example, for OpenAI:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({ temperature: 1 });

const resA = await model.call(
  "What would be a good company name a company that makes colorful socks?",
  { timeout: 1000 } // 1s timeout
);

console.log({ resA });
// '\n\nSocktastic Colors' }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/ai21

# AI21
You can get started with AI21Labs' Jurassic family of models, as well as see a full list of available foundational models, by signing up for an API key on their website.
Here's an example of initializing an instance in LangChain.js:
```typescript
import { AI21 } from "langchain/llms/ai21";

const model = new AI21({
  ai21ApiKey: "YOUR_AI21_API_KEY", // Or set as process.env.AI21_API_KEY
});

const res = await model.call(`Translate "I love programming" into German.`);

console.log({ res });

/*
  {
    res: "\nIch liebe das Programmieren."
  }
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/aleph_alpha

# AlephAlpha
LangChain.js supports AlephAlpha's Luminous family of models. You'll need to sign up for an API key on their website.
Here's an example:
```typescript
import { AlephAlpha } from "langchain/llms/aleph_alpha";

const model = new AlephAlpha({
  aleph_alpha_api_key: "YOUR_ALEPH_ALPHA_API_KEY", // Or set as process.env.ALEPH_ALPHA_API_KEY
});

const res = await model.call(`Is cereal soup?`);

console.log({ res });

/*
  {
    res: "\nIs soup a cereal? I don’t think so, but it is delicious."
  }
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/aws_sagemaker

# AWS SageMakerEndpoint
LangChain.js supports integration with AWS SageMaker-hosted endpoints. Check Amazon SageMaker JumpStart for a list of available models, and how to deploy your own.
Here's an example:
```typescript
npm install @aws-sdk/client-sagemaker-runtime
```
```typescript
yarn add @aws-sdk/client-sagemaker-runtime
```
```typescript
pnpm add @aws-sdk/client-sagemaker-runtime
```
```typescript
import {
  SageMakerLLMContentHandler,
  SageMakerEndpoint,
} from "langchain/llms/sagemaker_endpoint";

// Custom for whatever model you'll be using
class HuggingFaceTextGenerationGPT2ContentHandler
  implements SageMakerLLMContentHandler
{
  contentType = "application/json";

  accepts = "application/json";

  async transformInput(prompt: string, modelKwargs: Record<string, unknown>) {
    const inputString = JSON.stringify({
      text_inputs: prompt,
      ...modelKwargs,
    });
    return Buffer.from(inputString);
  }

  async transformOutput(output: Uint8Array) {
    const responseJson = JSON.parse(Buffer.from(output).toString("utf-8"));
    return responseJson.generated_texts[0];
  }
}

const contentHandler = new HuggingFaceTextGenerationGPT2ContentHandler();

const model = new SageMakerEndpoint({
  endpointName:
    "jumpstart-example-huggingface-textgener-2023-05-16-22-35-45-660", // Your endpoint name here
  modelKwargs: { temperature: 1e-10 },
  contentHandler,
  clientOptions: {
    region: "YOUR AWS ENDPOINT REGION",
    credentials: {
      accessKeyId: "YOUR AWS ACCESS ID",
      secretAccessKey: "YOUR AWS SECRET ACCESS KEY",
    },
  },
});

const res = await model.call("Hello, my name is ");

console.log({ res });

/*
  {
    res: "_____. I am a student at the University of California, Berkeley. I am a member of the American Association of University Professors."
  }
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/azure

# Azure OpenAI
You can also use the OpenAI class to call OpenAI models hosted on Azure. Here's an example:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-API-KEY",
  azureOpenAIApiInstanceName: "YOUR-INSTANCE-NAME",
  azureOpenAIApiDeploymentName: "YOUR-DEPLOYMENT-NAME",
  azureOpenAIApiVersion: "YOUR-API-VERSION",
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/cohere

# Cohere
LangChain.js supports Cohere LLMs. Here's an example:
```typescript
npm install cohere-ai
```
```typescript
yarn add cohere-ai
```
```typescript
pnpm add cohere-ai
```
```typescript
import { Cohere } from "langchain/llms/cohere";

const model = new Cohere({
  maxTokens: 20,
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.COHERE_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/google_palm

# Google PaLM
The Google PaLM API can be integrated by first
installing the required packages:
```typescript
npm install google-auth-library @google-ai/generativelanguage
```
```typescript
yarn add google-auth-library @google-ai/generativelanguage
```
```typescript
pnpm add google-auth-library @google-ai/generativelanguage
```
Create an API key from Google MakerSuite. You can then set
the key as GOOGLE_PALM_API_KEY environment variable or pass it as apiKey parameter while instantiating
the model.
```typescript
import { GooglePaLM } from "langchain/llms/googlepalm";

export const run = async () => {
  const model = new GooglePaLM({
    apiKey: "<YOUR API KEY>", // or set it in environment variable as `GOOGLE_PALM_API_KEY`
    // other params
    temperature: 1, // OPTIONAL
    modelName: "models/text-bison-001", // OPTIONAL
    maxOutputTokens: 1024, // OPTIONAL
    topK: 40, // OPTIONAL
    topP: 3, // OPTIONAL
    safetySettings: [
      // OPTIONAL
      {
        category: "HARM_CATEGORY_DANGEROUS",
        threshold: "BLOCK_MEDIUM_AND_ABOVE",
      },
    ],
    stopSequences: ["stop"], // OPTIONAL
  });
  const res = await model.call(
    "What would be a good company name for a company that makes colorful socks?"
  );
  console.log({ res });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/google_vertex_ai

# Google Vertex AI
The Vertex AI implementation is meant to be used in Node.js and not
directly in a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
```typescript
import { GoogleVertexAI } from "langchain/llms/googlevertexai";

/*
 * Before running this, you should make sure you have created a
 * Google Cloud Project that is permitted to the Vertex AI API.
 *
 * You will also need permission to access this project / API.
 * Typically, this is done in one of three ways:
 * - You are logged into an account permitted to that project.
 * - You are running this on a machine using a service account permitted to
 *   the project.
 * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the
 *   path of a credentials file for a service account permitted to the project.
 */
export const run = async () => {
  const model = new GoogleVertexAI({
    temperature: 0.7,
  });
  const res = await model.call(
    "What would be a good company name a company that makes colorful socks?"
  );
  console.log({ res });
};
```
#### API Reference:
Google also has separate models for their "Codey" code generation models.
The "code-gecko" model is useful for code completion:
```typescript
import { GoogleVertexAI } from "langchain/llms/googlevertexai";

/*
 * Before running this, you should make sure you have created a
 * Google Cloud Project that is permitted to the Vertex AI API.
 *
 * You will also need permission to access this project / API.
 * Typically, this is done in one of three ways:
 * - You are logged into an account permitted to that project.
 * - You are running this on a machine using a service account permitted to
 *   the project.
 * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the
 *   path of a credentials file for a service account permitted to the project.
 */

const model = new GoogleVertexAI({
  model: "code-gecko",
});
const res = await model.call("for (let co=0;");
console.log({ res });
```
#### API Reference:
While the "code-bison" model is better at larger code generation based on
a text prompt:
```typescript
import { GoogleVertexAI } from "langchain/llms/googlevertexai";

/*
 * Before running this, you should make sure you have created a
 * Google Cloud Project that is permitted to the Vertex AI API.
 *
 * You will also need permission to access this project / API.
 * Typically, this is done in one of three ways:
 * - You are logged into an account permitted to that project.
 * - You are running this on a machine using a service account permitted to
 *   the project.
 * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the
 *   path of a credentials file for a service account permitted to the project.
 */

const model = new GoogleVertexAI({
  model: "code-bison",
  maxOutputTokens: 2048,
});
const res = await model.call("A Javascript function that counts from 1 to 10.");
console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_inference

# HuggingFaceInference
Here's an example of calling a HugggingFaceInference model as an LLM:
```typescript
npm install @huggingface/inference@1
```
```typescript
yarn add @huggingface/inference@1
```
```typescript
pnpm add @huggingface/inference@1
```
```typescript
import { HuggingFaceInference } from "langchain/llms/hf";

const model = new HuggingFaceInference({
  model: "gpt2",
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
});
const res = await model.call("1 + 1 =");
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/openai

# OpenAI
Here's how you can initialize an OpenAI LLM instance:
```typescript
import { OpenAI } from "langchain/llms/openai";

const model = new OpenAI({
  modelName: "text-davinci-003", // Defaults to "text-davinci-003" if no model provided.
  temperature: 0.9,
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
console.log({ res });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/prompt_layer_openai

# PromptLayer OpenAI
LangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:
```typescript
import { PromptLayerOpenAI } from "langchain/llms/openai";

const model = new PromptLayerOpenAI({
  temperature: 0.9,
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  promptLayerApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
```
# Azure PromptLayerOpenAI
LangChain also integrates with PromptLayer for Azure-hosted OpenAI instances:
```typescript
import { PromptLayerOpenAI } from "langchain/llms/openai";

const model = new PromptLayerOpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-AOAI-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-AOAI-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-AOAI-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiCompletionsDeploymentName:
    "YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
  azureOpenAIApiEmbeddingsDeploymentName:
    "YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-AOAI-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
  promptLayerApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY
});
const res = await model.call(
  "What would be a good company name a company that makes colorful socks?"
);
```
The request and the response will be logged in the PromptLayer dashboard.
Note: In streaming mode PromptLayer will not log the response.



Page URL: https://js.langchain.com/docs/modules/model_io/models/llms/integrations/replicate

# Replicate
Here's an example of calling a Replicate model as an LLM:
```typescript
npm install replicate
```
```typescript
yarn add replicate
```
```typescript
pnpm add replicate
```
```typescript
import { Replicate } from "langchain/llms/replicate";

const model = new Replicate({
  model:
    "a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5",
});

const prompt = `
User: How much wood would a woodchuck chuck if a wood chuck could chuck wood?
Assistant:`;

const res = await model.call(prompt);
console.log({ res });
/*
  {
    res: "I'm happy to help! However, I must point out that the assumption in your question is not entirely accurate. " +
      + "Woodchucks, also known as groundhogs, do not actually chuck wood. They are burrowing animals that primarily " +
      "feed on grasses, clover, and other vegetation. They do not have the physical ability to chuck wood.\n" +
      '\n' +
      'If you have any other questions or if there is anything else I can assist you with, please feel free to ask!'
  }
*/
```
#### API Reference:
You can run other models through Replicate by changing the model parameter.
You can find a full list of models on Replicate's website.



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/

# Chat models
Chat models are a variation on language models.
While chat models use language models under the hood, the interface they expose is a bit different.
Rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.
Chat model APIs are fairly new, so we are still figuring out the correct abstractions.
The following sections of documentation are provided:
How-to guides: Walkthroughs of core functionality, like streaming, creating chat prompts, etc.
Integrations: How to use different chat model providers (OpenAI, Anthropic, etc).
## Get started​
### Setup​
To start we'll need to install the official OpenAI package:
```typescript
npm install -S openai
```
```typescript
yarn add openai
```
```typescript
pnpm add openai
```
Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:
```typescript
export OPENAI_API_KEY="..."
```
If you'd prefer not to set an environment variable you can pass the key in directly via the openAIApiKey parameter when initializing the ChatOpenAI class:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({
  openAIApiKey: "YOUR_KEY_HERE"
});
```
otherwise you can initialize it with an empty object:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({});
```
### Messages​
The chat model interface is based around messages rather than raw text.
The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, FunctionMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage
### call​
#### Messages in -> message out​
You can get chat completions by passing one or more messages to the chat model. The response will be a message.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const chat = new ChatOpenAI();
// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.
const response = await chat.call([
  new HumanMessage(
    "What is a good name for a company that makes colorful socks?"
  ),
]);
console.log(response);
// AIMessage { text: '\n\nRainbow Sox Co.' }
```
#### API Reference:
OpenAI's chat model also supports multiple messages as input. See here for more information. Here is an example of sending a system and user message to the chat model:
```typescript
const response2 = await chat.call([
  new SystemMessage(
    "You are a helpful assistant that translates English to French."
  ),
  new HumanMessage("Translate: I love programming."),
]);
console.log(response2);
// AIMessage { text: "J'aime programmer." }
```
### generate​
#### Batch calls, richer outputs​
You can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.
```typescript
const response3 = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love programming."
    ),
  ],
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
    new HumanMessage(
      "Translate this sentence from English to French. I love artificial intelligence."
    ),
  ],
]);
console.log(response3);
/*
  {
    generations: [
      [
        {
          text: "J'aime programmer.",
          message: AIMessage { text: "J'aime programmer." },
        }
      ],
      [
        {
          text: "J'aime l'intelligence artificielle.",
          message: AIMessage { text: "J'aime l'intelligence artificielle." }
        }
      ]
    ]
  }
*/
```
You can recover things like token usage from this LLMResult:
```typescript
console.log(response3.llmOutput);
/*
  {
    tokenUsage: { completionTokens: 20, promptTokens: 69, totalTokens: 89 }
  }
*/
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/cancelling_requests

# Cancelling requests
You can cancel a request by passing a signal option when you call the model. For example, for OpenAI:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const model = new ChatOpenAI({ temperature: 1 });
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.

const res = await model.call(
  [
    new HumanMessage(
      "What is a good name for a company that makes colorful socks?"
    ),
  ],
  { signal: controller.signal }
);

console.log(res);
/*
'\n\nSocktastic Colors'
*/
```
#### API Reference:
Note, this will only cancel the outgoing request if the underlying provider exposes that option. LangChain will cancel the underlying request if possible, otherwise it will cancel the processing of the response.



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/dealing_with_api_errors

# Dealing with API Errors
If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a maxRetries option when you instantiate the model. For example:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const model = new ChatOpenAI({ maxRetries: 10 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/dealing_with_rate_limits

# Dealing with rate limits
Some providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a maxConcurrency option when instantiating a Chat Model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.
For example, if you set maxConcurrency: 5, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.
To use this feature, simply pass maxConcurrency: <number> when you instantiate the LLM. For example:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const model = new ChatOpenAI({ maxConcurrency: 5 });
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/llm_chain

# LLMChain
You can use the existing LLMChain in a very similar way to before - provide a prompt and a model.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate
} from "langchain/prompts";

const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate)

const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt])

const chat = new ChatOpenAI({
  temperature: 0,
});

const chain = new LLMChain({
  llm: chat,
  prompt: chatPrompt,
});

const result = await chain.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming",
});

// { text: "J'adore programmer" }
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/prompts

# Prompts
Prompts for Chat models are built around messages, instead of just plain text.
You can make use of templating by using a ChatPromptTemplate from one or more MessagePromptTemplates, then using ChatPromptTemplate's
formatPrompt method.
For convenience, there is also a fromTemplate method exposed on the template. If you were to use this template, this is what it would look like:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { LLMChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate
} from "langchain/prompts";

const template = "You are a helpful assistant that translates {input_language} to {output_language}.";
const systemMessagePrompt = SystemMessagePromptTemplate.fromTemplate(template);
const humanTemplate = "{text}";
const humanMessagePrompt = HumanMessagePromptTemplate.fromTemplate(humanTemplate);

const chatPrompt = ChatPromptTemplate.fromPromptMessages([systemMessagePrompt, humanMessagePrompt]);

const chat = new ChatOpenAI({
  temperature: 0,
});

const chain = new LLMChain({
  llm: chat,
  prompt: chatPrompt,
});

const result = await chain.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming",
});
```
```typescript
// { text: "J'adore programmer" }
```



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/streaming

# Streaming
Some Chat models provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const chat = new ChatOpenAI({
  maxTokens: 25,
  streaming: true,
});

const response = await chat.call([new HumanMessage("Tell me a joke.")], {
  callbacks: [
    {
      handleLLMNewToken(token: string) {
        console.log({ token });
      },
    },
  ],
});

console.log(response);
// { token: '' }
// { token: '\n\n' }
// { token: 'Why' }
// { token: ' don' }
// { token: "'t" }
// { token: ' scientists' }
// { token: ' trust' }
// { token: ' atoms' }
// { token: '?\n\n' }
// { token: 'Because' }
// { token: ' they' }
// { token: ' make' }
// { token: ' up' }
// { token: ' everything' }
// { token: '.' }
// { token: '' }
// AIMessage {
//   text: "\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything."
// }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/subscribing_events

# Subscribing to events
Especially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a Chat Model processes a prompt. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the Chat Model, you can pass callbacks to the Chat Model for custom logging (or anything else you want to do) as the model goes through the steps:
For more info on the events available see the Callbacks section of the docs.
```typescript
import { HumanMessage, LLMResult } from "langchain/schema";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Serialized } from "langchain/load/serializable";

// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const model = new ChatOpenAI({
  callbacks: [
    {
      handleLLMStart: async (llm: Serialized, prompts: string[]) => {
        console.log(JSON.stringify(llm, null, 2));
        console.log(JSON.stringify(prompts, null, 2));
      },
      handleLLMEnd: async (output: LLMResult) => {
        console.log(JSON.stringify(output, null, 2));
      },
      handleLLMError: async (err: Error) => {
        console.error(err);
      },
    },
  ],
});

await model.call([
  new HumanMessage(
    "What is a good name for a company that makes colorful socks?"
  ),
]);
/*
{
  "name": "openai"
}
[
  "Human: What is a good name for a company that makes colorful socks?"
]
{
  "generations": [
    [
      {
        "text": "Rainbow Soles",
        "message": {
          "text": "Rainbow Soles"
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 4,
      "promptTokens": 21,
      "totalTokens": 25
    }
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a timeout option, in milliseconds, when you call the model. For example, for OpenAI:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";

const chat = new ChatOpenAI({ temperature: 1 });

const response = await chat.call(
  [
    new HumanMessage(
      "What is a good name for a company that makes colorful socks?"
    ),
  ],
  { timeout: 1000 } // 1s timeout
);
console.log(response);
// AIMessage { text: '\n\nRainbow Sox Co.' }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/anthropic

# ChatAnthropic
LangChain supports Anthropic's Claude family of chat models. You can initialize an instance like this:
```typescript
import { ChatAnthropic } from "langchain/chat_models/anthropic";

const model = new ChatAnthropic({
  temperature: 0.9,
  anthropicApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.ANTHROPIC_API_KEY
});
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/azure

# Azure ChatOpenAI
You can also use the ChatOpenAI class to access OpenAI instances hosted on Azure like this:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";

const model = new ChatOpenAI({
  temperature: 0.9,
  azureOpenAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/baidu_wenxin

# ChatBaiduWenxin
LangChain.js supports Baidu's ERNIE-bot family of models. Here's an example:
```typescript
import { ChatBaiduWenxin } from "langchain/chat_models/baiduwenxin";
import { HumanMessage } from "langchain/schema";

// Default model is ERNIE-Bot-turbo
const ernieTurbo = new ChatBaiduWenxin({
  baiduApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.BAIDU_API_KEY
  baiduSecretKey: "YOUR-SECRET-KEY", // In Node.js defaults to process.env.BAIDU_SECRET_KEY
});

// Use ERNIE-Bot
const ernie = new ChatBaiduWenxin({
  modelName: "ERNIE-Bot",
  temperature: 1, // Only ERNIE-Bot supports temperature
  baiduApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.BAIDU_API_KEY
  baiduSecretKey: "YOUR-SECRET-KEY", // In Node.js defaults to process.env.BAIDU_SECRET_KEY
});

const messages = [new HumanMessage("Hello")];

let res = await ernieTurbo.call(messages);
/*
AIChatMessage {
  text: 'Hello! How may I assist you today?',
  name: undefined,
  additional_kwargs: {}
  }
}
*/

res = await ernie.call(messages);
/*
AIChatMessage {
  text: 'Hello! How may I assist you today?',
  name: undefined,
  additional_kwargs: {}
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/google_palm

# ChatGooglePaLM
The Google PaLM API can be integrated by first
installing the required packages:
```typescript
npm install google-auth-library @google-ai/generativelanguage
```
```typescript
yarn add google-auth-library @google-ai/generativelanguage
```
```typescript
pnpm add google-auth-library @google-ai/generativelanguage
```
Create an API key from Google MakerSuite. You can then set
the key as GOOGLE_PALM_API_KEY environment variable or pass it as apiKey parameter while instantiating
the model.
```typescript
import { ChatGooglePaLM } from "langchain/chat_models/googlepalm";
import { AIMessage, HumanMessage, SystemMessage } from "langchain/schema";

export const run = async () => {
  const model = new ChatGooglePaLM({
    apiKey: "<YOUR API KEY>", // or set it in environment variable as `GOOGLE_PALM_API_KEY`
    temperature: 0.7, // OPTIONAL
    modelName: "models/chat-bison-001", // OPTIONAL
    topK: 40, // OPTIONAL
    topP: 3, // OPTIONAL
    examples: [
      // OPTIONAL
      {
        input: new HumanMessage("What is your favorite sock color?"),
        output: new AIMessage("My favorite sock color be arrrr-ange!"),
      },
    ],
  });

  // ask questions
  const questions = [
    new SystemMessage(
      "You are a funny assistant that answers in pirate language."
    ),
    new HumanMessage("What is your favorite food?"),
  ];

  // You can also use the model as part of a chain
  const res = await model.call(questions);
  console.log({ res });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/google_vertex_ai

# ChatGoogleVertexAI
The Vertex AI implementation is meant to be used in Node.js and not
directly from a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
The ChatGoogleVertexAI class works just like other chat-based LLMs,
with a few exceptions:
```typescript
import { ChatGoogleVertexAI } from "langchain/chat_models/googlevertexai";

const model = new ChatGoogleVertexAI({
  temperature: 0.7,
});
```
#### API Reference:
There is also an optional examples constructor parameter that can help the model understand what an appropriate response
looks like.
```typescript
import { ChatGoogleVertexAI } from "langchain/chat_models/googlevertexai";
import { AIMessage, HumanMessage, SystemMessage } from "langchain/schema";

export const run = async () => {
  const examples = [
    {
      input: new HumanMessage("What is your favorite sock color?"),
      output: new AIMessage("My favorite sock color be arrrr-ange!"),
    },
  ];
  const model = new ChatGoogleVertexAI({
    temperature: 0.7,
    examples,
  });
  const questions = [
    new SystemMessage(
      "You are a funny assistant that answers in pirate language."
    ),
    new HumanMessage("What is your favorite food?"),
  ];
  // You can also use the model as part of a chain
  const res = await model.call(questions);
  console.log({ res });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/openai

# ChatOpenAI
You can use OpenAI's chat models as follows:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage } from "langchain/schema";
import { SerpAPI } from "langchain/tools";

const model = new ChatOpenAI({
  temperature: 0.9,
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
});

// You can also pass tools or functions to the model, learn more here
// https://platform.openai.com/docs/guides/gpt/function-calling

const modelForFunctionCalling = new ChatOpenAI({
  modelName: "gpt-4-0613",
  temperature: 0,
});

await modelForFunctionCalling.predictMessages(
  [new HumanMessage("What is the weather in New York?")],
  { tools: [new SerpAPI()] }
  // Tools will be automatically formatted as functions in the OpenAI format
);
/*
AIMessage {
  text: '',
  name: undefined,
  additional_kwargs: {
    function_call: {
      name: 'search',
      arguments: '{\n  "input": "current weather in New York"\n}'
    }
  }
}
*/

await modelForFunctionCalling.predictMessages(
  [new HumanMessage("What is the weather in New York?")],
  {
    functions: [
      {
        name: "get_current_weather",
        description: "Get the current weather in a given location",
        parameters: {
          type: "object",
          properties: {
            location: {
              type: "string",
              description: "The city and state, e.g. San Francisco, CA",
            },
            unit: { type: "string", enum: ["celsius", "fahrenheit"] },
          },
          required: ["location"],
        },
      },
    ],
    // You can set the `function_call` arg to force the model to use a function
    function_call: {
      name: "get_current_weather",
    },
  }
);
/*
AIMessage {
  text: '',
  name: undefined,
  additional_kwargs: {
    function_call: {
      name: 'get_current_weather',
      arguments: '{\n  "location": "New York"\n}'
    }
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/models/chat/integrations/prompt_layer_openai

# PromptLayerChatOpenAI
You can pass in the optional returnPromptLayerId boolean to get a promptLayerRequestId like below. Here is an example of getting the PromptLayerChatOpenAI requestID:
```typescript
import { PromptLayerChatOpenAI } from "langchain/chat_models/openai";

const chat = new PromptLayerChatOpenAI({
  returnPromptLayerId: true,
});

const respA = await chat.generate([
  [
    new SystemMessage(
      "You are a helpful assistant that translates English to French."
    ),
  ],
]);

console.log(JSON.stringify(respA, null, 3));

/*
  {
    "generations": [
      [
        {
          "text": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?",
          "message": {
            "type": "ai",
            "data": {
              "content": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?"
            }
          },
          "generationInfo": {
            "promptLayerRequestId": 2300682
          }
        }
      ]
    ],
    "llmOutput": {
      "tokenUsage": {
        "completionTokens": 35,
        "promptTokens": 19,
        "totalTokens": 54
      }
    }
  }
*/
```



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/

# Output parsers
Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.
Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:
And then one optional one:
## Get started​
Below we go over one useful type of output parser, the StructuredOutputParser.
## Structured Output Parser​
This output parser can be used when you want to return multiple fields. If you want complex schema returned (i.e. a JSON object with arrays of strings), use the Zod Schema detailed below.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// With a `StructuredOutputParser` we can define a schema for the output.
const parser = StructuredOutputParser.fromNamesAndDescriptions({
  answer: "answer to the user's question",
  source: "source used to answer the user's question, should be a website.",
});

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
You must format your output as a JSON value that adheres to a given "JSON Schema" instance.

"JSON Schema" is a declarative language that allows you to annotate and validate JSON documents.

For example, the example "JSON Schema" instance {{"properties": {{"foo": {{"description": "a list of test words", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
would match an object with one required property, "foo". The "type" property specifies "foo" must be an "array", and the "description" property semantically describes it as "a list of test words". The items within "foo" must be strings.
Thus, the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of this example "JSON Schema". The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!

Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:
```json
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "source": "https://en.wikipedia.org/wiki/Paris"}
*/

console.log(await parser.parse(response));
// { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/Paris' }
```
#### API Reference:
## Structured Output Parser with Zod Schema​
This output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. z.date() is not allowed.
```typescript
import { z } from "zod";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.
const parser = StructuredOutputParser.fromZodSchema(
  z.object({
    answer: z.string().describe("answer to the user's question"),
    sources: z
      .array(z.string())
      .describe("sources used to answer the question, should be websites."),
  })
);

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Here is the output schema:
```
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"sources":{"type":"array","items":{"type":"string"},"description":"sources used to answer the question, should be websites."}},"required":["answer","sources"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "sources": ["https://en.wikipedia.org/wiki/Paris"]}
*/

console.log(await parser.parse(response));
/*
{ answer: 'Paris', sources: [ 'https://en.wikipedia.org/wiki/Paris' ] }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/how_to/use_with_llm_chain

# Use with LLMChains
For convenience, you can add an output parser to an LLMChain. This will automatically call .parse() on the output.
Don't forget to put the formatting instructions in the prompt!
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import {
  StructuredOutputParser,
  OutputFixingParser,
} from "langchain/output_parsers";

const outputParser = StructuredOutputParser.fromZodSchema(
  z
    .array(
      z.object({
        fields: z.object({
          Name: z.string().describe("The name of the country"),
          Capital: z.string().describe("The country's capital"),
        }),
      })
    )
    .describe("An array of Airtable records, each representing a country")
);

const chatModel = new ChatOpenAI({
  modelName: "gpt-4", // Or gpt-3.5-turbo
  temperature: 0, // For best results with the output fixing parser
});

const outputFixingParser = OutputFixingParser.fromLLM(chatModel, outputParser);

// Don't forget to include formatting instructions in the prompt!
const prompt = new PromptTemplate({
  template: `Answer the user's question as best you can:\n{format_instructions}\n{query}`,
  inputVariables: ["query"],
  partialVariables: {
    format_instructions: outputFixingParser.getFormatInstructions(),
  },
});

const answerFormattingChain = new LLMChain({
  llm: chatModel,
  prompt,
  outputKey: "records", // For readability - otherwise the chain output will default to a property named "text"
  outputParser: outputFixingParser,
});

const result = await answerFormattingChain.call({
  query: "List 5 countries.",
});

console.log(JSON.stringify(result.records, null, 2));

/*
[
  {
    "fields": {
      "Name": "United States",
      "Capital": "Washington, D.C."
    }
  },
  {
    "fields": {
      "Name": "Canada",
      "Capital": "Ottawa"
    }
  },
  {
    "fields": {
      "Name": "Germany",
      "Capital": "Berlin"
    }
  },
  {
    "fields": {
      "Name": "Japan",
      "Capital": "Tokyo"
    }
  },
  {
    "fields": {
      "Name": "Australia",
      "Capital": "Canberra"
    }
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/combining_output_parser

# Combining output parsers
Output parsers can be combined using CombiningOutputParser. This output parser takes in a list of output parsers, and will ask for (and parse) a combined output that contains all the fields of all the parsers.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import {
  StructuredOutputParser,
  RegexParser,
  CombiningOutputParser,
} from "langchain/output_parsers";

const answerParser = StructuredOutputParser.fromNamesAndDescriptions({
  answer: "answer to the user's question",
  source: "source used to answer the user's question, should be a website.",
});

const confidenceParser = new RegexParser(
  /Confidence: (A|B|C), Explanation: (.*)/,
  ["confidence", "explanation"],
  "noConfidence"
);

const parser = new CombiningOutputParser(answerParser, confidenceParser);
const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
Return the following outputs, each formatted as described below:

Output 1:
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Here is the output schema:
```
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

Output 2:
Your response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/

What is the capital of France?
*/

console.log(response);
/*
Output 1:
{"answer":"Paris","source":"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html"}

Output 2:
Confidence: A, Explanation: The capital of France is Paris.
*/

console.log(await parser.parse(response));
/*
{
  answer: 'Paris',
  source: 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html',
  confidence: 'A',
  explanation: 'The capital of France is Paris.'
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/comma_separated

# List parser
This output parser can be used when you want to return a list of comma-separated items.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { CommaSeparatedListOutputParser } from "langchain/output_parsers";

export const run = async () => {
  // With a `CommaSeparatedListOutputParser`, we can parse a comma separated list.
  const parser = new CommaSeparatedListOutputParser();

  const formatInstructions = parser.getFormatInstructions();

  const prompt = new PromptTemplate({
    template: "List five {subject}.\n{format_instructions}",
    inputVariables: ["subject"],
    partialVariables: { format_instructions: formatInstructions },
  });

  const model = new OpenAI({ temperature: 0 });

  const input = await prompt.format({ subject: "ice cream flavors" });
  const response = await model.call(input);

  console.log(input);
  /*
   List five ice cream flavors.
   Your response should be a list of comma separated values, eg: `foo, bar, baz`
  */

  console.log(response);
  // Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream

  console.log(await parser.parse(response));
  /*
  [
    'Vanilla',
    'Chocolate',
    'Strawberry',
    'Mint Chocolate Chip',
    'Cookies and Cream'
  ]
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/custom_list_parser

# Custom list parser
This output parser can be used when you want to return a list of items with a specific length and separator.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { CustomListOutputParser } from "langchain/output_parsers";

// With a `CustomListOutputParser`, we can parse a list with a specific length and separator.
const parser = new CustomListOutputParser({ length: 3, separator: "\n" });

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template: "Provide a list of {subject}.\n{format_instructions}",
  inputVariables: ["subject"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  subject: "great fiction books (book, author)",
});

const response = await model.call(input);

console.log(input);
/*
Provide a list of great fiction books (book, author).
Your response should be a list of 3 items separated by "\n" (eg: `foo\n bar\n baz`)
*/

console.log(response);
/*
The Catcher in the Rye, J.D. Salinger
To Kill a Mockingbird, Harper Lee
The Great Gatsby, F. Scott Fitzgerald
*/

console.log(await parser.parse(response));
/*
[
  'The Catcher in the Rye, J.D. Salinger',
  'To Kill a Mockingbird, Harper Lee',
  'The Great Gatsby, F. Scott Fitzgerald'
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser

# Auto-fixing parser
This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.
But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.
For this example, we'll use the structured output parser. Here's what happens if we pass it a result that does not comply with the schema:
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  StructuredOutputParser,
  OutputFixingParser,
} from "langchain/output_parsers";

export const run = async () => {
  const parser = StructuredOutputParser.fromZodSchema(
    z.object({
      answer: z.string().describe("answer to the user's question"),
      sources: z
        .array(z.string())
        .describe("sources used to answer the question, should be websites."),
    })
  );
  /** This is a bad output because sources is a string, not a list */
  const badOutput = `\`\`\`json
  {
    "answer": "foo",
    "sources": "foo.com"
  }
  \`\`\``;
  try {
    await parser.parse(badOutput);
  } catch (e) {
    console.log("Failed to parse bad output: ", e);
    /*
    Failed to parse bad output:  OutputParserException [Error]: Failed to parse. Text: ```json
      {
        "answer": "foo",
        "sources": "foo.com"
      }
      ```. Error: [
      {
        "code": "invalid_type",
        "expected": "array",
        "received": "string",
        "path": [
          "sources"
        ],
        "message": "Expected array, received string"
      }
    ]
    at StructuredOutputParser.parse (/Users/ankushgola/Code/langchainjs/langchain/src/output_parsers/structured.ts:71:13)
    at run (/Users/ankushgola/Code/langchainjs/examples/src/prompts/fix_parser.ts:25:18)
    at <anonymous> (/Users/ankushgola/Code/langchainjs/examples/src/index.ts:33:22)
   */
  }
  const fixParser = OutputFixingParser.fromLLM(
    new ChatOpenAI({ temperature: 0 }),
    parser
  );
  const output = await fixParser.parse(badOutput);
  console.log("Fixed output: ", output);
  // Fixed output:  { answer: 'foo', sources: [ 'foo.com' ] }
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/model_io/output_parsers/structured

# Structured output parser
This output parser can be used when you want to return multiple fields. If you want complex schema returned (i.e. a JSON object with arrays of strings), use the Zod Schema detailed below.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// With a `StructuredOutputParser` we can define a schema for the output.
const parser = StructuredOutputParser.fromNamesAndDescriptions({
  answer: "answer to the user's question",
  source: "source used to answer the user's question, should be a website.",
});

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
You must format your output as a JSON value that adheres to a given "JSON Schema" instance.

"JSON Schema" is a declarative language that allows you to annotate and validate JSON documents.

For example, the example "JSON Schema" instance {{"properties": {{"foo": {{"description": "a list of test words", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
would match an object with one required property, "foo". The "type" property specifies "foo" must be an "array", and the "description" property semantically describes it as "a list of test words". The items within "foo" must be strings.
Thus, the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of this example "JSON Schema". The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!

Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:
```json
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"source":{"type":"string","description":"source used to answer the user's question, should be a website."}},"required":["answer","source"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "source": "https://en.wikipedia.org/wiki/Paris"}
*/

console.log(await parser.parse(response));
// { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/Paris' }
```
#### API Reference:
## Structured Output Parser with Zod Schema​
This output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. z.date() is not allowed.
```typescript
import { z } from "zod";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { StructuredOutputParser } from "langchain/output_parsers";

// We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.
const parser = StructuredOutputParser.fromZodSchema(
  z.object({
    answer: z.string().describe("answer to the user's question"),
    sources: z
      .array(z.string())
      .describe("sources used to answer the question, should be websites."),
  })
);

const formatInstructions = parser.getFormatInstructions();

const prompt = new PromptTemplate({
  template:
    "Answer the users question as best as possible.\n{format_instructions}\n{question}",
  inputVariables: ["question"],
  partialVariables: { format_instructions: formatInstructions },
});

const model = new OpenAI({ temperature: 0 });

const input = await prompt.format({
  question: "What is the capital of France?",
});
const response = await model.call(input);

console.log(input);
/*
Answer the users question as best as possible.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}}}
the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.

Here is the output schema:
```
{"type":"object","properties":{"answer":{"type":"string","description":"answer to the user's question"},"sources":{"type":"array","items":{"type":"string"},"description":"sources used to answer the question, should be websites."}},"required":["answer","sources"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}
```

What is the capital of France?
*/

console.log(response);
/*
{"answer": "Paris", "sources": ["https://en.wikipedia.org/wiki/Paris"]}
*/

console.log(await parser.parse(response));
/*
{ answer: 'Paris', sources: [ 'https://en.wikipedia.org/wiki/Paris' ] }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/

# Data connection
Many LLM applications require user-specific data that is not part of the model's training set. LangChain gives you the
building blocks to load, transform, store and query your data via:




Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/

# Document loaders
Use document loaders to load data from a source as Document's. A Document is a piece of text
and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text
contents of any web page, or even for loading a transcript of a YouTube video.
Document loaders expose a "load" method for loading data as documents from a configured source. They optionally
implement a "lazy load" as well for lazily loading data into memory.
## Get started​
The simplest loader reads in a file as text and places it all into one Document.
```typescript
import { TextLoader } from "langchain/document_loaders/fs/text";

const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/creating_documents

# Creating documents
A document at its core is fairly simple. It consists of a piece of text and optional metadata. The piece of text is what we interact with the language model, while the optional metadata is useful for keeping track of metadata about the document (such as the source).
```typescript
interface Document {
  pageContent: string;
  metadata: Record<string, any>;
}
```
You can create a document object rather easily in LangChain with:
```typescript
import { Document } from "langchain/document";

const doc = new Document({ pageContent: "foo" });
```
You can create one with metadata with:
```typescript
import { Document } from "langchain/document";

const doc = new Document({ pageContent: "foo", metadata: { source: "1" } });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/csv

# CSV
A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.
Load CSV data with a single row per document.
## Setup​
```typescript
npm install d3-dsv@2
```
## Usage, extracting all columns​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader("src/document_loaders/example_data/example.csv");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 1
text: This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 2
text: This is another sentence.",
  },
]
*/
```
## Usage, extracting a single column​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader(
  "src/document_loaders/example_data/example.csv",
  "text"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/custom

# Custom document loaders
If you want to implement your own Document Loader, you have a few options.
### Subclassing BaseDocumentLoader​
You can extend the BaseDocumentLoader class directly. The BaseDocumentLoader class provides a few convenience methods for loading documents from a variety of sources.
```typescript
abstract class BaseDocumentLoader implements DocumentLoader {
  abstract load(): Promise<Document[]>;
}
```
### Subclassing TextLoader​
If you want to load documents from a text file, you can extend the TextLoader class. The TextLoader class takes care of reading the file, so all you have to do is implement a parse method.
```typescript
abstract class TextLoader extends BaseDocumentLoader {
  abstract parse(raw: string): Promise<string[]>;
}
```
### Subclassing BufferLoader​
If you want to load documents from a binary file, you can extend the BufferLoader class. The BufferLoader class takes care of reading the file, so all you have to do is implement a parse method.
```typescript
abstract class BufferLoader extends BaseDocumentLoader {
  abstract parse(
    raw: Buffer,
    metadata: Document["metadata"]
  ): Promise<Document[]>;
}
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/file_directory

# File Directory
This covers how to load all documents in a directory.
The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.
Example folder:
```typescript
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
```
Example code:
```typescript
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new DirectoryLoader(
  "src/document_loaders/example_data/example",
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);
const docs = await loader.load();
console.log({ docs });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/json

# JSON
JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).
JSON Lines is a file format where each line is a valid JSON value.
The JSON loader uses JSON pointer to target keys in your JSON files you want to target.
### No JSON pointer example​
The most simple way of using it is to specify no JSON pointer.
The loader will load all strings it finds in the JSON object.
Example JSON file:
```typescript
{
  "texts": ["This is a sentence.", "This is another sentence."]
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader("src/document_loaders/example_data/example.json");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```
### Using JSON pointer example​
You can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.
In this example, we want to only extract information from "from" and "surname" entries.
```typescript
{
  "1": {
    "body": "BD 2023 SUMMER",
    "from": "LinkedIn Job",
    "labels": ["IMPORTANT", "CATEGORY_UPDATES", "INBOX"]
  },
  "2": {
    "body": "Intern, Treasury and other roles are available",
    "from": "LinkedIn Job2",
    "labels": ["IMPORTANT"],
    "other": {
      "name": "plop",
      "surname": "bob"
    }
  }
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader(
  "src/document_loaders/example_data/example.json",
  ["/from", "/surname"]
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "BD 2023 SUMMER",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "LinkedIn Job",
  },
  ...
]
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/how_to/pdf

# PDF
Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.
This covers how to load PDF documents into the Document format that we use downstream.
By default, one document will be created for each page in the PDF file. You can change this behavior by setting the splitPages option to false.
## Setup​
```typescript
npm install pdf-parse
```
## Usage, one document per page​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf");

const docs = await loader.load();
```
## Usage, one document per file​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  splitPages: false,
});

const docs = await loader.load();
```
## Usage, custom pdfjs build​
By default we use the pdfjs build bundled with pdf-parse, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of pdfjs-dist or if you want to use a custom build of pdfjs-dist, you can do so by providing a custom pdfjs function that returns a promise that resolves to the PDFJS object.
In the following example we use the "legacy" (see pdfjs docs) build of pdfjs-dist, which includes several polyfills not included in the default build.
```typescript
npm install pdfjs-dist
```
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  // you may need to add `.then(m => m.default)` to the end of the import
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/

# File Loaders
Only available on Node.js.
These loaders are used to load files given a filesystem path or a Blob object.
## 📄️ Folders with multiple files
This example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.
## 📄️ CSV files
This example goes over how to load data from CSV files. The second argument is the column name to extract from the CSV file. One document will be created for each row in the CSV file. When column is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's pageContent. When column is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.
## 📄️ Docx files
This example goes over how to load data from docx files.
## 📄️ EPUB files
This example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the splitChapters option to false.
## 📄️ JSON files
The JSON loader use JSON pointer to target keys in your JSON files you want to target.
## 📄️ JSONLines files
This example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.
## 📄️ Notion markdown export
This example goes over how to load data from your Notion pages exported from the notion dashboard.
## 📄️ PDF files
This example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the splitPages option to false.
## 📄️ Subtitles
This example goes over how to load data from subtitle files. One document will be created for each subtitles file.
## 📄️ Text files
This example goes over how to load data from text files.
## 📄️ Unstructured
This example covers how to use Unstructured to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/directory

# Folders with multiple files
This example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.
Example folder:
```typescript
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
```
Example code:
```typescript
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";
import {
  JSONLoader,
  JSONLinesLoader,
} from "langchain/document_loaders/fs/json";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new DirectoryLoader(
  "src/document_loaders/example_data/example",
  {
    ".json": (path) => new JSONLoader(path, "/texts"),
    ".jsonl": (path) => new JSONLinesLoader(path, "/html"),
    ".txt": (path) => new TextLoader(path),
    ".csv": (path) => new CSVLoader(path, "text"),
  }
);
const docs = await loader.load();
console.log({ docs });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/csv

# CSV files
This example goes over how to load data from CSV files. The second argument is the column name to extract from the CSV file. One document will be created for each row in the CSV file. When column is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's pageContent. When column is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.
## Setup​
```typescript
npm install d3-dsv@2
```
```typescript
yarn add d3-dsv@2
```
```typescript
pnpm add d3-dsv@2
```
## Usage, extracting all columns​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader("src/document_loaders/example_data/example.csv");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 1
text: This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "id: 2
text: This is another sentence.",
  },
]
*/
```
## Usage, extracting a single column​
Example CSV file:
```typescript
id,text
1,This is a sentence.
2,This is another sentence.
```
Example code:
```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader(
  "src/document_loaders/example_data/example.csv",
  "text"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "line": 1,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "line": 2,
      "source": "src/document_loaders/example_data/example.csv",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/docx

# Docx files
This example goes over how to load data from docx files.
# Setup
```typescript
npm install mammoth
```
```typescript
yarn add mammoth
```
```typescript
pnpm add mammoth
```
# Usage
```typescript
import { DocxLoader } from "langchain/document_loaders/fs/docx";

const loader = new DocxLoader(
  "src/document_loaders/tests/example_data/attention.docx"
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/epub

# EPUB files
This example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the splitChapters option to false.
# Setup
```typescript
npm install epub2 html-to-text
```
```typescript
yarn add epub2 html-to-text
```
```typescript
pnpm add epub2 html-to-text
```
# Usage, one document per chapter
```typescript
import { EPubLoader } from "langchain/document_loaders/fs/epub";

const loader = new EPubLoader("src/document_loaders/example_data/example.epub");

const docs = await loader.load();
```
# Usage, one document per file
```typescript
import { EPubLoader } from "langchain/document_loaders/fs/epub";

const loader = new EPubLoader(
  "src/document_loaders/example_data/example.epub",
  {
    splitChapters: false,
  }
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/json

# JSON files
The JSON loader use JSON pointer to target keys in your JSON files you want to target.
### No JSON pointer example​
The most simple way of using it, is to specify no JSON pointer.
The loader will load all strings it finds in the JSON object.
Example JSON file:
```typescript
{
  "texts": ["This is a sentence.", "This is another sentence."]
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader("src/document_loaders/example_data/example.json");

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```
### Using JSON pointer example​
You can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.
In this example, we want to only extract information from "from" and "surname" entries.
```typescript
{
  "1": {
    "body": "BD 2023 SUMMER",
    "from": "LinkedIn Job",
    "labels": ["IMPORTANT", "CATEGORY_UPDATES", "INBOX"]
  },
  "2": {
    "body": "Intern, Treasury and other roles are available",
    "from": "LinkedIn Job2",
    "labels": ["IMPORTANT"],
    "other": {
      "name": "plop",
      "surname": "bob"
    }
  }
}
```
Example code:
```typescript
import { JSONLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLoader(
  "src/document_loaders/example_data/example.json",
  ["/from", "/surname"]
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "BD 2023 SUMMER",
  },
  Document {
    "metadata": {
      "blobType": "application/json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "LinkedIn Job",
  },
  ...
]
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/jsonlines

# JSONLines files
This example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.
Example JSONLines file:
```typescript
{"html": "This is a sentence."}
{"html": "This is another sentence."}
```
Example code:
```typescript
import { JSONLinesLoader } from "langchain/document_loaders/fs/json";

const loader = new JSONLinesLoader(
  "src/document_loaders/example_data/example.jsonl",
  "/html"
);

const docs = await loader.load();
/*
[
  Document {
    "metadata": {
      "blobType": "application/jsonl+json",
      "line": 1,
      "source": "blob",
    },
    "pageContent": "This is a sentence.",
  },
  Document {
    "metadata": {
      "blobType": "application/jsonl+json",
      "line": 2,
      "source": "blob",
    },
    "pageContent": "This is another sentence.",
  },
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/notion_markdown

# Notion markdown export
This example goes over how to load data from your Notion pages exported from the notion dashboard.
First, export your notion pages as Markdown & CSV as per the offical explanation here. Make sure to select include subpages and Create folders for subpages.
Then, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.
Once the folder is in your repository, simply run the example below:
```typescript
import { NotionLoader } from "langchain/document_loaders/fs/notion";

export const run = async () => {
  /** Provide the directory path of your notion folder */
  const directoryPath = "Notion_DB";
  const loader = new NotionLoader(directoryPath);
  const docs = await loader.load();
  console.log({ docs });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/pdf

# PDF files
This example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the splitPages option to false.
## Setup​
```typescript
npm install pdf-parse
```
```typescript
yarn add pdf-parse
```
```typescript
pnpm add pdf-parse
```
## Usage, one document per page​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf");

const docs = await loader.load();
```
## Usage, one document per file​
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  splitPages: false,
});

const docs = await loader.load();
```
## Usage, custom pdfjs build​
By default we use the pdfjs build bundled with pdf-parse, which is compatible with most environments, including Node.js and modern browsers. If you want to use a more recent version of pdfjs-dist or if you want to use a custom build of pdfjs-dist, you can do so by providing a custom pdfjs function that returns a promise that resolves to the PDFJS object.
In the following example we use the "legacy" (see pdfjs docs) build of pdfjs-dist, which includes several polyfills not included in the default build.
```typescript
npm install pdfjs-dist
```
```typescript
yarn add pdfjs-dist
```
```typescript
pnpm add pdfjs-dist
```
```typescript
import { PDFLoader } from "langchain/document_loaders/fs/pdf";

const loader = new PDFLoader("src/document_loaders/example_data/example.pdf", {
  // you may need to add `.then(m => m.default)` to the end of the import
  pdfjs: () => import("pdfjs-dist/legacy/build/pdf.js"),
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/subtitles

# Subtitles
This example goes over how to load data from subtitle files. One document will be created for each subtitles file.
## Setup​
```typescript
npm install srt-parser-2
```
```typescript
yarn add srt-parser-2
```
```typescript
pnpm add srt-parser-2
```
## Usage​
```typescript
import { SRTLoader } from "langchain/document_loaders/fs/srt";

const loader = new SRTLoader(
  "src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/text

# Text files
This example goes over how to load data from text files.
```typescript
import { TextLoader } from "langchain/document_loaders/fs/text";

const loader = new TextLoader("src/document_loaders/example_data/example.txt");

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/file_loaders/unstructured

# Unstructured
This example covers how to use Unstructured to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.
## Setup​
You can run Unstructured locally in your computer using Docker. To do so, you need to have Docker installed. You can find the instructions to install Docker here.
```typescript
docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0
```
## Usage​
Once Unstructured is running, you can use it to load files from your computer. You can use the following code to load a file from your computer.
```typescript
import { UnstructuredLoader } from "langchain/document_loaders/fs/unstructured";

const options = {
  apiKey: "MY_API_KEY",
};

const loader = new UnstructuredLoader(
  "src/document_loaders/example_data/notion.md",
  options
);
const docs = await loader.load();
```
#### API Reference:
## Directories​
You can also load all of the files in the directory using UnstructuredDirectoryLoader, which inherits from DirectoryLoader:
```typescript
import { UnstructuredDirectoryLoader } from "langchain/document_loaders/fs/unstructured";

const options = {
  apiKey: "MY_API_KEY",
};

const loader = new UnstructuredDirectoryLoader(
  "langchain/src/document_loaders/tests/example_data",
  options
);
const docs = await loader.load();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/

# Web Loaders
These loaders are used to load web resources.
## 📄️ Cheerio
This example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.
## 📄️ Puppeteer
Only available on Node.js.
## 📄️ Playwright
Only available on Node.js.
## 📄️ Apify Dataset
This guide shows how to use Apify with LangChain to load documents from an Apify Dataset.
## 📄️ Azure Blob Storage Container
Only available on Node.js.
## 📄️ Azure Blob Storage File
Only available on Node.js.
## 📄️ College Confidential
This example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.
## 📄️ Confluence
Only available on Node.js.
## 📄️ Figma
This example goes over how to load data from a Figma file.
## 📄️ GitBook
This example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.
## 📄️ GitHub
This example goes over how to load data from a GitHub repository.
## 📄️ Hacker News
This example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.
## 📄️ IMSDB
This example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.
## 📄️ Notion API
This guide will take you through the steps required to load documents from Notion pages and databases using the Notion API.
## 📄️ S3 File
Only available on Node.js.
## 📄️ SerpAPI Loader
This guide shows how to use SerpAPI with LangChain to load web search results.
## 📄️ Sonix Audio
Only available on Node.js.
## 📄️ Blockchain Data
This example shows how to load blockchain data, including NFT metadata and transactions for a contract address, via the sort.xyz SQL API.



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/web_cheerio

# Webpages, with Cheerio
This example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.
Cheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio to extract data from web pages, without having to render them in a browser.
However, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the PlaywrightWebBaseLoader or PuppeteerWebBaseLoader instead.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";

const loader = new CheerioWebBaseLoader(
  "https://news.ycombinator.com/item?id=34817881"
);

const docs = await loader.load();
```
## Usage, with a custom selector​
```typescript
import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";

const loader = new CheerioWebBaseLoader(
  "https://news.ycombinator.com/item?id=34817881",
  {
    selector: "p.athing",
  }
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/web_puppeteer

# Webpages, with Puppeteer
Only available on Node.js.
This example goes over how to load data from webpages using Puppeteer. One document will be created for each webpage.
Puppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.
If you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the CheerioWebBaseLoader instead.
## Setup​
```typescript
npm install puppeteer
```
```typescript
yarn add puppeteer
```
```typescript
pnpm add puppeteer
```
## Usage​
```typescript
import { PuppeteerWebBaseLoader } from "langchain/document_loaders/web/puppeteer";

/**
 * Loader uses `page.evaluate(() => document.body.innerHTML)`
 * as default evaluate function
 **/
const loader = new PuppeteerWebBaseLoader("https://www.tabnews.com.br/");

const docs = await loader.load();
```
## Options​
Here's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:
```typescript
type PuppeteerWebBaseLoaderOptions = {
  launchOptions?: PuppeteerLaunchOptions;
  gotoOptions?: PuppeteerGotoOptions;
  evaluate?: (page: Page, browser: Browser) => Promise<string>;
};
```
launchOptions: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.
gotoOptions: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.
evaluate: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.
By passing these options to the PuppeteerWebBaseLoader constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.
Here is a basic example to do it:
```typescript
import { PuppeteerWebBaseLoader } from "langchain/document_loaders/web/puppeteer";

const loader = new PuppeteerWebBaseLoader("https://www.tabnews.com.br/", {
  launchOptions: {
    headless: true,
  },
  gotoOptions: {
    waitUntil: "domcontentloaded",
  },
  /** Pass custom evaluate, in this case you get page and browser instances */
  async evaluate(page: Page, browser: Browser) {
    await page.waitForResponse("https://www.tabnews.com.br/va/view");

    const result = await page.evaluate(() => document.body.innerHTML);
    return result;
  },
});

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/web_playwright

# Webpages, with Playwright
Only available on Node.js.
This example goes over how to load data from webpages using Playwright. One document will be created for each webpage.
Playwright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.
If you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the CheerioWebBaseLoader instead.
## Setup​
```typescript
npm install playwright
```
```typescript
yarn add playwright
```
```typescript
pnpm add playwright
```
## Usage​
```typescript
import { PlaywrightWebBaseLoader } from "langchain/document_loaders/web/playwright";

/**
 * Loader uses `page.content()`
 * as default evaluate function
 **/
const loader = new PlaywrightWebBaseLoader("https://www.tabnews.com.br/");

const docs = await loader.load();
```
## Options​
Here's an explanation of the parameters you can pass to the PlaywrightWebBaseLoader constructor using the PlaywrightWebBaseLoaderOptions interface:
```typescript
type PlaywrightWebBaseLoaderOptions = {
  launchOptions?: LaunchOptions;
  gotoOptions?: PlaywrightGotoOptions;
  evaluate?: PlaywrightEvaluate;
};
```
launchOptions: an optional object that specifies additional options to pass to the playwright.chromium.launch() method. This can include options such as the headless flag to launch the browser in headless mode.
gotoOptions: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.
evaluate: an optional function that can be used to evaluate JavaScript code on the page using a custom evaluation function. This can be useful for extracting data from the page, interacting with page elements, or handling specific HTTP responses. The function should return a Promise that resolves to a string containing the result of the evaluation.
By passing these options to the PlaywrightWebBaseLoader constructor, you can customize the behavior of the loader and use Playwright's powerful features to scrape and interact with web pages.
Here is a basic example to do it:
```typescript
import {
  PlaywrightWebBaseLoader,
  Page,
  Browser,
} from "langchain/document_loaders/web/playwright";

const url = "https://www.tabnews.com.br/";
const loader = new PlaywrightWebBaseLoader(url);
const docs = await loader.load();

// raw HTML page content
const extractedContents = docs[0].pageContent;
```
And a more advanced example:
```typescript
import {
  PlaywrightWebBaseLoader,
  Page,
  Browser,
} from "langchain/document_loaders/web/playwright";

const loader = new PlaywrightWebBaseLoader("https://www.tabnews.com.br/", {
  launchOptions: {
    headless: true,
  },
  gotoOptions: {
    waitUntil: "domcontentloaded",
  },
  /** Pass custom evaluate, in this case you get page and browser instances */
  async evaluate(page: Page, browser: Browser, response: Response | null) {
    await page.waitForResponse("https://www.tabnews.com.br/va/view");

    const result = await page.evaluate(() => document.body.innerHTML);
    return result;
  },
});

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/apify_dataset

# Apify Dataset
This guide shows how to use Apify with LangChain to load documents from an Apify Dataset.
## Overview​
Apify is a cloud platform for web scraping and data extraction,
which provides an ecosystem of more than a thousand
ready-made apps called Actors for various web scraping, crawling, and data extraction use cases.
This guide shows how to load documents
from an Apify Dataset — a scalable append-only
storage built for storing structured web scraping results,
such as a list of products or Google SERPs, and then export them to various
formats like JSON, CSV, or Excel.
Datasets are typically used to save results of Actors.
For example, Website Content Crawler Actor
deeply crawls websites such as documentation, knowledge bases, help centers, or blogs,
and then stores the text content of webpages into a dataset,
from which you can feed the documents into a vector index and answer questions from it.
## Setup​
You'll first need to install the official Apify client:
```typescript
npm install apify-client
```
```typescript
yarn add apify-client
```
```typescript
pnpm add apify-client
```
You'll also need to sign up and retrieve your Apify API token.
## Usage​
### From a New Dataset​
If you don't already have an existing dataset on the Apify platform, you'll need to initialize the document loader by calling an Actor and waiting for the results.
Note: Calling an Actor can take a significant amount of time, on the order of hours, or even days for large sites!
Here's an example:
```typescript
import { ApifyDatasetLoader } from "langchain/document_loaders/web/apify_dataset";
import { Document } from "langchain/document";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RetrievalQAChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";

/*
 * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
 * In the below example, the Apify dataset format looks like this:
 * {
 *   "url": "https://apify.com",
 *   "text": "Apify is the best web scraping and automation platform."
 * }
 */
const loader = await ApifyDatasetLoader.fromActorCall(
  "apify/website-content-crawler",
  {
    startUrls: [{ url: "https://js.langchain.com/docs/" }],
  },
  {
    datasetMappingFunction: (item) =>
      new Document({
        pageContent: (item.text || "") as string,
        metadata: { source: item.url },
      }),
    clientOptions: {
      token: "your-apify-token", // Or set as process.env.APIFY_API_TOKEN
    },
  }
);

const docs = await loader.load();

const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const model = new OpenAI({
  temperature: 0,
});

const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {
  returnSourceDocuments: true,
});
const res = await chain.call({ query: "What is LangChain?" });

console.log(res.text);
console.log(res.sourceDocuments.map((d: Document) => d.metadata.source));

/*
  LangChain is a framework for developing applications powered by language models.
  [
    'https://js.langchain.com/docs/',
    'https://js.langchain.com/docs/modules/chains/',
    'https://js.langchain.com/docs/modules/chains/llmchain/',
    'https://js.langchain.com/docs/category/functions-4'
  ]
*/
```
#### API Reference:
## From an Existing Dataset​
If you already have an existing dataset on the Apify platform, you can initialize the document loader with the constructor directly:
```typescript
import { ApifyDatasetLoader } from "langchain/document_loaders/web/apify_dataset";
import { Document } from "langchain/document";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RetrievalQAChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";

/*
 * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
 * In the below example, the Apify dataset format looks like this:
 * {
 *   "url": "https://apify.com",
 *   "text": "Apify is the best web scraping and automation platform."
 * }
 */
const loader = new ApifyDatasetLoader("your-dataset-id", {
  datasetMappingFunction: (item) =>
    new Document({
      pageContent: (item.text || "") as string,
      metadata: { source: item.url },
    }),
  clientOptions: {
    token: "your-apify-token", // Or set as process.env.APIFY_API_TOKEN
  },
});

const docs = await loader.load();

const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const model = new OpenAI({
  temperature: 0,
});

const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {
  returnSourceDocuments: true,
});
const res = await chain.call({ query: "What is LangChain?" });

console.log(res.text);
console.log(res.sourceDocuments.map((d: Document) => d.metadata.source));

/*
  LangChain is a framework for developing applications powered by language models.
  [
    'https://js.langchain.com/docs/',
    'https://js.langchain.com/docs/modules/chains/',
    'https://js.langchain.com/docs/modules/chains/llmchain/',
    'https://js.langchain.com/docs/category/functions-4'
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/azure_blob_storage_container

# Azure Blob Storage Container
Only available on Node.js.
This covers how to load document objects from a container on Azure Blob Storage.
## Setup​
To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.
See the docs here for information on how to do that.
You'll also need to install the official Azure Storage Blob client library:
```typescript
npm install @azure/storage-blob
```
```typescript
yarn add @azure/storage-blob
```
```typescript
pnpm add @azure/storage-blob
```
## Usage​
Once Unstructured is configured, you can use the Azure Blob Storage Container loader to load files and then convert them into a Document.
```typescript
import { AzureBlobStorageContainerLoader } from "langchain/document_loaders/web/azure_blob_storage_container";

const loader = new AzureBlobStorageContainerLoader({
  azureConfig: {
    connectionString: "",
    container: "container_name",
  },
  unstructuredConfig: {
    apiUrl: "http://localhost:8000/general/v0/general",
    apiKey: "", // this will be soon required
  },
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/azure_blob_storage_file

# Azure Blob Storage File
Only available on Node.js.
This covers how to load document objects from a Azure Files.
## Setup​
To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.
See the docs here for information on how to do that.
You'll also need to install the official Azure Storage Blob client library:
```typescript
npm install @azure/storage-blob
```
```typescript
yarn add @azure/storage-blob
```
```typescript
pnpm add @azure/storage-blob
```
## Usage​
Once Unstructured is configured, you can use the Azure Blob Storage File loader to load files and then convert them into a Document.
```typescript
import { AzureBlobStorageFileLoader } from "langchain/document_loaders/web/azure_blob_storage_file";

const loader = new AzureBlobStorageFileLoader({
  azureConfig: {
    connectionString: "",
    container: "container_name",
    blobName: "example.txt",
  },
  unstructuredConfig: {
    apiUrl: "http://localhost:8000/general/v0/general",
    apiKey: "", // this will be soon required
  },
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/college_confidential

# College Confidential
This example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { CollegeConfidentialLoader } from "langchain/document_loaders/web/college_confidential";

const loader = new CollegeConfidentialLoader(
  "https://www.collegeconfidential.com/colleges/brown-university/"
);

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/confluence

# Confluence
Only available on Node.js.
This covers how to load document objects from pages in a Confluence space.
## Credentials​
```typescript
npm install html-to-text
```
```typescript
yarn add html-to-text
```
```typescript
pnpm add html-to-text
```
## Usage​
```typescript
import { ConfluencePagesLoader } from "langchain/document_loaders/web/confluence";

const username = process.env.CONFLUENCE_USERNAME;
const accessToken = process.env.CONFLUENCE_ACCESS_TOKEN;

if (username && accessToken) {
  const loader = new ConfluencePagesLoader({
    baseUrl: "https://example.atlassian.net/wiki",
    spaceKey: "~EXAMPLE362906de5d343d49dcdbae5dEXAMPLE",
    username,
    accessToken,
  });

  const documents = await loader.load();
  console.log(documents);
} else {
  console.log(
    "You must provide a username and access token to run this example."
  );
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/figma

# Figma
This example goes over how to load data from a Figma file.
You will need a Figma access token in order to get started.
```typescript
import { FigmaFileLoader } from "langchain/document_loaders/web/figma";

const loader = new FigmaFileLoader({
  accessToken: "FIGMA_ACCESS_TOKEN", // or load it from process.env.FIGMA_ACCESS_TOKEN
  nodeIds: ["id1", "id2", "id3"],
  fileKey: "key",
});
const docs = await loader.load();

console.log({ docs });
```
#### API Reference:
You can find your Figma file's key and node ids by opening the file in your browser and extracting them from the URL:
```typescript
https://www.figma.com/file/<YOUR FILE KEY HERE>/LangChainJS-Test?type=whiteboard&node-id=<YOUR NODE ID HERE>&t=e6lqWkKecuYQRyRg-0
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/gitbook

# GitBook
This example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Load from single GitBook page​
```typescript
import { GitbookLoader } from "langchain/document_loaders/web/gitbook";

const loader = new GitbookLoader(
  "https://docs.gitbook.com/product-tour/navigation"
);

const docs = await loader.load();
```
## Load from all paths in a given GitBook​
For this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have shouldLoadAllPaths set to true.
```typescript
import { GitbookLoader } from "langchain/document_loaders/web/gitbook";

const loader = new GitbookLoader("https://docs.gitbook.com", {
  shouldLoadAllPaths: true,
});

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/github

# GitHub
This example goes over how to load data from a GitHub repository.
You can set the GITHUB_ACCESS_TOKEN environment variable to a GitHub access token to increase the rate limit and access private repositories.
## Setup​
The GitHub loader requires the ignore npm package as a peer dependency. Install it like this:
```typescript
npm install ignore
```
```typescript
yarn add ignore
```
```typescript
pnpm add ignore
```
## Usage​
```typescript
import { GithubRepoLoader } from "langchain/document_loaders/web/github";

export const run = async () => {
  const loader = new GithubRepoLoader(
    "https://github.com/hwchase17/langchainjs",
    { branch: "main", recursive: false, unknown: "warn" }
  );
  const docs = await loader.load();
  console.log({ docs });
};
```
#### API Reference:
The loader will ignore binary files like images.
### Using .gitignore syntax​
To ignore specific files, you can pass in an ignorePaths array into the constructor:
```typescript
import { GithubRepoLoader } from "langchain/document_loaders/web/github";

export const run = async () => {
  const loader = new GithubRepoLoader(
    "https://github.com/hwchase17/langchainjs",
    { branch: "main", recursive: false, unknown: "warn", ignorePaths: ["*.md"] }
  );
  const docs = await loader.load();
  console.log({ docs });
  // Will not include any .md files
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/hn

# Hacker News
This example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { HNLoader } from "langchain/document_loaders/web/hn";

const loader = new HNLoader("https://news.ycombinator.com/item?id=34817881");

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/imsdb

# IMSDB
This example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.
## Setup​
```typescript
npm install cheerio
```
```typescript
yarn add cheerio
```
```typescript
pnpm add cheerio
```
## Usage​
```typescript
import { IMSDBLoader } from "langchain/document_loaders/web/imsdb";

const loader = new IMSDBLoader("https://imsdb.com/scripts/BlacKkKlansman.html");

const docs = await loader.load();
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/notionapi

# Notion API
This guide will take you through the steps required to load documents from Notion pages and databases using the Notion API.
## Overview​
Notion is a versatile productivity platform that consolidates note-taking, task management, and data organization tools into one interface.
This document loader is able to take full Notion pages and databases and turn them into a LangChain Documents ready to be integrated into your projects.
## Setup​
```typescript
npm install @notionhq/client notion-to-md
```
```typescript
yarn add @notionhq/client notion-to-md
```
```typescript
pnpm add @notionhq/client notion-to-md
```
The 32 char hex in the url path represents the ID. For example:
PAGE_ID: https://www.notion.so/skarard/LangChain-Notion-API-b34ca03f219c4420a6046fc4bdfdf7b4
DATABASE_ID: https://www.notion.so/skarard/c393f19c3903440da0d34bf9c6c12ff2?v=9c70a0f4e174498aa0f9021e0a9d52de
REGEX: /(?<!=)[0-9a-f]{32}/
## Example Usage​
```typescript
import { NotionAPILoader } from "langchain/document_loaders/web/notionapi";

// Loading a page (including child pages all as separate documents)
const pageLoader = new NotionAPILoader({
  clientOptions: {
    auth: "<NOTION_INTEGRATION_TOKEN>",
  },
  id: "<PAGE_ID>",
  type: "page",
});

// A page contents is likely to be more than 1000 characters so it's split into multiple documents (important for vectorization)
const pageDocs = await pageLoader.loadAndSplit();

console.log({ pageDocs });

// Loading a database (each row is a separate document with all properties as metadata)
const dbLoader = new NotionAPILoader({
  clientOptions: {
    auth: "<NOTION_INTEGRATION_TOKEN>",
  },
  id: "<DATABASE_ID>",
  type: "database",
});

// A database row contents is likely to be less than 1000 characters so it's not split into multiple documents
const dbDocs = await dbLoader.load();

console.log({ dbDocs });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/s3

# S3 File
Only available on Node.js.
This covers how to load document objects from an s3 file object.
## Setup​
To run this index you'll need to have Unstructured already set up and ready to use at an available URL endpoint. It can also be configured to run locally.
See the docs here for information on how to do that.
You'll also need to install the official AWS SDK:
```typescript
npm install @aws-sdk/client-s3
```
```typescript
yarn add @aws-sdk/client-s3
```
```typescript
pnpm add @aws-sdk/client-s3
```
## Usage​
Once Unstructured is configured, you can use the S3 loader to load files and then convert them into a Document.
You can optionally provide a s3Config parameter to specify your bucket region, access key, and secret access key. If these are not provided, you will need to have them in your environment (e.g., by running aws configure).
```typescript
import { S3Loader } from "langchain/document_loaders/web/s3";

const loader = new S3Loader({
  bucket: "my-document-bucket-123",
  key: "AccountingOverview.pdf",
  s3Config: {
    region: "us-east-1",
    credentials: {
      accessKeyId: "AKIAIOSFODNN7EXAMPLE",
      secretAccessKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY",
    },
  },
  unstructuredAPIURL: "http://localhost:8000/general/v0/general",
  unstructuredAPIKey: "", // this will be soon required
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/serpapi

# SerpAPI Loader
This guide shows how to use SerpAPI with LangChain to load web search results.
## Overview​
SerpAPI is a real-time API that provides access to search results from various search engines. It is commonly used for tasks like competitor analysis and rank tracking. It empowers businesses to scrape, extract, and make sense of data from all search engines' result pages.
This guide shows how to load web search results using the SerpAPILoader in LangChain. The SerpAPILoader simplifies the process of loading and processing web search results from SerpAPI.
## Setup​
You'll need to sign up and retrieve your SerpAPI API key.
## Usage​
Here's an example of how to use the SerpAPILoader:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SerpAPILoader } from "langchain/document_loaders/web/serpapi";

// Initialize the necessary components
const llm = new OpenAI();
const embeddings = new OpenAIEmbeddings();
const apiKey = "Your SerpAPI API key";

// Define your question and query
const question = "Your question here";
const query = "Your query here";

// Use SerpAPILoader to load web search results
const loader = new SerpAPILoader({ q: query, apiKey });
const docs = await loader.load();

// Use MemoryVectorStore to store the loaded documents in memory
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Use RetrievalQAChain to retrieve documents and answer the question
const chain = RetrievalQAChain.fromLLM(llm, vectorStore.asRetriever());
const answer = await chain.call({ query: question });

console.log(answer.text);
```
#### API Reference:
In this example, the SerpAPILoader is used to load web search results, which are then stored in memory using MemoryVectorStore. The RetrievalQAChain is then used to retrieve the most relevant documents from the memory and answer the question based on these documents. This demonstrates how the SerpAPILoader can streamline the process of loading and processing web search results.



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/sonix_audio_transcription

# Sonix Audio
Only available on Node.js.
This covers how to load document objects from an audio file using the Sonix API.
## Setup​
To run this loader you will need to create an account on the https://sonix.ai/ and obtain an auth key from the https://my.sonix.ai/api page.
You'll also need to install the sonix-speech-recognition library:
```typescript
npm install sonix-speech-recognition
```
```typescript
yarn add sonix-speech-recognition
```
```typescript
pnpm add sonix-speech-recognition
```
## Usage​
Once auth key is configured, you can use the loader to create transcriptions and then convert them into a Document.
In the request parameter, you can either specify a local file by setting audioFilePath or a remote file using audioUrl.
You will also need to specify the audio language. See the list of supported languages here.
```typescript
import { SonixAudioTranscriptionLoader } from "langchain/document_loaders/web/sonix_audio";

const loader = new SonixAudioTranscriptionLoader({
  sonixAuthKey: "SONIX_AUTH_KEY",
  request: {
    audioFilePath: "LOCAL_AUDIO_FILE_PATH",
    fileName: "FILE_NAME",
    language: "en",
  },
});

const docs = await loader.load();

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_loaders/integrations/web_loaders/sort_xyz_blockchain

# Blockchain Data
This example shows how to load blockchain data, including NFT metadata and transactions for a contract address, via the sort.xyz SQL API.
You will need a free Sort API key, visiting sort.xyz to obtain one.
```typescript
import { SortXYZBlockchainLoader } from "langchain/document_loaders/web/sort_xyz_blockchain";
import { OpenAI } from "langchain/llms/openai";

/**
 * See https://docs.sort.xyz/docs/api-keys to get your free Sort API key.
 * See https://docs.sort.xyz for more information on the available queries.
 * See https://docs.sort.xyz/reference for more information about Sort's REST API.
 */

/**
 * Run the example.
 */
export const run = async () => {
  // Initialize the OpenAI model. Use OPENAI_API_KEY from .env in /examples
  const model = new OpenAI({ temperature: 0.9 });

  const apiKey = "YOUR_SORTXYZ_API_KEY";
  const contractAddress =
    "0x887F3909C14DAbd9e9510128cA6cBb448E932d7f".toLowerCase();

  /*
  Load NFT metadata from the Ethereum blockchain. Hint: to load by a specific ID, see SQL query example below.
  */

  const nftMetadataLoader = new SortXYZBlockchainLoader({
    apiKey,
    query: {
      type: "NFTMetadata",
      blockchain: "ethereum",
      contractAddress,
    },
  });

  const nftMetadataDocs = await nftMetadataLoader.load();

  const nftPrompt =
    "Describe the character with the attributes from the following json document in a 4 sentence story. ";
  const nftResponse = await model.call(
    nftPrompt + JSON.stringify(nftMetadataDocs[0], null, 2)
  );
  console.log(`user > ${nftPrompt}`);
  console.log(`chatgpt > ${nftResponse}`);

  /*
    Load the latest transactions for a contract address from the Ethereum blockchain.
  */
  const latestTransactionsLoader = new SortXYZBlockchainLoader({
    apiKey,
    query: {
      type: "latestTransactions",
      blockchain: "ethereum",
      contractAddress,
    },
  });

  const latestTransactionsDocs = await latestTransactionsLoader.load();

  const latestPrompt =
    "Describe the following json documents in only 4 sentences per document. Include as much detail as possible. ";
  const latestResponse = await model.call(
    latestPrompt + JSON.stringify(latestTransactionsDocs[0], null, 2)
  );
  console.log(`\n\nuser > ${nftPrompt}`);
  console.log(`chatgpt > ${latestResponse}`);

  /*
    Load metadata for a specific NFT by using raw SQL and the NFT index. See https://docs.sort.xyz for forumulating SQL.
  */

  const sqlQueryLoader = new SortXYZBlockchainLoader({
    apiKey,
    query: `SELECT * FROM ethereum.nft_metadata WHERE contract_address = '${contractAddress}' AND token_id = 1 LIMIT 1`,
  });

  const sqlDocs = await sqlQueryLoader.load();

  const sqlPrompt =
    "Describe the character with the attributes from the following json document in an ad for a new coffee shop. ";
  const sqlResponse = await model.call(
    sqlPrompt + JSON.stringify(sqlDocs[0], null, 2)
  );
  console.log(`\n\nuser > ${sqlPrompt}`);
  console.log(`chatgpt > ${sqlResponse}`);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/

# Document transformers
Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example
is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain
has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.
## Text splitters​
When you want to deal with long pieces of text, it is necessary to split up that text into chunks.
As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What "semantically related" means could depend on the type of text.
This notebook showcases several ways to do that.
At a high level, text splitters work as following:
That means there are two different axes along which you can customize your text splitter:
## Get started with text splitters​
The recommended TextSplitter is the RecursiveCharacterTextSplitter. This will split documents recursively by different characters - starting with "\n\n", then "\n", then " ". This is nice because it will try to keep all the semantically relevant content in the same place for as long as possible.
Important parameters to know here are chunkSize and chunkOverlap. chunkSize controls the max size (in terms of number of characters) of the final documents. chunkOverlap specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to 1000 and 200 respectively.
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const output = await splitter.createDocuments([text]);
```
You'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.
```typescript
import { Document } from "langchain/document";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/integrations/openai_metadata_tagger

# OpenAI functions metadata tagger
It can often be useful to tag ingested documents with structured metadata, such as the title, tone, or length of a document, to allow for more targeted similarity search later. However, for large numbers of documents, performing this labelling process manually can be tedious.
The MetadataTagger document transformer automates this process by extracting metadata from each provided document according to a provided schema. It uses a configurable OpenAI Functions-powered chain under the hood, so if you pass a custom LLM instance, it must be an OpenAI model with functions support.
Note: This document transformer works best with complete documents, so it's best to run it first with whole documents before doing any other splitting or processing!
### Usage​
For example, let's say you wanted to index a set of movie reviews. You could initialize the document transformer as follows:
```typescript
import { z } from "zod";
import { createMetadataTaggerFromZod } from "langchain/document_transformers/openai_functions";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Document } from "langchain/document";

const zodSchema = z.object({
  movie_title: z.string(),
  critic: z.string(),
  tone: z.enum(["positive", "negative"]),
  rating: z
    .optional(z.number())
    .describe("The number of stars the critic rated the movie"),
});

const metadataTagger = createMetadataTaggerFromZod(zodSchema, {
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo" }),
});

const documents = [
  new Document({
    pageContent:
      "Review of The Bee Movie\nBy Roger Ebert\nThis is the greatest movie ever made. 4 out of 5 stars.",
  }),
  new Document({
    pageContent:
      "Review of The Godfather\nBy Anonymous\n\nThis movie was super boring. 1 out of 5 stars.",
    metadata: { reliable: false },
  }),
];
const taggedDocuments = await metadataTagger.transformDocuments(documents);

console.log(taggedDocuments);

/*
  [
    Document {
      pageContent: 'Review of The Bee Movie\n' +
        'By Roger Ebert\n' +
        'This is the greatest movie ever made. 4 out of 5 stars.',
      metadata: {
        movie_title: 'The Bee Movie',
        critic: 'Roger Ebert',
        tone: 'positive',
        rating: 4
      }
    },
    Document {
      pageContent: 'Review of The Godfather\n' +
        'By Anonymous\n' +
        '\n' +
        'This movie was super boring. 1 out of 5 stars.',
      metadata: {
        movie_title: 'The Godfather',
        critic: 'Anonymous',
        tone: 'negative',
        rating: 1,
        reliable: false
      }
    }
  ]
*/
```
#### API Reference:
There is an additional createMetadataTagger method that accepts a valid JSON Schema object as well.
### Customization​
You can pass the underlying tagging chain the standard LLMChain arguments in the second options parameter.
For example, if you wanted to ask the LLM to focus specific details in the input documents, or extract metadata in a certain style, you could pass in a custom prompt:
```typescript
import { z } from "zod";
import { createMetadataTaggerFromZod } from "langchain/document_transformers/openai_functions";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Document } from "langchain/document";
import { PromptTemplate } from "langchain/prompts";

const taggingChainTemplate = `Extract the desired information from the following passage.
Anonymous critics are actually Roger Ebert.

Passage:
{input}
`;

const zodSchema = z.object({
  movie_title: z.string(),
  critic: z.string(),
  tone: z.enum(["positive", "negative"]),
  rating: z
    .optional(z.number())
    .describe("The number of stars the critic rated the movie"),
});

const metadataTagger = createMetadataTaggerFromZod(zodSchema, {
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo" }),
  prompt: PromptTemplate.fromTemplate(taggingChainTemplate),
});

const documents = [
  new Document({
    pageContent:
      "Review of The Bee Movie\nBy Roger Ebert\nThis is the greatest movie ever made. 4 out of 5 stars.",
  }),
  new Document({
    pageContent:
      "Review of The Godfather\nBy Anonymous\n\nThis movie was super boring. 1 out of 5 stars.",
    metadata: { reliable: false },
  }),
];
const taggedDocuments = await metadataTagger.transformDocuments(documents);

console.log(taggedDocuments);

/*
  [
    Document {
      pageContent: 'Review of The Bee Movie\n' +
        'By Roger Ebert\n' +
        'This is the greatest movie ever made. 4 out of 5 stars.',
      metadata: {
        movie_title: 'The Bee Movie',
        critic: 'Roger Ebert',
        tone: 'positive',
        rating: 4
      }
    },
    Document {
      pageContent: 'Review of The Godfather\n' +
        'By Anonymous\n' +
        '\n' +
        'This movie was super boring. 1 out of 5 stars.',
      metadata: {
        movie_title: 'The Godfather',
        critic: 'Roger Ebert',
        tone: 'negative',
        rating: 1,
        reliable: false
      }
    }
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter

# Split by character
This is the simplest method. This splits based on characters (by default "\n\n") and measure chunk length by number of characters.
# CharacterTextSplitter
Besides the RecursiveCharacterTextSplitter, there is also the more standard CharacterTextSplitter. This splits only on one type of character (defaults to "\n\n"). You can use it in the exact same way.
```typescript
import { Document } from "langchain/document";
import { CharacterTextSplitter } from "langchain/text_splitter";

const text = "foo bar baz 123";
const splitter = new CharacterTextSplitter({
  separator: " ",
  chunkSize: 7,
  chunkOverlap: 3,
});
const output = await splitter.createDocuments([text]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter

# Split code and markup
CodeTextSplitter allows you to split your code and markup with support for multiple languages.
LangChain supports a variety of different markup and programming language-specific text splitters to split your text based on language-specific syntax.
This results in more semantically self-contained chunks that are more useful to a vector store or other retriever.
Popular languages like JavaScript, Python, Solidity, and Rust are supported as well as Latex, HTML, and Markdown.
## Usage​
Initialize a standard RecursiveCharacterTextSplitter with the fromLanguage factory method. Below are some examples for various languages.
## JavaScript​
```typescript
import {
  SupportedTextSplitterLanguages,
  RecursiveCharacterTextSplitter,
} from "langchain/text_splitter";

console.log(SupportedTextSplitterLanguages); // Array of supported languages

/*
  [
    'cpp',      'go',
    'java',     'js',
    'php',      'proto',
    'python',   'rst',
    'ruby',     'rust',
    'scala',    'swift',
    'markdown', 'latex',
    'html'
  ]
*/

const jsCode = `function helloWorld() {
  console.log("Hello, World!");
}
// Call the function
helloWorld();`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("js", {
  chunkSize: 32,
  chunkOverlap: 0,
});
const jsOutput = await splitter.createDocuments([jsCode]);

console.log(jsOutput);

/*
  [
    Document {
      pageContent: 'function helloWorld() {',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'console.log("Hello, World!");',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '}\n// Call the function',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'helloWorld();',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:
## Python​
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const pythonCode = `def hello_world():
  print("Hello, World!")
# Call the function
hello_world()`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("python", {
  chunkSize: 32,
  chunkOverlap: 0,
});

const pythonOutput = await splitter.createDocuments([pythonCode]);

console.log(pythonOutput);

/*
  [
    Document {
      pageContent: 'def hello_world():',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'print("Hello, World!")',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '# Call the function',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'hello_world()',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:
## HTML​
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `<!DOCTYPE html>
<html>
  <head>
    <title>🦜️🔗 LangChain</title>
    <style>
      body {
        font-family: Arial, sans-serif;
      }
      h1 {
        color: darkblue;
      }
    </style>
  </head>
  <body>
    <div>
      <h1>🦜️🔗 LangChain</h1>
      <p>⚡ Building applications with LLMs through composability ⚡</p>
    </div>
    <div>
      As an open source project in a rapidly developing field, we are extremely open to contributions.
    </div>
  </body>
</html>`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("html", {
  chunkSize: 175,
  chunkOverlap: 20,
});
const output = await splitter.createDocuments([text]);

console.log(output);

/*
  [
    Document {
      pageContent: '<!DOCTYPE html>\n<html>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<head>\n    <title>🦜️🔗 LangChain</title>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<style>\n' +
        '      body {\n' +
        '        font-family: Arial, sans-serif;\n' +
        '      }\n' +
        '      h1 {\n' +
        '        color: darkblue;\n' +
        '      }\n' +
        '    </style>\n' +
        '  </head>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<body>\n' +
        '    <div>\n' +
        '      <h1>🦜️🔗 LangChain</h1>\n' +
        '      <p>⚡ Building applications with LLMs through composability ⚡</p>\n' +
        '    </div>',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '<div>\n' +
        '      As an open source project in a rapidly developing field, we are extremely open to contributions.\n' +
        '    </div>\n' +
        '  </body>\n' +
        '</html>',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:
## Latex​
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `\\begin{document}
\\title{🦜️🔗 LangChain}
⚡ Building applications with LLMs through composability ⚡

\\section{Quick Install}

\\begin{verbatim}
Hopefully this code block isn't split
yarn add langchain
\\end{verbatim}

As an open source project in a rapidly developing field, we are extremely open to contributions.

\\end{document}`;

const splitter = RecursiveCharacterTextSplitter.fromLanguage("latex", {
  chunkSize: 100,
  chunkOverlap: 0,
});
const output = await splitter.createDocuments([text]);

console.log(output);

/*
  [
    Document {
      pageContent: '\\begin{document}\n' +
        '\\title{🦜️🔗 LangChain}\n' +
        '⚡ Building applications with LLMs through composability ⚡',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '\\section{Quick Install}',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '\\begin{verbatim}\n' +
        "Hopefully this code block isn't split\n" +
        'yarn add langchain\n' +
        '\\end{verbatim}',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: 'As an open source project in a rapidly developing field, we are extremely open to contributions.',
      metadata: { loc: [Object] }
    },
    Document {
      pageContent: '\\end{document}',
      metadata: { loc: [Object] }
    }
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/contextual_chunk_headers

# Contextual chunk headers
Consider a scenario where you want to store a large, arbitrary collection of documents in a vector store and perform Q&A tasks on them.
Simply splitting documents with overlapping text may not provide sufficient context for LLMs to determine if multiple chunks are referencing the same information, or how to resolve information from contradictory sources.
Tagging each document with metadata is a solution if you know what to filter against, but you may not know ahead of time exactly what kind of queries your vector store will be expected to handle.
Including additional contextual information directly in each chunk in the form of headers can help deal with arbitrary queries.
Here's an example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAStuffChain } from "langchain/chains";
import { CharacterTextSplitter } from "langchain/text_splitter";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { HNSWLib } from "langchain/vectorstores/hnswlib";

const splitter = new CharacterTextSplitter({
  chunkSize: 1536,
  chunkOverlap: 200,
});

const jimDocs = await splitter.createDocuments(
  [`My favorite color is blue.`],
  [],
  {
    chunkHeader: `DOCUMENT NAME: Jim Interview\n\n---\n\n`,
    appendChunkOverlapHeader: true,
  }
);

const pamDocs = await splitter.createDocuments(
  [`My favorite color is red.`],
  [],
  {
    chunkHeader: `DOCUMENT NAME: Pam Interview\n\n---\n\n`,
    appendChunkOverlapHeader: true,
  }
);

const vectorStore = await HNSWLib.fromDocuments(
  jimDocs.concat(pamDocs),
  new OpenAIEmbeddings()
);

const model = new OpenAI({ temperature: 0 });

const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAStuffChain(model),
  retriever: vectorStore.asRetriever(),
  returnSourceDocuments: true,
});
const res = await chain.call({
  query: "What is Pam's favorite color?",
});

console.log(JSON.stringify(res, null, 2));

/*
  {
    "text": " Red.",
    "sourceDocuments": [
      {
        "pageContent": "DOCUMENT NAME: Pam Interview\n\n---\n\nMy favorite color is red.",
        "metadata": {
          "loc": {
            "lines": {
              "from": 1,
              "to": 1
            }
          }
        }
      },
      {
        "pageContent": "DOCUMENT NAME: Jim Interview\n\n---\n\nMy favorite color is blue.",
        "metadata": {
          "loc": {
            "lines": {
              "from": 1,
              "to": 1
            }
          }
        }
      }
    ]
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/custom_text_splitter

# Custom text splitters
If you want to implement your own custom Text Splitter, you only need to subclass TextSplitter and implement a single method: splitText. The method takes a string and returns a list of strings. The returned strings will be used as the chunks.
```typescript
abstract class TextSplitter {
  abstract splitText(text: string): Promise<string[]>;
}
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter

# Recursively split by character
This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is ["\n\n", "\n", " ", ""]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.
Important parameters to know here are chunkSize and chunkOverlap. chunkSize controls the max size (in terms of number of characters) of the final documents. chunkOverlap specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to 1000 and 200 respectively.
```typescript
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const output = await splitter.createDocuments([text]);
```
You'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.
```typescript
import { Document } from "langchain/document";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

const text = `Hi.\n\nI'm Harrison.\n\nHow? Are? You?\nOkay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.\n\n
Bye!\n\n-H.`;
const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 10,
  chunkOverlap: 1,
});

const docOutput = await splitter.splitDocuments([
  new Document({ pageContent: text }),
]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/token

# TokenTextSplitter
Finally, TokenTextSplitter splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.
```typescript
import { Document } from "langchain/document";
import { TokenTextSplitter } from "langchain/text_splitter";

const text = "foo bar baz 123";

const splitter = new TokenTextSplitter({
  encodingName: "gpt2",
  chunkSize: 10,
  chunkOverlap: 0,
});

const output = await splitter.createDocuments([text]);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/

# Text embedding models
The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.
Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.
The base Embeddings class in LangChain exposes two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).
## Get started​
Embeddings can be used to create a numerical representation of textual data. This numerical representation is useful because it can be used to find similar documents.
Below is an example of how to use the OpenAI embeddings. Embeddings occasionally have different embedding methods for queries versus documents, so the embedding class exposes a embedQuery and embedDocuments method.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

/* Create instance */
const embeddings = new OpenAIEmbeddings();

/* Embed queries */
const res = await embeddings.embedQuery("Hello world");
/*
[
   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,
   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,
   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,
    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,
    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,
    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,
    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,
    0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,
      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,
    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,
   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,
    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,
   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,
    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,
  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,
  ... 1436 more items
]
*/

/* Embed documents */
const documentRes = await embeddings.embedDocuments(["Hello world", "Bye bye"]);
/*
[
  [
    -0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,
      0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,
      0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,
      0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,
      -0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,
    -0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,
      0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,
      -0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,
      0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,
      0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,
      0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,
      0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,
      0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,
      0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,
      0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,
      0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,
        0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,
      0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,
      0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,
    -0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,
    ... 1436 more items
  ],
  [
      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,
      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,
      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,
      -0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,
      -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,
      -0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,
        0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,
      -0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,
      0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,
      0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,
        0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,
      0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,
      0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,
      -0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,
      0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,
      0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,
      0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,
    -0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,
    0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,
      0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,
    ... 1436 more items
  ]
]
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/how_to/api_errors

# Dealing with API errors
If the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a maxRetries option when you instantiate the model. For example:
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const model = new OpenAIEmbeddings({ maxRetries: 10 });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/how_to/rate_limits

# Dealing with rate limits
Some providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a maxConcurrency option when instantiating an Embeddings model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.
For example, if you set maxConcurrency: 5, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.
To use this feature, simply pass maxConcurrency: <number> when you instantiate the LLM. For example:
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const model = new OpenAIEmbeddings({ maxConcurrency: 5 });
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a timeout option, in milliseconds, when you instantiate the model. For example, for OpenAI:
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings({
  timeout: 1000, // 1s timeout
});
/* Embed queries */
const res = await embeddings.embedQuery("Hello world");
console.log(res);
/* Embed documents */
const documentRes = await embeddings.embedDocuments(["Hello world", "Bye bye"]);
console.log({ documentRes });
```
#### API Reference:
Currently, the timeout option is only supported for OpenAI models.



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/azure_openai

# Azure OpenAI
The OpenAIEmbeddings class can also use the OpenAI API on Azure to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing stripNewLines: false to the constructor.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings({
  azureOpenAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
  azureOpenAIApiInstanceName: "YOUR-INSTANCE-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
  azureOpenAIApiDeploymentName: "YOUR-DEPLOYMENT-NAME", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
  azureOpenAIApiVersion: "YOUR-API-VERSION", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
  azureOpenAIBasePath: "YOUR-AZURE-OPENAI-BASE-PATH", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/cohere

# Cohere
The CohereEmbeddings class uses the Cohere API to generate embeddings for a given text.
```typescript
npm install cohere-ai
```
```typescript
yarn add cohere-ai
```
```typescript
pnpm add cohere-ai
```
```typescript
import { CohereEmbeddings } from "langchain/embeddings/cohere";

const embeddings = new CohereEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.COHERE_API_KEY
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/google_palm

# Google PaLM
The Google PaLM API can be integrated by first
installing the required packages:
```typescript
npm install google-auth-library @google-ai/generativelanguage
```
```typescript
yarn add google-auth-library @google-ai/generativelanguage
```
```typescript
pnpm add google-auth-library @google-ai/generativelanguage
```
Create an API key from Google MakerSuite. You can then set
the key as GOOGLE_PALM_API_KEY environment variable or pass it as apiKey parameter while instantiating
the model.
```typescript
import { GooglePaLMEmbeddings } from "langchain/embeddings/googlepalm";

export const run = async () => {
  const model = new GooglePaLMEmbeddings({
    apiKey: "<YOUR API KEY>", // or set it in environment variable as `GOOGLE_PALM_API_KEY`
    modelName: "models/embedding-gecko-001", // OPTIONAL
  });
  /* Embed queries */
  const res = await model.embedQuery(
    "What would be a good company name for a company that makes colorful socks?"
  );
  console.log({ res });
  /* Embed documents */
  const documentRes = await model.embedDocuments(["Hello world", "Bye bye"]);
  console.log({ documentRes });
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/google_vertex_ai

# Google Vertex AI
The GoogleVertexAIEmbeddings class uses Google's Vertex AI PaLM models
to generate embeddings for a given text.
The Vertex AI implementation is meant to be used in Node.js and not
directly in a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
```typescript
import { GoogleVertexAIEmbeddings } from "langchain/embeddings/googlevertexai";

export const run = async () => {
  const model = new GoogleVertexAIEmbeddings();
  const res = await model.embedQuery(
    "What would be a good company name for a company that makes colorful socks?"
  );
  console.log({ res });
};
```
#### API Reference:
Note: The default Google Vertex AI embeddings model, textembedding-gecko, has a different number of dimensions than OpenAI's text-embedding-ada-002 model
and may not be supported by all vector store providers.



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/hugging_face_inference

# HuggingFace Inference
This Embeddings integration uses the HuggingFace Inference API to generate embeddings for a given text using by default the sentence-transformers/distilbert-base-nli-mean-tokens model. You can pass a different model name to the constructor to use a different model.
```typescript
npm install @huggingface/inference@1
```
```typescript
yarn add @huggingface/inference@1
```
```typescript
pnpm add @huggingface/inference@1
```
```typescript
import { HuggingFaceInferenceEmbeddings } from "langchain/embeddings/hf";

const embeddings = new HuggingFaceInferenceEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/openai

# OpenAI
The OpenAIEmbeddings class uses the OpenAI API to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing stripNewLines: false to the constructor.
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings({
  openAIApiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
});
```



Page URL: https://js.langchain.com/docs/modules/data_connection/text_embedding/integrations/tensorflow

# TensorFlow
This Embeddings integration runs the embeddings entirely in your browser or Node.js environment, using TensorFlow.js. This means that your data isn't sent to any third party, and you don't need to sign up for any API keys. However, it does require more memory and processing power than the other integrations.
```typescript
npm install @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
```
```typescript
yarn add @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
```
```typescript
pnpm add @tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
```
```typescript
import "@tensorflow/tfjs-backend-cpu";
import { TensorFlowEmbeddings } from "langchain/embeddings/tensorflow";

const embeddings = new TensorFlowEmbeddings();
```
This example uses the CPU backend, which works in any JS environment. However, you can use any of the backends supported by TensorFlow.js, including GPU and WebAssembly, which will be a lot faster. For Node.js you can use the @tensorflow/tfjs-node package, and for the browser you can use the @tensorflow/tfjs-backend-webgl package. See the TensorFlow.js documentation for more information.



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/

# Vector stores
One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding
vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are
'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search
for you.
## Get started​
This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.
This walkthrough uses a basic, unoptimized implementation called MemoryVectorStore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings.
## Usage​
### Create a new index from texts​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MemoryVectorStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await MemoryVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const resultOne = await vectorStore.similaritySearch("hello world", 1);

console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
Here is the current base interface all vector stores share:
```typescript
interface VectorStore {
  /**
   * Add more documents to an existing VectorStore.
   * Some providers support additional parameters, e.g. to associate custom ids
   * with added documents or to change the batch size of bulk inserts.
   * Returns an array of ids for the documents or nothing.
   */
  addDocuments(
    documents: Document[],
    options?: Record<string, any>
  ): Promise<string[] | void>;

  /**
   * Search for the most similar documents to a query
   */
  similaritySearch(
    query: string,
    k?: number,
    filter?: object | undefined
  ): Promise<Document[]>;

  /**
   * Search for the most similar documents to a query,
   * and return their similarity score
   */
  similaritySearchWithScore(
    query: string,
    k = 4,
    filter: object | undefined = undefined
  ): Promise<[object, number][]>;

  /**
   * Turn a VectorStore into a Retriever
   */
  asRetriever(k?: number): BaseRetriever;

  /**
   * Delete embedded documents from the vector store matching the passed in parameter.
   * Not supported by every provider.
   */
  delete(params?: Record<string, any>): Promise<void>;

  /**
   * Advanced: Add more documents to an existing VectorStore,
   * when you already have their embeddings
   */
  addVectors(
    vectors: number[][],
    documents: Document[],
    options?: Record<string, any>
  ): Promise<string[] | void>;

  /**
   * Advanced: Search for the most similar documents to a query,
   * when you already have the embedding of the query
   */
  similaritySearchVectorWithScore(
    query: number[],
    k: number,
    filter?: object
  ): Promise<[Document, number][]>;
}
```
You can create a vector store from a list of Documents, or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.
```typescript
abstract class BaseVectorStore implements VectorStore {
  static fromTexts(
    texts: string[],
    metadatas: object[] | object,
    embeddings: Embeddings,
    dbConfig: Record<string, any>
  ): Promise<VectorStore>;

  static fromDocuments(
    docs: Document[],
    embeddings: Embeddings,
    dbConfig: Record<string, any>
  ): Promise<VectorStore>;
}
```
## Which one to pick?​
Here's a quick guide to help you pick the right vector store for your use case:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/

# Vector Stores: Integrations
## 📄️ Memory
MemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by ml-distance.
## 📄️ AnalyticDB
AnalyticDB for PostgreSQL is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.
## 📄️ Chroma
Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.
## 📄️ Elasticsearch
Only available on Node.js.
## 📄️ Faiss
Only available on Node.js.
## 📄️ HNSWLib
Only available on Node.js.
## 📄️ LanceDB
LanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.
## 📄️ Milvus
Milvus is a vector database built for embeddings similarity search and AI applications.
## 📄️ MongoDB Atlas
Only available on Node.js.
## 📄️ MyScale
Only available on Node.js.
## 📄️ OpenSearch
Only available on Node.js.
## 📄️ Pinecone
Only available on Node.js.
## 📄️ Prisma
For augmenting existing models in PostgreSQL database with vector search, Langchain supports using Prisma together with PostgreSQL and pgvector Postgres extension.
## 📄️ Qdrant
Qdrant is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.
## 📄️ Redis
Redis is a fast open source, in-memory data store.
## 📄️ SingleStore
SingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premise. It provides vector storage, as well as vector functions like dotproduct and euclideandistance, thereby supporting AI applications that require text similarity matching.
## 📄️ Supabase
Langchain supports using Supabase Postgres database as a vector store, using the pgvector postgres extension. Refer to the Supabase blog post for more information.
## 📄️ Tigris
Tigris makes it easy to build AI applications with vector embeddings.
## 📄️ TypeORM
To enable vector search in a generic PostgreSQL database, LangChainJS supports using TypeORM with the pgvector Postgres extension.
## 📄️ Typesense
Vector store that utilizes the Typesense search engine.
## 📄️ Vectara
Vectara is a developer-first API platform for easily building conversational search experiences.
## 📄️ Weaviate
Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/memory

# MemoryVectorStore
MemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by ml-distance.
## Usage​
### Create a new index from texts​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MemoryVectorStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await MemoryVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const resultOne = await vectorStore.similaritySearch("hello world", 1);

console.log(resultOne);

/*
  [
    Document {
      pageContent: "Hello world",
      metadata: { id: 2 }
    }
  ]
*/
```
#### API Reference:
### Use a custom similarity metric​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { similarity } from "ml-distance";

const vectorStore = await MemoryVectorStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings(),
  { similarity: similarity.pearson }
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/analyticdb

# AnalyticDB
AnalyticDB for PostgreSQL is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.
AnalyticDB for PostgreSQL is developed based on the open source Greenplum Database project and is enhanced with in-depth extensions by Alibaba Cloud. AnalyticDB for PostgreSQL is compatible with the ANSI SQL 2003 syntax and the PostgreSQL and Oracle database ecosystems. AnalyticDB for PostgreSQL also supports row store and column store. AnalyticDB for PostgreSQL processes petabytes of data offline at a high performance level and supports highly concurrent online queries.
This notebook shows how to use functionality related to the AnalyticDB vector database.
To run, you should have an AnalyticDB instance up and running:
Only available on Node.js.
## Setup​
LangChain.js accepts node-postgres as the connections pool for AnalyticDB vectorstore.
```typescript
npm install -S pg
```
```typescript
yarn add pg
```
```typescript
pnpm add pg
```
And we need pg-copy-streams to add batch vectors quickly.
```typescript
npm install -S pg-copy-streams
```
```typescript
yarn add pg-copy-streams
```
```typescript
pnpm add pg-copy-streams
```
## Usage​
```typescript
import { AnalyticDBVectorStore } from "langchain/vectorstores/analyticdb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const connectionOptions = {
  host: process.env.ANALYTICDB_HOST || "localhost",
  port: Number(process.env.ANALYTICDB_PORT) || 5432,
  database: process.env.ANALYTICDB_DATABASE || "your_database",
  user: process.env.ANALYTICDB_USERNAME || "username",
  password: process.env.ANALYTICDB_PASSWORD || "password",
};

const vectorStore = await AnalyticDBVectorStore.fromTexts(
  ["foo", "bar", "baz"],
  [{ page: 1 }, { page: 2 }, { page: 3 }],
  new OpenAIEmbeddings(),
  { connectionOptions }
);
const result = await vectorStore.similaritySearch("foo", 1);
console.log(JSON.stringify(result));
// [{"pageContent":"foo","metadata":{"page":1}}]

await vectorStore.addDocuments([{ pageContent: "foo", metadata: { page: 4 } }]);

const filterResult = await vectorStore.similaritySearch("foo", 1, {
  page: 4,
});
console.log(JSON.stringify(filterResult));
// [{"pageContent":"foo","metadata":{"page":4}}]

const filterWithScoreResult = await vectorStore.similaritySearchWithScore(
  "foo",
  1,
  { page: 3 }
);
console.log(JSON.stringify(filterWithScoreResult));
// [[{"pageContent":"baz","metadata":{"page":3}},0.26075905561447144]]

const filterNoMatchResult = await vectorStore.similaritySearchWithScore(
  "foo",
  1,
  { page: 5 }
);
console.log(JSON.stringify(filterNoMatchResult));
// []

// need to manually close the Connection pool
await vectorStore.end();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/chroma

# Chroma
Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.
## Setup​
```typescript
git clone git@github.com:chroma-core/chroma.git
docker-compose up -d --build
```
```typescript
npm install -S chromadb
```
```typescript
yarn add chromadb
```
```typescript
pnpm add chromadb
```
Chroma is fully-typed, fully-tested and fully-documented.
Like any other database, you can:
View full docs at docs.
## Usage, Index and query Documents​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Create vector store and index the docs
const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {
  collectionName: "a-test-collection",
});

// Search for the most similar document
const response = await vectorStore.similaritySearch("hello", 1);

console.log(response);
/*
[
  Document {
    pageContent: 'Foo\nBar\nBaz\n\n',
    metadata: { source: 'src/document_loaders/example_data/example.txt' }
  }
]
*/
```
#### API Reference:
## Usage, Index and query texts​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// text sample from Godel, Escher, Bach
const vectorStore = await Chroma.fromTexts(
  [
    `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
        Harmonic Labyrinth of the dreaded Majotaur?`,
    "Achilles: Yiikes! What is that?",
    `Tortoise: They say-although I person never believed it myself-that an I
        Majotaur has created a tiny labyrinth sits in a pit in the middle of
        it, waiting innocent victims to get lost in its fears complexity.
        Then, when they wander and dazed into the center, he laughs and
        laughs at them-so hard, that he laughs them to death!`,
    "Achilles: Oh, no!",
    "Tortoise: But it's only a myth. Courage, Achilles.",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings(),
  {
    collectionName: "godel-escher-bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);

console.log(response);
/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/

// You can also filter by metadata
const filteredResponse = await vectorStore.similaritySearch("scared", 2, {
  id: 1,
});

console.log(filteredResponse);
/*
[
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:
## Usage, Query docs from existing collection​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await Chroma.fromExistingCollection(
  new OpenAIEmbeddings(),
  { collectionName: "godel-escher-bach" }
);

const response = await vectorStore.similaritySearch("scared", 2);
console.log(response);
/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:
## Usage, delete docs​
```typescript
import { Chroma } from "langchain/vectorstores/chroma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const embeddings = new OpenAIEmbeddings();
const vectorStore = new Chroma(embeddings, {
  collectionName: "test-deletion",
});

const documents = [
  {
    pageContent: `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
    Harmonic Labyrinth of the dreaded Majotaur?`,
    metadata: {
      speaker: "Tortoise",
    },
  },
  {
    pageContent: "Achilles: Yiikes! What is that?",
    metadata: {
      speaker: "Achilles",
    },
  },
  {
    pageContent: `Tortoise: They say-although I person never believed it myself-that an I
    Majotaur has created a tiny labyrinth sits in a pit in the middle of
    it, waiting innocent victims to get lost in its fears complexity.
    Then, when they wander and dazed into the center, he laughs and
    laughs at them-so hard, that he laughs them to death!`,
    metadata: {
      speaker: "Tortoise",
    },
  },
  {
    pageContent: "Achilles: Oh, no!",
    metadata: {
      speaker: "Achilles",
    },
  },
  {
    pageContent: "Tortoise: But it's only a myth. Courage, Achilles.",
    metadata: {
      speaker: "Tortoise",
    },
  },
];

// Also supports an additional {ids: []} parameter for upsertion
const ids = await vectorStore.addDocuments(documents);

const response = await vectorStore.similaritySearch("scared", 2);
console.log(response);
/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/

// You can also pass a "filter" parameter instead
await vectorStore.delete({ ids });

const response2 = await vectorStore.similaritySearch("scared", 2);
console.log(response2);

/*
  []
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/elasticsearch

# Elasticsearch
Only available on Node.js.
Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. It supports also vector search using the k-nearest neighbor (kNN) algorithm and also custom models for Natural Language Processing (NLP).
You can read more about the support of vector search in Elasticsearch here.
LangChain.js accepts @elastic/elasticsearch as the client for Elasticsearch vectorstore.
## Setup​
```typescript
npm install -S @elastic/elasticsearch
```
```typescript
yarn add @elastic/elasticsearch
```
```typescript
pnpm add @elastic/elasticsearch
```
You'll also need to have an Elasticsearch instance running. You can use the official Docker image to get started, or you can use Elastic Cloud the official cloud service provided by Elastic.
For connecting to Elastic Cloud you can read the documentation reported here for obtaining an API key.
## Example: index docs, vector search and LLM integration​
Below is an example that indexes 4 documents in Elasticsearch,
runs a vector search query, and finally uses an LLM to answer a question in natural language
based on the retrieved documents.
```typescript
import { Client, ClientOptions } from "@elastic/elasticsearch";
import { Document } from "langchain/document";
import { OpenAI } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import {
  ElasticClientArgs,
  ElasticVectorSearch,
} from "langchain/vectorstores/elasticsearch";
import { VectorDBQAChain } from "langchain/chains";

// to run this first run Elastic's docker-container with `docker-compose up -d --build`
export async function run() {
  const config: ClientOptions = {
    node: process.env.ELASTIC_URL ?? "http://127.0.0.1:9200",
  };
  if (process.env.ELASTIC_API_KEY) {
    config.auth = {
      apiKey: process.env.ELASTIC_API_KEY,
    };
  } else if (process.env.ELASTIC_USERNAME && process.env.ELASTIC_PASSWORD) {
    config.auth = {
      username: process.env.ELASTIC_USERNAME,
      password: process.env.ELASTIC_PASSWORD,
    };
  }
  const clientArgs: ElasticClientArgs = {
    client: new Client(config),
    indexName: process.env.ELASTIC_INDEX ?? "test_vectorstore",
  };

  // Index documents

  const docs = [
    new Document({
      metadata: { foo: "bar" },
      pageContent: "Elasticsearch is a powerful vector db",
    }),
    new Document({
      metadata: { foo: "bar" },
      pageContent: "the quick brown fox jumped over the lazy dog",
    }),
    new Document({
      metadata: { baz: "qux" },
      pageContent: "lorem ipsum dolor sit amet",
    }),
    new Document({
      metadata: { baz: "qux" },
      pageContent:
        "Elasticsearch a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.",
    }),
  ];

  const embeddings = new OpenAIEmbeddings(undefined, {
    baseOptions: { temperature: 0 },
  });

  // await ElasticVectorSearch.fromDocuments(docs, embeddings, clientArgs);
  const vectorStore = new ElasticVectorSearch(embeddings, clientArgs);

  // Also supports an additional {ids: []} parameter for upsertion
  const ids = await vectorStore.addDocuments(docs);

  /* Search the vector DB independently with meta filters */
  const results = await vectorStore.similaritySearch("fox jump", 1);
  console.log(JSON.stringify(results, null, 2));
  /* [
        {
          "pageContent": "the quick brown fox jumped over the lazy dog",
          "metadata": {
            "foo": "bar"
          }
        }
    ]
  */

  /* Use as part of a chain (currently no metadata filters) for LLM query */
  const model = new OpenAI();
  const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
    k: 1,
    returnSourceDocuments: true,
  });
  const response = await chain.call({ query: "What is Elasticsearch?" });

  console.log(JSON.stringify(response, null, 2));
  /*
    {
      "text": " Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.",
      "sourceDocuments": [
        {
          "pageContent": "Elasticsearch a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.",
          "metadata": {
            "baz": "qux"
          }
        }
      ]
    }
    */

  await vectorStore.delete({ ids });

  const response2 = await chain.call({ query: "What is Elasticsearch?" });

  console.log(JSON.stringify(response2, null, 2));

  /*
    []
  */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss

# Faiss
Only available on Node.js.
Faiss is a library for efficient similarity search and clustering of dense vectors.
Langchainjs supports using Faiss as a vectorstore that can be saved to file. It also provides the ability to read the saved file from Python's implementation.
## Setup​
Install the faiss-node, which is a Node.js bindings for Faiss.
```typescript
npm install -S faiss-node
```
```typescript
yarn add faiss-node
```
```typescript
pnpm add faiss-node
```
To enable the ability to read the saved file from Python's implementation, the pickleparser also needs to install.
```typescript
npm install -S pickleparser
```
```typescript
yarn add pickleparser
```
```typescript
pnpm add pickleparser
```
## Usage​
### Create a new index from texts​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await FaissStore.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings()
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
};
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await FaissStore.fromDocuments(
  docs,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);
```
#### API Reference:
### Save an index to file and load it again​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// Create a vector store through any method, here from texts as an example
const vectorStore = await FaissStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

// Save the vector store to a directory
const directory = "your/directory/here";

await vectorStore.save(directory);

// Load the vector store from the same directory
const loadedVectorStore = await FaissStore.load(
  directory,
  new OpenAIEmbeddings()
);

// vectorStore and loadedVectorStore are identical
const result = await loadedVectorStore.similaritySearch("hello world", 1);
console.log(result);
```
#### API Reference:
### Load the saved file from Python's implementation​
```typescript
import { FaissStore } from "langchain/vectorstores/faiss";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// The directory of data saved from Python
const directory = "your/directory/here";

// Load the vector store from the directory
const loadedVectorStore = await FaissStore.loadFromPython(
  directory,
  new OpenAIEmbeddings()
);

// Search for the most similar document
const result = await loadedVectorStore.similaritySearch("test", 2);
console.log("result", result);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/hnswlib

# HNSWLib
Only available on Node.js.
HNSWLib is an in-memory vectorstore that can be saved to a file. It uses HNSWLib.
## Setup​
On Windows, you might need to install Visual Studio first in order to properly build the hnswlib-node package.
You can install it with
```typescript
npm install hnswlib-node
```
```typescript
yarn add hnswlib-node
```
```typescript
pnpm add hnswlib-node
```
## Usage​
### Create a new index from texts​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await HNSWLib.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const resultOne = await vectorStore.similaritySearch("hello world", 1);
console.log(resultOne);
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

// Load the docs into the vector store
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Search for the most similar document
const result = await vectorStore.similaritySearch("hello world", 1);
console.log(result);
```
#### API Reference:
### Save an index to a file and load it again​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// Create a vector store through any method, here from texts as an example
const vectorStore = await HNSWLib.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

// Save the vector store to a directory
const directory = "your/directory/here";
await vectorStore.save(directory);

// Load the vector store from the same directory
const loadedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());

// vectorStore and loadedVectorStore are identical

const result = await loadedVectorStore.similaritySearch("hello world", 1);
console.log(result);
```
#### API Reference:
### Filter documents​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await HNSWLib.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [{ id: 2 }, { id: 1 }, { id: 3 }],
  new OpenAIEmbeddings()
);

const result = await vectorStore.similaritySearch(
  "hello world",
  10,
  (document) => document.metadata.id === 3
);

// only "hello nice world" will be returned
console.log(result);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/lancedb

# LanceDB
LanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.
LanceDB datasets are persisted to disk and can be shared between Node.js and Python.
## Setup​
Install the LanceDB Node.js bindings:
```typescript
npm install -S vectordb
```
```typescript
yarn add vectordb
```
```typescript
pnpm add vectordb
```
## Usage​
### Create a new index from texts​
```typescript
import { LanceDB } from "langchain/vectorstores/lancedb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { connect } from "vectordb";
import * as fs from "node:fs/promises";
import * as path from "node:path";
import os from "node:os";

export const run = async () => {
  const dir = await fs.mkdtemp(path.join(os.tmpdir(), "lancedb-"));
  const db = await connect(dir);
  const table = await db.createTable("vectors", [
    { vector: Array(1536), text: "sample", id: 1 },
  ]);

  const vectorStore = await LanceDB.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings(),
    { table }
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
  // [ Document { pageContent: 'hello nice world', metadata: { id: 3 } } ]
};
```
#### API Reference:
### Create a new index from a loader​
```typescript
import { LanceDB } from "langchain/vectorstores/lancedb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import fs from "node:fs/promises";
import path from "node:path";
import os from "node:os";
import { connect } from "vectordb";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

export const run = async () => {
  const dir = await fs.mkdtemp(path.join(os.tmpdir(), "lancedb-"));
  const db = await connect(dir);
  const table = await db.createTable("vectors", [
    { vector: Array(1536), text: "sample", source: "a" },
  ]);

  const vectorStore = await LanceDB.fromDocuments(
    docs,
    new OpenAIEmbeddings(),
    { table }
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);

  // [
  //   Document {
  //     pageContent: 'Foo\nBar\nBaz\n\n',
  //     metadata: { source: 'src/document_loaders/example_data/example.txt' }
  //   }
  // ]
};
```
#### API Reference:
### Open an existing dataset​
```typescript
import { LanceDB } from "langchain/vectorstores/lancedb";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { connect } from "vectordb";
import * as fs from "node:fs/promises";
import * as path from "node:path";
import os from "node:os";

//
//  You can open a LanceDB dataset created elsewhere, such as LangChain Python, by opening
//     an existing table
//
export const run = async () => {
  const uri = await createdTestDb();
  const db = await connect(uri);
  const table = await db.openTable("vectors");

  const vectorStore = new LanceDB(new OpenAIEmbeddings(), { table });

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
  // [ Document { pageContent: 'Hello world', metadata: { id: 1 } } ]
};

async function createdTestDb(): Promise<string> {
  const dir = await fs.mkdtemp(path.join(os.tmpdir(), "lancedb-"));
  const db = await connect(dir);
  await db.createTable("vectors", [
    { vector: Array(1536), text: "Hello world", id: 1 },
    { vector: Array(1536), text: "Bye bye", id: 2 },
    { vector: Array(1536), text: "hello nice world", id: 3 },
  ]);
  return dir;
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/milvus

# Milvus
Milvus is a vector database built for embeddings similarity search and AI applications.
Only available on Node.js.
## Setup​
Run Milvus instance with Docker on your computer docs
Install the Milvus Node.js SDK.
```typescript
npm install -S @zilliz/milvus2-sdk-node
```
```typescript
yarn add @zilliz/milvus2-sdk-node
```
```typescript
pnpm add @zilliz/milvus2-sdk-node
```
Setup Env variables for Milvus before running the code
3.1 OpenAI
```typescript
export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530
```
3.2 Azure OpenAI
```typescript
export AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE
export AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE
export AZURE_OPENAI_BASE_PATH=YOUR_AZURE_OPENAI_BASE_PATH_HERE
export MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530
```
## Index and query docs​
```typescript
import { Milvus } from "langchain/vectorstores/milvus";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// text sample from Godel, Escher, Bach
const vectorStore = await Milvus.fromTexts(
  [
    "Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\
            Harmonic Labyrinth of the dreaded Majotaur?",
    "Achilles: Yiikes! What is that?",
    "Tortoise: They say-although I person never believed it myself-that an I\
            Majotaur has created a tiny labyrinth sits in a pit in the middle of\
            it, waiting innocent victims to get lost in its fears complexity.\
            Then, when they wander and dazed into the center, he laughs and\
            laughs at them-so hard, that he laughs them to death!",
    "Achilles: Oh, no!",
    "Tortoise: But it's only a myth. Courage, Achilles.",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings(),
  {
    collectionName: "goldel_escher_bach",
  }
);

// or alternatively from docs
const vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {
  collectionName: "goldel_escher_bach",
});

const response = await vectorStore.similaritySearch("scared", 2);
```
## Query docs from existing collection​
```typescript
import { Milvus } from "langchain/vectorstores/milvus";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await Milvus.fromExistingCollection(
  new OpenAIEmbeddings(),
  {
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);
```



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/mongodb_atlas

# MongoDB Atlas
Only available on Node.js.
Langchain supports MongoDB Atlas as a vector store.
## Setup​
### Installation​
First, add the Node MongoDB SDK to your project:
```typescript
npm install -S mongodb
```
```typescript
yarn add mongodb
```
```typescript
pnpm add mongodb
```
### Initial Cluster Configuration​
Next, you'll need create a MongoDB Atlas cluster. Navigate to the MongoDB Atlas website and create an account if you don't already have one.
Create and name a cluster when prompted, then find it under Database. Select Collections and create either a blank collection or one from the provided sample data.
### Creating an Index​
After configuring your cluster, you'll need to create an index on the collection field you want to search over.
Go to the Search tab within your cluster, then select Create Search Index. Using the JSON editor option, add an index to the collection you wish to use.
```typescript
{
  "mappings": {
    "fields": {
      // Default value, should match the name of the field within your collection that contains embeddings
      "embedding": [
        {
          "dimensions": 1024,
          "similarity": "euclidean",
          "type": "knnVector"
        }
      ]
    }
  }
}
```
The dimensions property should match the dimensionality of the embeddings you are using. For example, Cohere embeddings have 1024 dimensions, and OpenAI embeddings have 1536.
Note: By default the vector store expects an index name of default, an indexed collection field name of embedding, and a raw text field name of text. You should initialize the vector store with field names matching your collection schema as shown below.
Finally, proceed to build the index.
## Usage​
### Ingestion​
```typescript
import { MongoDBAtlasVectorSearch } from "langchain/vectorstores/mongodb_atlas";
import { CohereEmbeddings } from "langchain/embeddings/cohere";
import { MongoClient } from "mongodb";

export const run = async () => {
  const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
  const namespace = "langchain.test";
  const [dbName, collectionName] = namespace.split(".");
  const collection = client.db(dbName).collection(collectionName);

  await MongoDBAtlasVectorSearch.fromTexts(
    ["Hello world", "Bye bye", "What's this?"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new CohereEmbeddings(),
    {
      collection,
      indexName: "default", // The name of the Atlas search index. Defaults to "default"
      textKey: "text", // The name of the collection field containing the raw content. Defaults to "text"
      embeddingKey: "embedding", // The name of the collection field containing the embedded text. Defaults to "embedding"
    }
  );

  await client.close();
};
```
#### API Reference:
### Search​
```typescript
import { MongoDBAtlasVectorSearch } from "langchain/vectorstores/mongodb_atlas";
import { CohereEmbeddings } from "langchain/embeddings/cohere";
import { MongoClient } from "mongodb";

export const run = async () => {
  const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
  const namespace = "langchain.test";
  const [dbName, collectionName] = namespace.split(".");
  const collection = client.db(dbName).collection(collectionName);

  const vectorStore = new MongoDBAtlasVectorSearch(new CohereEmbeddings(), {
    collection,
    indexName: "default", // The name of the Atlas search index. Defaults to "default"
    textKey: "text", // The name of the collection field containing the raw content. Defaults to "text"
    embeddingKey: "embedding", // The name of the collection field containing the embedded text. Defaults to "embedding"
  });

  const resultOne = await vectorStore.similaritySearch("Hello world", 1);
  console.log(resultOne);

  await client.close();
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/myscale

# MyScale
Only available on Node.js.
MyScale is an emerging AI database that harmonizes the power of vector search and SQL analytics, providing a managed, efficient, and responsive experience.
## Setup​
```typescript
npm install -S @clickhouse/client
```
```typescript
yarn add @clickhouse/client
```
```typescript
pnpm add @clickhouse/client
```
## Index and Query Docs​
```typescript
import { MyScaleStore } from "langchain/vectorstores/myscale";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MyScaleStore.fromTexts(
  ["Hello world", "Bye bye", "hello nice world"],
  [
    { id: 2, name: "2" },
    { id: 1, name: "1" },
    { id: 3, name: "3" },
  ],
  new OpenAIEmbeddings(),
  {
    host: process.env.MYSCALE_HOST || "localhost",
    port: process.env.MYSCALE_PORT || "8443",
    username: process.env.MYSCALE_USERNAME || "username",
    password: process.env.MYSCALE_PASSWORD || "password",
  }
);

const results = await vectorStore.similaritySearch("hello world", 1);
console.log(results);

const filteredResults = await vectorStore.similaritySearch("hello world", 1, {
  whereStr: "metadata.name = '1'",
});
console.log(filteredResults);
```
#### API Reference:
## Query Docs From an Existing Collection​
```typescript
import { MyScaleStore } from "langchain/vectorstores/myscale";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await MyScaleStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  {
    host: process.env.MYSCALE_HOST || "localhost",
    port: process.env.MYSCALE_PORT || "8443",
    username: process.env.MYSCALE_USERNAME || "username",
    password: process.env.MYSCALE_PASSWORD || "password",
    database: "your_database", // defaults to "default"
    table: "your_table", // defaults to "vector_table"
  }
);

const results = await vectorStore.similaritySearch("hello world", 1);
console.log(results);

const filteredResults = await vectorStore.similaritySearch("hello world", 1, {
  whereStr: "metadata.name = '1'",
});
console.log(filteredResults);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/opensearch

# OpenSearch
Only available on Node.js.
OpenSearch is a fork of Elasticsearch that is fully compatible with the Elasticsearch API. Read more about their support for Approximate Nearest Neighbors here.
Langchain.js accepts @opensearch-project/opensearch as the client for OpenSearch vectorstore.
## Setup​
```typescript
npm install -S @opensearch-project/opensearch
```
```typescript
yarn add @opensearch-project/opensearch
```
```typescript
pnpm add @opensearch-project/opensearch
```
You'll also need to have an OpenSearch instance running. You can use the official Docker image to get started. You can also find an example docker-compose file here.
## Index docs​
```typescript
import { Client } from "@opensearch-project/opensearch";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { OpenSearchVectorStore } from "langchain/vectorstores/opensearch";

const client = new Client({
  nodes: [process.env.OPENSEARCH_URL ?? "http://127.0.0.1:9200"],
});

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "opensearch is also a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent:
      "OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications",
  }),
];

await OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {
  client,
  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`
});
```
## Query docs​
```typescript
import { Client } from "@opensearch-project/opensearch";
import { VectorDBQAChain } from "langchain/chains";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { OpenAI } from "langchain/llms/openai";
import { OpenSearchVectorStore } from "langchain/vectorstores/opensearch";

const client = new Client({
  nodes: [process.env.OPENSEARCH_URL ?? "http://127.0.0.1:9200"],
});

const vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {
  client,
});

/* Search the vector DB independently with meta filters */
const results = await vectorStore.similaritySearch("hello world", 1);
console.log(JSON.stringify(results, null, 2));
/* [
    {
      "pageContent": "Hello world",
      "metadata": {
        "id": 2
      }
    }
  ] */

/* Use as part of a chain (currently no metadata filters) */
const model = new OpenAI();
const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
  k: 1,
  returnSourceDocuments: true,
});
const response = await chain.call({ query: "What is opensearch?" });

console.log(JSON.stringify(response, null, 2));
/* 
  {
    "text": " Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.",
    "sourceDocuments": [
      {
        "pageContent": "What's this?",
        "metadata": {
          "id": 3
        }
      }
    ]
  } 
  */
```



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/pinecone

# Pinecone
Only available on Node.js.
LangChain.js accepts @pinecone-database/pinecone as the client for Pinecone vectorstore. Install the library with:
```typescript
npm install -S dotenv @pinecone-database/pinecone
```
```typescript
yarn add dotenv @pinecone-database/pinecone
```
```typescript
pnpm add dotenv @pinecone-database/pinecone
```
## Index docs​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";
import * as dotenv from "dotenv";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PineconeStore } from "langchain/vectorstores/pinecone";

dotenv.config();

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const pineconeIndex = client.Index(process.env.PINECONE_INDEX);

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "pinecone is a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "pinecones are the woody fruiting body and of a pine tree",
  }),
];

await PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {
  pineconeIndex,
});
```
## Query docs​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";
import * as dotenv from "dotenv";
import { VectorDBQAChain } from "langchain/chains";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { OpenAI } from "langchain/llms/openai";
import { PineconeStore } from "langchain/vectorstores/pinecone";

dotenv.config();

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const pineconeIndex = client.Index(process.env.PINECONE_INDEX);

const vectorStore = await PineconeStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  { pineconeIndex }
);

/* Search the vector DB independently with meta filters */
const results = await vectorStore.similaritySearch("pinecone", 1, {
  foo: "bar",
});
console.log(results);
/*
[
  Document {
    pageContent: 'pinecone is a vector db',
    metadata: { foo: 'bar' }
  }
]
*/

/* Use as part of a chain (currently no metadata filters) */
const model = new OpenAI();
const chain = VectorDBQAChain.fromLLM(model, vectorStore, {
  k: 1,
  returnSourceDocuments: true,
});
const response = await chain.call({ query: "What is pinecone?" });
console.log(response);
/*
{
  text: ' A pinecone is the woody fruiting body of a pine tree.',
  sourceDocuments: [
    Document {
      pageContent: 'pinecones are the woody fruiting body and of a pine tree',
      metadata: [Object]
    }
  ]
}
*/
```
## Delete docs​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";
import * as dotenv from "dotenv";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PineconeStore } from "langchain/vectorstores/pinecone";

dotenv.config();

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const pineconeIndex = client.Index(process.env.PINECONE_INDEX);
const embeddings = new OpenAIEmbeddings();
const pineconeStore = new PineconeStore(embeddings, { pineconeIndex });

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "pinecone is a vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "pinecones are the woody fruiting body and of a pine tree",
  }),
];

// Also takes an additional {ids: []} parameter for upsertion
const ids = await pineconeStore.addDocuments(docs);

const results = await pineconeStore.similaritySearch(pageContent, 2, {
  foo: "bar",
});

console.log(results);
/*
[
  Document {
    pageContent: 'pinecone is a vector db',
    metadata: { foo: 'bar' },
  },
  Document {
    pageContent: "the quick brown fox jumped over the lazy dog",
    metadata: { foo: "bar" },
  }
]
*/

await pineconeStore.delete({
  ids: [ids[0], ids[1]],
});

const results2 = await pineconeStore.similaritySearch(pageContent, 2, {
  foo: "bar",
});

console.log(results2);
/*
  []
*/
```



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/prisma

# Prisma
For augmenting existing models in PostgreSQL database with vector search, Langchain supports using Prisma together with PostgreSQL and pgvector Postgres extension.
## Setup​
### Setup database instance with Supabase​
Refer to the Prisma and Supabase integration guide to setup a new database instance with Supabase and Prisma.
### Install Prisma​
```typescript
npm install prisma
```
```typescript
yarn add prisma
```
```typescript
pnpm add prisma
```
### Setup pgvector self hosted instance with docker-compose​
pgvector provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.
```typescript
services:
  db:
    image: ankane/pgvector
    ports:
      - 5432:5432
    volumes:
      - db:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=
      - POSTGRES_USER=
      - POSTGRES_DB=

volumes:
  db:
```
### Create a new schema​
Assuming you haven't created a schema yet, create a new model with a vector field of type Unsupported("vector"):
```typescript
model Document {
  id      String                 @id @default(cuid())
  content String
  vector  Unsupported("vector")?
}
```
Afterwards, create a new migration with --create-only to avoid running the migration directly.
```typescript
npx prisma migrate dev --create-only
```
```typescript
npx prisma migrate dev --create-only
```
```typescript
npx prisma migrate dev --create-only
```
Add the following line to the newly created migration to enable pgvector extension if it hasn't been enabled yet:
```typescript
CREATE EXTENSION IF NOT EXISTS vector;
```
Run the migration afterwards:
```typescript
npx prisma migrate dev
```
```typescript
npx prisma migrate dev
```
```typescript
npx prisma migrate dev
```
## Usage​
Table names and column names (in fields such as tableName, vectorColumnName, columns and filter) are passed into SQL queries directly without parametrisation.
These fields must be sanitized beforehand to avoid SQL injection.
```typescript
import { PrismaVectorStore } from "langchain/vectorstores/prisma";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PrismaClient, Prisma, Document } from "@prisma/client";

export const run = async () => {
  const db = new PrismaClient();

  // Use the `withModel` method to get proper type hints for `metadata` field:
  const vectorStore = PrismaVectorStore.withModel<Document>(db).create(
    new OpenAIEmbeddings(),
    {
      prisma: Prisma,
      tableName: "Document",
      vectorColumnName: "vector",
      columns: {
        id: PrismaVectorStore.IdColumn,
        content: PrismaVectorStore.ContentColumn,
      },
    }
  );

  const texts = ["Hello world", "Bye bye", "What's this?"];
  await vectorStore.addModels(
    await db.$transaction(
      texts.map((content) => db.document.create({ data: { content } }))
    )
  );

  const resultOne = await vectorStore.similaritySearch("Hello world", 1);
  console.log(resultOne);

  // create an instance with default filter
  const vectorStore2 = PrismaVectorStore.withModel<Document>(db).create(
    new OpenAIEmbeddings(),
    {
      prisma: Prisma,
      tableName: "Document",
      vectorColumnName: "vector",
      columns: {
        id: PrismaVectorStore.IdColumn,
        content: PrismaVectorStore.ContentColumn,
      },
      filter: {
        content: {
          equals: "default",
        },
      },
    }
  );

  await vectorStore2.addModels(
    await db.$transaction(
      texts.map((content) => db.document.create({ data: { content } }))
    )
  );

  // Use the default filter a.k.a {"content": "default"}
  const resultTwo = await vectorStore.similaritySearch("Hello world", 1);
  console.log(resultTwo);

  // Override the local filter
  const resultThree = await vectorStore.similaritySearchWithScore(
    "Hello world",
    1,
    { content: { equals: "different_content" } }
  );
  console.log(resultThree);
};
```
#### API Reference:
The samples above uses the following schema:
```typescript
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Document {
  id        String                 @id @default(cuid())
  content   String
  namespace String?                @default("default")
  vector    Unsupported("vector")?
}
```
#### API Reference:
You can remove namespace if you don't need it.



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/qdrant

# Qdrant
Qdrant is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.
Only available on Node.js.
## Setup​
Run a Qdrant instance with Docker on your computer by following the Qdrant setup instructions.
Install the Qdrant Node.js SDK.
```typescript
npm install -S @qdrant/js-client-rest
```
```typescript
yarn add @qdrant/js-client-rest
```
```typescript
pnpm add @qdrant/js-client-rest
```
Setup Env variables for Qdrant before running the code
3.1 OpenAI
```typescript
export OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
export QDRANT_URL=YOUR_QDRANT_URL_HERE # for example http://localhost:6333
```
3.2 Azure OpenAI
```typescript
export AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE
export AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE
export AZURE_OPENAI_BASE_PATH=YOUR_AZURE_OPENAI_BASE_PATH_HERE
export QDRANT_URL=YOUR_QDRANT_URL_HERE # for example http://localhost:6333
```
## Usage​
### Create a new index from texts​
```typescript
import { QdrantVectorStore } from "langchain/vectorstores/qdrant";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
// text sample from Godel, Escher, Bach
const vectorStore = await QdrantVectorStore.fromTexts(
  [
    `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
Harmonic Labyrinth of the dreaded Majotaur?`,
    `Achilles: Yiikes! What is that?`,
    `Tortoise: They say-although I person never believed it myself-that an I
            Majotaur has created a tiny labyrinth sits in a pit in the middle of
            it, waiting innocent victims to get lost in its fears complexity.
            Then, when they wander and dazed into the center, he laughs and
            laughs at them-so hard, that he laughs them to death!`,
    `Achilles: Oh, no!`,
    `Tortoise: But it's only a myth. Courage, Achilles.`,
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings(),
  {
    url: process.env.QDRANT_URL,
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);

console.log(response);

/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:
### Create a new index from docs​
```typescript
import { QdrantVectorStore } from "langchain/vectorstores/qdrant";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";

// Create docs with a loader
const loader = new TextLoader("src/document_loaders/example_data/example.txt");
const docs = await loader.load();

const vectorStore = await QdrantVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings(),
  {
    url: process.env.QDRANT_URL,
    collectionName: "a_test_collection",
  }
);

// Search for the most similar document
const response = await vectorStore.similaritySearch("hello", 1);

console.log(response);
/*
[
  Document {
    pageContent: 'Foo\nBar\nBaz\n\n',
    metadata: { source: 'src/document_loaders/example_data/example.txt' }
  }
]
*/
```
#### API Reference:
### Query docs from existing collection​
```typescript
import { QdrantVectorStore } from "langchain/vectorstores/qdrant";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = await QdrantVectorStore.fromExistingCollection(
  new OpenAIEmbeddings(),
  {
    url: process.env.QDRANT_URL,
    collectionName: "goldel_escher_bach",
  }
);

const response = await vectorStore.similaritySearch("scared", 2);

console.log(response);

/*
[
  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
  Document {
    pageContent: 'Achilles: Yiikes! What is that?',
    metadata: { id: 1 }
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/redis

# Redis
Redis is a fast open source, in-memory data store.
As part of the Redis Stack, RediSearch is the module that enables vector similarity semantic search, as well as many other types of searching.
Only available on Node.js.
LangChain.js accepts node-redis as the client for Redis vectorstore.
## Setup​
```typescript
npm install -S redis
```
```typescript
yarn add redis
```
```typescript
pnpm add redis
```
## Index docs​
```typescript
import { createClient, createCluster } from "redis";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RedisVectorStore } from "langchain/vectorstores/redis";

const client = createClient({
  url: process.env.REDIS_URL ?? "redis://localhost:6379",
});
await client.connect();

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "redis is fast",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "consectetur adipiscing elit",
  }),
];

const vectorStore = await RedisVectorStore.fromDocuments(
  docs,
  new OpenAIEmbeddings(),
  {
    redisClient: client,
    indexName: "docs",
  }
);

await client.disconnect();
```
#### API Reference:
## Query docs​
```typescript
import { createClient } from "redis";
import { OpenAI } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RetrievalQAChain } from "langchain/chains";
import { RedisVectorStore } from "langchain/vectorstores/redis";

const client = createClient({
  url: process.env.REDIS_URL ?? "redis://localhost:6379",
});
await client.connect();

const vectorStore = new RedisVectorStore(new OpenAIEmbeddings(), {
  redisClient: client,
  indexName: "docs",
});

/* Simple standalone search in the vector DB */
const simpleRes = await vectorStore.similaritySearch("redis", 1);
console.log(simpleRes);
/*
[
  Document {
    pageContent: "redis is fast",
    metadata: { foo: "bar" }
  }
]
*/

/* Search in the vector DB using filters */
const filterRes = await vectorStore.similaritySearch("redis", 3, ["qux"]);
console.log(filterRes);
/*
[
  Document {
    pageContent: "consectetur adipiscing elit",
    metadata: { baz: "qux" },
  },
  Document {
    pageContent: "lorem ipsum dolor sit amet",
    metadata: { baz: "qux" },
  }
]
*/

/* Usage as part of a chain */
const model = new OpenAI();
const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(1), {
  returnSourceDocuments: true,
});
const chainRes = await chain.call({ query: "What did the fox do?" });
console.log(chainRes);
/*
{
  text: " The fox jumped over the lazy dog.",
  sourceDocuments: [
    Document {
      pageContent: "the quick brown fox jumped over the lazy dog",
      metadata: [Object]
    }
  ]
}
*/

await client.disconnect();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/singlestore

# SingleStore
SingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premise. It provides vector storage, as well as vector functions like dot_product and euclidean_distance, thereby supporting AI applications that require text similarity matching.
Only available on Node.js.
LangChain.js requires the mysql2 library to create a connection to a SingleStoreDB instance.
## Setup​
```typescript
npm install -S mysql2
```
```typescript
yarn add mysql2
```
```typescript
pnpm add mysql2
```
## Usage​
SingleStoreVectorStore manages a connection pool. It is recommended to call await store.end(); before terminating your application to assure all connections are appropriately closed and prevent any possible resource leaks.
### Standard usage​
Below is a straightforward example showcasing how to import the relevant module and perform a base similarity search using the SingleStoreVectorStore:
```typescript
import { SingleStoreVectorStore } from "langchain/vectorstores/singlestore";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await SingleStoreVectorStore.fromTexts(
    ["Hello world", "Bye bye", "hello nice world"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings(),
    {
      connectionOptions: {
        host: process.env.SINGLESTORE_HOST,
        port: Number(process.env.SINGLESTORE_PORT),
        user: process.env.SINGLESTORE_USERNAME,
        password: process.env.SINGLESTORE_PASSWORD,
        database: process.env.SINGLESTORE_DATABASE,
      },
    }
  );

  const resultOne = await vectorStore.similaritySearch("hello world", 1);
  console.log(resultOne);
  await vectorStore.end();
};
```
#### API Reference:
### Metadata Filtering​
If it is needed to filter results based on specific metadata fields, you can pass a filter parameter to narrow down your search to the documents that match all specified fields in the filter object:
```typescript
import { SingleStoreVectorStore } from "langchain/vectorstores/singlestore";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export const run = async () => {
  const vectorStore = await SingleStoreVectorStore.fromTexts(
    ["Good afternoon", "Bye bye", "Boa tarde!", "Até logo!"],
    [
      { id: 1, language: "English" },
      { id: 2, language: "English" },
      { id: 3, language: "Portugese" },
      { id: 4, language: "Portugese" },
    ],
    new OpenAIEmbeddings(),
    {
      connectionOptions: {
        host: process.env.SINGLESTORE_HOST,
        port: Number(process.env.SINGLESTORE_PORT),
        user: process.env.SINGLESTORE_USERNAME,
        password: process.env.SINGLESTORE_PASSWORD,
        database: process.env.SINGLESTORE_DATABASE,
      },
      distanceMetric: "EUCLIDEAN_DISTANCE",
    }
  );

  const resultOne = await vectorStore.similaritySearch("greetings", 1, {
    language: "Portugese",
  });
  console.log(resultOne);
  await vectorStore.end();
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/supabase

# Supabase
Langchain supports using Supabase Postgres database as a vector store, using the pgvector postgres extension. Refer to the Supabase blog post for more information.
## Setup​
### Install the library with​
```typescript
npm install -S @supabase/supabase-js
```
```typescript
yarn add @supabase/supabase-js
```
```typescript
pnpm add @supabase/supabase-js
```
### Create a table and search function in your database​
Run this in your database:
```typescript
-- Enable the pgvector extension to work with embedding vectors
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);

-- Create a function to search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;
```
## Usage​
### Standard Usage​
The below example shows how to perform a basic similarity search with Supabase:
```typescript
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const vectorStore = await SupabaseVectorStore.fromTexts(
    ["Hello world", "Bye bye", "What's this?"],
    [{ id: 2 }, { id: 1 }, { id: 3 }],
    new OpenAIEmbeddings(),
    {
      client,
      tableName: "documents",
      queryName: "match_documents",
    }
  );

  const resultOne = await vectorStore.similaritySearch("Hello world", 1);

  console.log(resultOne);
};
```
#### API Reference:
### Metadata Filtering​
Given the above match_documents Postgres function, you can also pass a filter parameter to only documents with a specific metadata field value. This filter parameter is a JSON object, and the match_documents function will use the Postgres JSONB Containment operator @> to filter documents by the metadata field values you specify. See details on the Postgres JSONB Containment operator for more information.
Note: If you've previously been using SupabaseVectorStore, you may need to drop and recreate the match_documents function per the updated SQL above to use this functionality.
```typescript
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const vectorStore = await SupabaseVectorStore.fromTexts(
    ["Hello world", "Hello world", "Hello world"],
    [{ user_id: 2 }, { user_id: 1 }, { user_id: 3 }],
    new OpenAIEmbeddings(),
    {
      client,
      tableName: "documents",
      queryName: "match_documents",
    }
  );

  const result = await vectorStore.similaritySearch("Hello world", 1, {
    user_id: 3,
  });

  console.log(result);
};
```
#### API Reference:
### Metadata Query Builder Filtering​
You can also use query builder-style filtering similar to how the Supabase JavaScript library works instead of passing an object. Note that since most of the filter properties are in the metadata column, you need to use arrow operators (-> for integer or ->> for text) as defined in Postgrest API documentation and specify the data type of the property (e.g. the column should look something like metadata->some_int_value::int).
```typescript
import {
  SupabaseFilterRPCCall,
  SupabaseVectorStore,
} from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const embeddings = new OpenAIEmbeddings();

  const store = new SupabaseVectorStore(embeddings, {
    client,
    tableName: "documents",
  });

  const docs = [
    {
      pageContent:
        "This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theorectical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle.",
      metadata: { b: 1, c: 10, stuff: "right" },
    },
    {
      pageContent:
        "This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore.",
      metadata: { b: 2, c: 9, stuff: "right" },
    },
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "right" } },
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "wrong" } },
    { pageContent: "hi", metadata: { b: 2, c: 8, stuff: "right" } },
    { pageContent: "bye", metadata: { b: 3, c: 7, stuff: "right" } },
    { pageContent: "what's this", metadata: { b: 4, c: 6, stuff: "right" } },
  ];

  // Also supports an additional {ids: []} parameter for upsertion
  await store.addDocuments(docs);

  const funcFilterA: SupabaseFilterRPCCall = (rpc) =>
    rpc
      .filter("metadata->b::int", "lt", 3)
      .filter("metadata->c::int", "gt", 7)
      .textSearch("content", `'multidimensional' & 'spaces'`, {
        config: "english",
      });

  const resultA = await store.similaritySearch("quantum", 4, funcFilterA);

  const funcFilterB: SupabaseFilterRPCCall = (rpc) =>
    rpc
      .filter("metadata->b::int", "lt", 3)
      .filter("metadata->c::int", "gt", 7)
      .filter("metadata->>stuff", "eq", "right");

  const resultB = await store.similaritySearch("hello", 2, funcFilterB);

  console.log(resultA, resultB);
};
```
#### API Reference:
### Document deletion​
```typescript
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase

const privateKey = process.env.SUPABASE_PRIVATE_KEY;
if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);

const url = process.env.SUPABASE_URL;
if (!url) throw new Error(`Expected env var SUPABASE_URL`);

export const run = async () => {
  const client = createClient(url, privateKey);

  const embeddings = new OpenAIEmbeddings();

  const store = new SupabaseVectorStore(embeddings, {
    client,
    tableName: "documents",
  });

  const docs = [
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "right" } },
    { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "wrong" } },
  ];

  // Also takes an additional {ids: []} parameter for upsertion
  const ids = await store.addDocuments(docs);

  const resultA = await store.similaritySearch("hello", 2);
  console.log(resultA);

  /*
    [
      Document { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "right" } },
      Document { pageContent: "hello", metadata: { b: 1, c: 9, stuff: "wrong" } },
    ]
  */

  await store.delete({ ids });

  const resultB = await store.similaritySearch("hello", 2);
  console.log(resultB);

  /*
    []
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/tigris

# Tigris
Tigris makes it easy to build AI applications with vector embeddings.
It is a fully managed cloud-native database that allows you store and
index documents and vector embeddings for fast and scalable vector search.
Only available on Node.js.
## Setup​
### 1. Install the Tigris SDK​
Install the SDK as follows
```typescript
npm install -S @tigrisdata/vector
```
```typescript
yarn add @tigrisdata/vector
```
```typescript
pnpm add @tigrisdata/vector
```
### 2. Fetch Tigris API credentials​
You can sign up for a free Tigris account here.
Once you have signed up for the Tigris account, create a new project called vectordemo.
Next, make a note of the clientId and clientSecret, which you can get from the
Application Keys section of the project.
## Index docs​
```typescript
import { VectorDocumentStore } from "@tigrisdata/vector";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TigrisVectorStore } from "langchain/vectorstores/tigris";

const index = new VectorDocumentStore({
  connection: {
    serverUrl: "api.preview.tigrisdata.cloud",
    projectName: process.env.TIGRIS_PROJECT,
    clientId: process.env.TIGRIS_CLIENT_ID,
    clientSecret: process.env.TIGRIS_CLIENT_SECRET,
  },
  indexName: "examples_index",
  numDimensions: 1536, // match the OpenAI embedding size
});

const docs = [
  new Document({
    metadata: { foo: "bar" },
    pageContent: "tigris is a cloud-native vector db",
  }),
  new Document({
    metadata: { foo: "bar" },
    pageContent: "the quick brown fox jumped over the lazy dog",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "lorem ipsum dolor sit amet",
  }),
  new Document({
    metadata: { baz: "qux" },
    pageContent: "tigris is a river",
  }),
];

await TigrisVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), { index });
```
#### API Reference:
## Query docs​
```typescript
import { VectorDocumentStore } from "@tigrisdata/vector";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TigrisVectorStore } from "langchain/vectorstores/tigris";

const index = new VectorDocumentStore({
  connection: {
    serverUrl: "api.preview.tigrisdata.cloud",
    projectName: process.env.TIGRIS_PROJECT,
    clientId: process.env.TIGRIS_CLIENT_ID,
    clientSecret: process.env.TIGRIS_CLIENT_SECRET,
  },
  indexName: "examples_index",
  numDimensions: 1536, // match the OpenAI embedding size
});

const vectorStore = await TigrisVectorStore.fromExistingIndex(
  new OpenAIEmbeddings(),
  { index }
);

/* Search the vector DB independently with metadata filters */
const results = await vectorStore.similaritySearch("tigris", 1, {
  "metadata.foo": "bar",
});
console.log(JSON.stringify(results, null, 2));
/*
[
  Document {
    pageContent: 'tigris is a cloud-native vector db',
    metadata: { foo: 'bar' }
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/typeorm

# TypeORM
To enable vector search in a generic PostgreSQL database, LangChainJS supports using TypeORM with the pgvector Postgres extension.
## Setup​
To work with TypeORM, you need to install the typeorm and pg packages:
```typescript
npm install typeorm
```
```typescript
yarn add typeorm
```
```typescript
pnpm add typeorm
```
```typescript
npm install pg
```
```typescript
yarn add pg
```
```typescript
pnpm add pg
```
### Setup a pgvector self hosted instance with docker-compose​
pgvector provides a prebuilt Docker image that can be used to quickly setup a self-hosted Postgres instance.
Create a file below named docker-compose.yml:
```typescript
services:
  db:
    image: ankane/pgvector
    ports:
      - 5432:5432
    volumes:
      - ./data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=ChangeMe
      - POSTGRES_USER=myuser
      - POSTGRES_DB=api
```
#### API Reference:
And then in the same directory, run docker compose up to start the container.
You can find more information on how to setup pgvector in the official repository.
## Usage​
One complete example of using TypeORMVectorStore is the following:
```typescript
import { DataSourceOptions } from "typeorm";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { TypeORMVectorStore } from "langchain/vectorstores/typeorm";

// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/typeorm

export const run = async () => {
  const args = {
    postgresConnectionOptions: {
      type: "postgres",
      host: "localhost",
      port: 5432,
      username: "myuser",
      password: "ChangeMe",
      database: "api",
    } as DataSourceOptions,
  };

  const typeormVectorStore = await TypeORMVectorStore.fromDataSource(
    new OpenAIEmbeddings(),
    args
  );

  await typeormVectorStore.ensureTableInDatabase();

  await typeormVectorStore.addDocuments([
    { pageContent: "what's this", metadata: { a: 2 } },
    { pageContent: "Cat drinks milk", metadata: { a: 1 } },
  ]);

  const results = await typeormVectorStore.similaritySearch("hello", 2);

  console.log(results);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/typesense

# Typesense
Vector store that utilizes the Typesense search engine.
### Basic Usage​
```typescript
import { Typesense, TypesenseConfig } from "langchain/vectorstores/typesense";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { Client } from "typesense";
import { Document } from "langchain/document";

const vectorTypesenseClient = new Client({
  nodes: [
    {
      // Ideally should come from your .env file
      host: "...",
      port: 123,
      protocol: "https",
    },
  ],
  // Ideally should come from your .env file
  apiKey: "...",
  numRetries: 3,
  connectionTimeoutSeconds: 60,
});

const typesenseVectorStoreConfig = {
  // Typesense client
  typesenseClient: vectorTypesenseClient,
  // Name of the collection to store the vectors in
  schemaName: "your_schema_name",
  // Optional column names to be used in Typesense
  columnNames: {
    // "vec" is the default name for the vector column in Typesense but you can change it to whatever you want
    vector: "vec",
    // "text" is the default name for the text column in Typesense but you can change it to whatever you want
    pageContent: "text",
    // Names of the columns that you will save in your typesense schema and need to be retrieved as metadata when searching
    metadataColumnNames: ["foo", "bar", "baz"],
  },
  // Optional search parameters to be passed to Typesense when searching
  searchParams: {
    q: "*",
    filter_by: "foo:[fooo]",
    query_by: "",
  },
  // You can override the default Typesense import function if you want to do something more complex
  // Default import function:
  // async importToTypesense<
  //   T extends Record<string, unknown> = Record<string, unknown>
  // >(data: T[], collectionName: string) {
  //   const chunkSize = 2000;
  //   for (let i = 0; i < data.length; i += chunkSize) {
  //     const chunk = data.slice(i, i + chunkSize);

  //     await this.caller.call(async () => {
  //       await this.client
  //         .collections<T>(collectionName)
  //         .documents()
  //         .import(chunk, { action: "emplace", dirty_values: "drop" });
  //     });
  //   }
  // }
  import: async (data, collectionName) => {
    await vectorTypesenseClient
      .collections(collectionName)
      .documents()
      .import(data, { action: "emplace", dirty_values: "drop" });
  },
} satisfies TypesenseConfig;

/**
 * Creates a Typesense vector store from a list of documents.
 * Will update documents if there is a document with the same id, at least with the default import function.
 * @param documents list of documents to create the vector store from
 * @returns Typesense vector store
 */
const createVectorStoreWithTypesense = async (documents: Document[] = []) =>
  Typesense.fromDocuments(
    documents,
    new OpenAIEmbeddings(),
    typesenseVectorStoreConfig
  );

/**
 * Returns a Typesense vector store from an existing index.
 * @returns Typesense vector store
 */
const getVectorStoreWithTypesense = async () =>
  new Typesense(new OpenAIEmbeddings(), typesenseVectorStoreConfig);

// Do a similarity search
const vectorStore = await getVectorStoreWithTypesense();
const documents = await vectorStore.similaritySearch("hello world");

// Add filters based on metadata with the search parameters of Typesense
// will exclude documents with author:JK Rowling, so if Joe Rowling & JK Rowling exists, only Joe Rowling will be returned
vectorStore.similaritySearch("Rowling", undefined, {
  filter_by: "author:!=JK Rowling",
});

// Delete a document
vectorStore.deleteDocuments(["document_id_1", "document_id_2"]);
```
### Constructor​
Before starting, create a schema in Typesense with an id, a field for the vector and a field for the text. Add as many other fields as needed for the metadata.
### Methods​



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/vectara

# Vectara
Vectara is a developer-first API platform for easily building conversational search experiences.
You can use Vectara as a vector store with LangChain.js.
## 👉 Embeddings Included​
Vectara uses its own embeddings under the hood, so you don't have to provide any yourself or call another service to obtain embeddings.
This also means that if you provide your own embeddings, they'll be a no-op.
```typescript
const store = await VectaraStore.fromTexts(
  ["hello world", "hi there"],
  [{ foo: "bar" }, { foo: "baz" }],
  // This won't have an effect. Provide a FakeEmbeddings instance instead for clarity.
  new OpenAIEmbeddings(),
  args
);
```
## Setup​
You'll need to:
Configure your .env file or provide args to connect LangChain to your Vectara corpus:
```typescript
VECTARA_CUSTOMER_ID=your_customer_id
VECTARA_CORPUS_ID=your_corpus_id
VECTARA_API_KEY=your-vectara-api-key
```
## Usage​
```typescript
import { VectaraStore } from "langchain/vectorstores/vectara";
import { Document } from "langchain/document";

// Create the store.
const store = new VectaraStore({
  customerId: Number(process.env.VECTARA_CUSTOMER_ID),
  corpusId: Number(process.env.VECTARA_CORPUS_ID),
  apiKey: String(process.env.VECTARA_API_KEY),
  verbose: true,
});

// Store your data.
await store.addDocuments([
  new Document({
    pageContent: "Do I dare to eat a peach?",
    metadata: {
      foo: "baz",
    },
  }),
  new Document({
    pageContent: "In the room the women come and go talking of Michelangelo",
    metadata: {
      foo: "bar",
    },
  }),
]);

// "Added 2 documents to Vectara"

const resultsWithScore = await store.similaritySearchWithScore(
  "What were the women talking about?",
  1,
  {
    lambda: 0.025,
  }
);

console.log(JSON.stringify(resultsWithScore, null, 2));
// [
//   [
//     {
//       "pageContent": "In the room the women come and go talking of Michelangelo",
//       "metadata": [
//         {
//           "name": "lang",
//           "value": "eng"
//         },
//         {
//           "name": "offset",
//           "value": "0"
//         },
//         {
//           "name": "len",
//           "value": "57"
//         }
//       ]
//     },
//     0.38169062
//   ]
// ]
```
#### API Reference:
Note that lambda is a parameter related to Vectara's hybrid search capbility, providing a tradeoff between neural search and boolean/exact match as described here. We recommend the value of 0.025 as a default, while providing a way for advanced users to customize this value if needed.
## APIs​
Vectara's LangChain vector store consumes Vectara's core APIs:



Page URL: https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate

# Weaviate
Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.
LangChain inserts vectors directly to Weaviate, and queries Weaviate for the nearest neighbors of a given vector, so that you can use all the LangChain Embeddings integrations with Weaviate.
## Setup​
```typescript
npm install weaviate-ts-client graphql
```
```typescript
yarn add weaviate-ts-client graphql
```
```typescript
pnpm add weaviate-ts-client graphql
```
You'll need to run Weaviate either locally or on a server, see the Weaviate documentation for more information.
## Usage, insert documents​
```typescript
/* eslint-disable @typescript-eslint/no-explicit-any */
import weaviate from "weaviate-ts-client";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // Something wrong with the weaviate-ts-client types, so we need to disable
  const client = (weaviate as any).client({
    scheme: process.env.WEAVIATE_SCHEME || "https",
    host: process.env.WEAVIATE_HOST || "localhost",
    apiKey: new (weaviate as any).ApiKey(
      process.env.WEAVIATE_API_KEY || "default"
    ),
  });

  // Create a store and fill it with some texts + metadata
  await WeaviateStore.fromTexts(
    ["hello world", "hi there", "how are you", "bye now"],
    [{ foo: "bar" }, { foo: "baz" }, { foo: "qux" }, { foo: "bar" }],
    new OpenAIEmbeddings(),
    {
      client,
      indexName: "Test",
      textKey: "text",
      metadataKeys: ["foo"],
    }
  );
}
```
#### API Reference:
## Usage, query documents​
```typescript
/* eslint-disable @typescript-eslint/no-explicit-any */
import weaviate from "weaviate-ts-client";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // Something wrong with the weaviate-ts-client types, so we need to disable
  const client = (weaviate as any).client({
    scheme: process.env.WEAVIATE_SCHEME || "https",
    host: process.env.WEAVIATE_HOST || "localhost",
    apiKey: new (weaviate as any).ApiKey(
      process.env.WEAVIATE_API_KEY || "default"
    ),
  });

  // Create a store for an existing index
  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {
    client,
    indexName: "Test",
    metadataKeys: ["foo"],
  });

  // Search the index without any filters
  const results = await store.similaritySearch("hello world", 1);
  console.log(results);
  /*
  [ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]
  */

  // Search the index with a filter, in this case, only return results where
  // the "foo" metadata key is equal to "baz", see the Weaviate docs for more
  // https://weaviate.io/developers/weaviate/api/graphql/filters
  const results2 = await store.similaritySearch("hello world", 1, {
    where: {
      operator: "Equal",
      path: ["foo"],
      valueText: "baz",
    },
  });
  console.log(results2);
  /*
  [ Document { pageContent: 'hi there', metadata: { foo: 'baz' } } ]
  */
}
```
#### API Reference:
## Usage delete documents​
```typescript
/* eslint-disable @typescript-eslint/no-explicit-any */
import weaviate from "weaviate-ts-client";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

export async function run() {
  // Something wrong with the weaviate-ts-client types, so we need to disable
  const client = (weaviate as any).client({
    scheme: process.env.WEAVIATE_SCHEME || "https",
    host: process.env.WEAVIATE_HOST || "localhost",
    apiKey: new (weaviate as any).ApiKey(
      process.env.WEAVIATE_API_KEY || "default"
    ),
  });

  // Create a store for an existing index
  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {
    client,
    indexName: "Test",
    metadataKeys: ["foo"],
  });

  const docs = [{ pageContent: "see ya!", metadata: { foo: "bar" } }];

  // Also supports an additional {ids: []} parameter for upsertion
  const ids = await store.addDocuments(docs);

  // Search the index without any filters
  const results = await store.similaritySearch("see ya!", 1);
  console.log(results);
  /*
  [ Document { pageContent: 'see ya!', metadata: { foo: 'bar' } } ]
  */

  await store.delete({ ids });

  const results2 = await store.similaritySearch("see ya!", 1);
  console.log(results2);
  /*
  []
  */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/

# Retrievers
A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) it. Vector stores can be used
as the backbone of a retriever, but there are other types of retrievers as well.
## Get started​
The public API of the BaseRetriever class in LangChain.js is as follows:
```typescript
export abstract class BaseRetriever {
  abstract getRelevantDocuments(query: string): Promise<Document[]>;
}
```
It's that simple! You can call getRelevantDocuments to retrieve documents relevant to a query, where "relevance" is defined by
the specific retriever object you are calling.
Of course, we also help construct what we think useful Retrievers are. The main type of Retriever in LangChain is a vector store retriever. We will focus on that here.
Note: Before reading, it's important to understand what a vector store is.
This example showcases question answering over documents.
We have chosen this as the example for getting started because it nicely combines a lot of different elements (Text splitters, embeddings, vectorstores) and then also shows how to use them in a chain.
Question answering over documents consists of four steps:
Each of the steps has multiple sub steps and potential configurations, but we'll go through one common flow using HNSWLib, a local vector store.
This assumes you're using Node, but you can swap in another integration if necessary.
First, install the required dependency:
```typescript
npm install -S hnswlib-node
```
```typescript
yarn add hnswlib-node
```
```typescript
pnpm add hnswlib-node
```
You can download the state_of_the_union.txt file here.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const chain = RetrievalQAChain.fromLLM(model, vectorStoreRetriever);
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
    and retiring Justice of the United States Supreme Court and thanked him for his service.'
  }
}
*/
```
#### API Reference:
Let's walk through what's happening here.
We first load a long text and split it into smaller documents using a text splitter.
We then load those documents (which also embeds the documents using the passed OpenAIEmbeddings instance) into HNSWLib, our vector store, creating our index.
Though we can query the vector store directly, we convert the vector store into a retriever to return retrieved documents in the right format for the question answering chain.
We initialize a RetrievalQAChain with the .fromLLM method, which we'll call later in step 4.
We ask questions!
See the individual sections for deeper dives on specific retrievers.



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/contextual_compression/

# Contextual compression
One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.
Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.
To use the Contextual Compression Retriever, you'll need:
The Contextual Compression Retriever passes queries to the base Retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of Documents and shortens it by reducing the contents of Documents or dropping Documents altogether.

## Get started​
Here's an example of how this works:
```typescript
import * as fs from "fs";

import { OpenAI } from "langchain/llms/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { ContextualCompressionRetriever } from "langchain/retrievers/contextual_compression";
import { LLMChainExtractor } from "langchain/retrievers/document_compressors/chain_extract";

const model = new OpenAI();
const baseCompressor = LLMChainExtractor.fromLLM(model);

const text = fs.readFileSync("state_of_the_union.txt", "utf8");

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const retriever = new ContextualCompressionRetriever({
  baseCompressor,
  baseRetriever: vectorStore.asRetriever(),
});

const chain = RetrievalQAChain.fromLLM(model, retriever);

const res = await chain.call({
  query: "What did the speaker say about Justice Breyer?",
});

console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/

# Self-querying
A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.

All Self Query retrievers require peggy as a peer dependency:
```typescript
npm install -S peggy
```
```typescript
yarn add peggy
```
```typescript
pnpm add peggy
```
Here's a basic example with an in-memory, unoptimized vector store:
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "langchain/retrievers/self_query/functional";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/chroma-self-query

# Chroma Self Query Retriever
This example shows how to use a self query retriever with a Chroma vector store.
## Usage​
```typescript
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { ChromaTranslator } from "langchain/retrievers/self_query/chroma";
import { OpenAI } from "langchain/llms/openai";
import { Chroma } from "langchain/vectorstores/chroma";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await Chroma.fromDocuments(docs, embeddings, {
  collectionName: "a-movie-collection",
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new ChromaTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/hnswlib-self-query

# HNSWLib Self Query Retriever
This example shows how to use a self query retriever with an HNSWLib vector store.
## Usage​
```typescript
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "langchain/retrievers/self_query/functional";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await HNSWLib.fromDocuments(docs, embeddings);
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/memory-self-query

# Memory Vector Store Self Query Retriever
This example shows how to use a self query retriever with a basic, in-memory vector store.
## Usage​
```typescript
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { FunctionalTranslator } from "langchain/retrievers/self_query/functional";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new FunctionalTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/pinecone-self-query

# Pinecone Self Query Retriever
This example shows how to use a self query retriever with a Pinecone vector store.
## Usage​
```typescript
import { PineconeClient } from "@pinecone-database/pinecone";

import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { PineconeTranslator } from "langchain/retrievers/self_query/pinecone";
import { PineconeStore } from "langchain/vectorstores/pinecone";
import { OpenAI } from "langchain/llms/openai";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 * We also need to provide an embeddings object. This is used to embed the documents.
 */
if (
  !process.env.PINECONE_API_KEY ||
  !process.env.PINECONE_ENVIRONMENT ||
  !process.env.PINECONE_INDEX
) {
  throw new Error(
    "PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set"
  );
}

const client = new PineconeClient();
await client.init({
  apiKey: process.env.PINECONE_API_KEY,
  environment: process.env.PINECONE_ENVIRONMENT,
});
const index = client.Index(process.env.PINECONE_INDEX);

const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const vectorStore = await PineconeStore.fromDocuments(docs, embeddings, {
  pineconeIndex: index,
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to create a basic translator that translates the queries into a
   * filter format that the vector store can understand. We provide a basic translator
   * translator here, but you can create your own translator by extending BaseTranslator
   * abstract class. Note that the vector store needs to support filtering on the metadata
   * attributes you want to query on.
   */
  structuredQueryTranslator: new PineconeTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/supabase-self-query

# Supabase Self Query Retriever
This example shows how to use a self query retriever with a Supabase vector store.
If you haven't already set up Supabase, please follow the instructions here.
## Usage​
```typescript
import { createClient } from "@supabase/supabase-js";

import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { SupabaseTranslator } from "langchain/retrievers/self_query/supabase";
import { OpenAI } from "langchain/llms/openai";
import { SupabaseVectorStore } from "langchain/vectorstores/supabase";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 */
if (!process.env.SUPABASE_URL || !process.env.SUPABASE_PRIVATE_KEY) {
  throw new Error(
    "Supabase URL or private key not set. Please set it in the .env file"
  );
}

const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
const client = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_PRIVATE_KEY
);
const vectorStore = await SupabaseVectorStore.fromDocuments(docs, embeddings, {
  client,
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. LangChain provides one here.
   */
  structuredQueryTranslator: new SupabaseTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are less than 90 minutes?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query3 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
const query4 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are either comedy or drama and are less than 90 minutes?"
);
console.log(query1, query2, query3, query4);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/weaviate-self-query

# Weaviate Self Query Retriever
This example shows how to use a self query retriever with a Weaviate vector store.
If you haven't already set up Weaviate, please follow the instructions here.
## Usage​
This example shows how to intialize a SelfQueryRetriever with a vector store:
```typescript
import weaviate from "weaviate-ts-client";

import { AttributeInfo } from "langchain/schema/query_constructor";
import { Document } from "langchain/document";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { SelfQueryRetriever } from "langchain/retrievers/self_query";
import { OpenAI } from "langchain/llms/openai";
import { WeaviateStore } from "langchain/vectorstores/weaviate";
import { WeaviateTranslator } from "langchain/retrievers/self_query/weaviate";

/**
 * First, we create a bunch of documents. You can load your own documents here instead.
 * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
 */
const docs = [
  new Document({
    pageContent:
      "A bunch of scientists bring back dinosaurs and mayhem breaks loose",
    metadata: { year: 1993, rating: 7.7, genre: "science fiction" },
  }),
  new Document({
    pageContent:
      "Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
    metadata: { year: 2010, director: "Christopher Nolan", rating: 8.2 },
  }),
  new Document({
    pageContent:
      "A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
    metadata: { year: 2006, director: "Satoshi Kon", rating: 8.6 },
  }),
  new Document({
    pageContent:
      "A bunch of normal-sized women are supremely wholesome and some men pine after them",
    metadata: { year: 2019, director: "Greta Gerwig", rating: 8.3 },
  }),
  new Document({
    pageContent: "Toys come alive and have a blast doing so",
    metadata: { year: 1995, genre: "animated" },
  }),
  new Document({
    pageContent: "Three men walk into the Zone, three men walk out of the Zone",
    metadata: {
      year: 1979,
      director: "Andrei Tarkovsky",
      genre: "science fiction",
      rating: 9.9,
    },
  }),
];

/**
 * Next, we define the attributes we want to be able to query on.
 * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
 * We also provide a description of each attribute and the type of the attribute.
 * This is used to generate the query prompts.
 */
const attributeInfo: AttributeInfo[] = [
  {
    name: "genre",
    description: "The genre of the movie",
    type: "string or array of strings",
  },
  {
    name: "year",
    description: "The year the movie was released",
    type: "number",
  },
  {
    name: "director",
    description: "The director of the movie",
    type: "string",
  },
  {
    name: "rating",
    description: "The rating of the movie (1-10)",
    type: "number",
  },
  {
    name: "length",
    description: "The length of the movie in minutes",
    type: "number",
  },
];

/**
 * Next, we instantiate a vector store. This is where we store the embeddings of the documents.
 */
const embeddings = new OpenAIEmbeddings();
const llm = new OpenAI();
const documentContents = "Brief summary of a movie";
// eslint-disable-next-line @typescript-eslint/no-explicit-any
const client = (weaviate as any).client({
  scheme: process.env.WEAVIATE_SCHEME || "https",
  host: process.env.WEAVIATE_HOST || "localhost",
  apiKey: process.env.WEAVIATE_API_KEY
    ? // eslint-disable-next-line @typescript-eslint/no-explicit-any
      new (weaviate as any).ApiKey(process.env.WEAVIATE_API_KEY)
    : undefined,
});

const vectorStore = await WeaviateStore.fromDocuments(docs, embeddings, {
  client,
  indexName: "Test",
  textKey: "text",
  metadataKeys: ["year", "director", "rating", "genre"],
});
const selfQueryRetriever = await SelfQueryRetriever.fromLLM({
  llm,
  vectorStore,
  documentContents,
  attributeInfo,
  /**
   * We need to use a translator that translates the queries into a
   * filter format that the vector store can understand. LangChain provides one here.
   */
  structuredQueryTranslator: new WeaviateTranslator(),
});

/**
 * Now we can query the vector store.
 * We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
 * We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
 * The retriever will automatically convert these questions into queries that can be used to retrieve documents.
 *
 * Note that unlike other vector stores, you have to make sure each metadata keys are actually presnt in the database,
 * meaning that Weaviate will throw an error if the self query chain generate a query with a metadata key that does
 * not exist in your Weaviate database.
 */
const query1 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are rated higher than 8.5?"
);
const query2 = await selfQueryRetriever.getRelevantDocuments(
  "Which movies are directed by Greta Gerwig?"
);
console.log(query1, query2);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/time_weighted_vectorstore

# Time-weighted vector store retriever
This retriever uses a combination of semantic similarity and a time decay.
The algorithm for scoring them is:
```typescript
semantic_similarity + (1.0 - decay_rate) ^ hours_passed
```
Notably, hours_passed refers to the hours passed since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain "fresh."
```typescript
let score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;
```
this.decayRate is a configurable decimal number between 0 and 1. A lower number means that documents will be "remembered" for longer, while a higher number strongly weights more recently accessed documents.
Note that setting a decay rate of exactly 0 or 1 makes hoursPassed irrelevant and makes this retriever equivalent to a standard vector lookup.
## Usage​
This example shows how to intialize a TimeWeightedVectorStoreRetriever with a vector store.
It is important to note that due to required metadata, all documents must be added to the backing vector store using the addDocuments method on the retriever, not the vector store itself.
```typescript
import { TimeWeightedVectorStoreRetriever } from "langchain/retrievers/time_weighted";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());

const retriever = new TimeWeightedVectorStoreRetriever({
  vectorStore,
  memoryStream: [],
  searchKwargs: 2,
});

const documents = [
  "My name is John.",
  "My name is Bob.",
  "My favourite food is pizza.",
  "My favourite food is pasta.",
  "My favourite food is sushi.",
].map((pageContent) => ({ pageContent, metadata: {} }));

// All documents must be added using this method on the retriever (not the vector store!)
// so that the correct access history metadata is populated
await retriever.addDocuments(documents);

const results1 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results1);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */

const results2 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results2);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/vectorstore

# Vector store-backed retriever
A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the Vector Store class to make it conform to the Retriever interface.
It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.
Once you construct a Vector store, it's very easy to construct a retriever. Let's walk through an example.
```typescript
const vectorStore = ...
const retriever = vectorStore.asRetriever();
```
Here's a more end-to-end example:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

const docs = retriever.getRelevantDocuments("what did he say about ketanji brown jackson");
```
## Configuration​
You can specify a maximum number of documents to retrieve as well as a vector store-specific filter to use when retrieving.
```typescript
// Return up to 2 documents with `metadataField` set to `"value"`
const retriever = vectorStore.asRetriever(2, { metadataField: "value" });

const docs = retriever.getRelevantDocuments("what did he say about ketanji brown jackson");
```



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/chatgpt-retriever-plugin

# ChatGPT Plugin Retriever
This example shows how to use the ChatGPT Retriever Plugin within LangChain.
To set up the ChatGPT Retriever Plugin, please follow instructions here.
## Usage​
```typescript
import { ChatGPTPluginRetriever } from "langchain/retrievers/remote";

export const run = async () => {
  const retriever = new ChatGPTPluginRetriever({
    url: "http://0.0.0.0:8000",
    auth: {
      bearer: "super-secret-jwt-token-with-at-least-32-characters-long",
    },
  });

  const docs = await retriever.getRelevantDocuments("hello world");

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/contextual-compression-retriever

# Contextual Compression Retriever
A Contextual Compression Retriever is designed to improve the answers returned from vector store document similarity searches by better taking into account the context from the query.
It wraps another retriever, and uses a Document Compressor as an intermediate step after the initial similarity search that removes information irrelevant to the initial query from the retrieved documents.
This reduces the amount of distraction a subsequent chain has to deal with when parsing the retrieved documents and making its final judgements.
## Usage​
This example shows how to intialize a ContextualCompressionRetriever with a vector store and a document compressor:
```typescript
import * as fs from "fs";

import { OpenAI } from "langchain/llms/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { ContextualCompressionRetriever } from "langchain/retrievers/contextual_compression";
import { LLMChainExtractor } from "langchain/retrievers/document_compressors/chain_extract";

const model = new OpenAI();
const baseCompressor = LLMChainExtractor.fromLLM(model);

const text = fs.readFileSync("state_of_the_union.txt", "utf8");

const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

const retriever = new ContextualCompressionRetriever({
  baseCompressor,
  baseRetriever: vectorStore.asRetriever(),
});

const chain = RetrievalQAChain.fromLLM(model, retriever);

const res = await chain.call({
  query: "What did the speaker say about Justice Breyer?",
});

console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/databerry-retriever

# Databerry Retriever
This example shows how to use the Databerry Retriever in a RetrievalQAChain to retrieve documents from a Databerry.ai datastore.
## Usage​
```typescript
import { DataberryRetriever } from "langchain/retrievers/databerry";

export const run = async () => {
  const retriever = new DataberryRetriever({
    datastoreUrl: "https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc",
    apiKey: "DATABERRY_API_KEY", // optional: needed for private datastores
    topK: 8, // optional: default value is 3
  });

  const docs = await retriever.getRelevantDocuments("hello");

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/hyde

# HyDE Retriever
This example shows how to use the HyDE Retriever, which implements Hypothetical Document Embeddings (HyDE) as described in this paper.
At a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.
In order to use HyDE, we therefore need to provide a base embedding model, as well as an LLM that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own, which should have a single input variable {question}.
## Usage​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { HydeRetriever } from "langchain/retrievers/hyde";
import { Document } from "langchain/document";

const embeddings = new OpenAIEmbeddings();
const vectorStore = new MemoryVectorStore(embeddings);
const llm = new OpenAI();
const retriever = new HydeRetriever({
  vectorStore,
  llm,
  k: 1,
});

await vectorStore.addDocuments(
  [
    "My name is John.",
    "My name is Bob.",
    "My favourite food is pizza.",
    "My favourite food is pasta.",
  ].map((pageContent) => new Document({ pageContent }))
);

const results = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results);
/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/kendra-retriever

# Amazon Kendra Retriever
Amazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.
With Kendra, users can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results.
## Setup​
```typescript
npm i @aws-sdk/client-kendra
```
```typescript
yarn add @aws-sdk/client-kendra
```
```typescript
pnpm add @aws-sdk/client-kendra
```
## Usage​
```typescript
import { AmazonKendraRetriever } from "langchain/retrievers/amazon_kendra";

const retriever = new AmazonKendraRetriever({
  topK: 10,
  indexId: "YOUR_INDEX_ID",
  region: "us-east-2", // Your region
  clientOptions: {
    credentials: {
      accessKeyId: "YOUR_ACCESS_KEY_ID",
      secretAccessKey: "YOUR_SECRET_ACCESS_KEY",
    },
  },
});

const docs = await retriever.getRelevantDocuments("How are clouds formed?");

console.log(docs);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/metal-retriever

# Metal Retriever
This example shows how to use the Metal Retriever in a RetrievalQAChain to retrieve documents from a Metal index.
## Setup​
```typescript
npm i @getmetal/metal-sdk
```
```typescript
yarn add @getmetal/metal-sdk
```
```typescript
pnpm add @getmetal/metal-sdk
```
## Usage​
```typescript
/* eslint-disable @typescript-eslint/no-non-null-assertion */
import Metal from "@getmetal/metal-sdk";
import { MetalRetriever } from "langchain/retrievers/metal";

export const run = async () => {
  const MetalSDK = Metal;

  const client = new MetalSDK(
    process.env.METAL_API_KEY!,
    process.env.METAL_CLIENT_ID!,
    process.env.METAL_INDEX_ID
  );
  const retriever = new MetalRetriever({ client });

  const docs = await retriever.getRelevantDocuments("hello");

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/remote-retriever

# Remote Retriever
This example shows how to use a Remote Retriever in a RetrievalQAChain to retrieve documents from a remote server.
## Usage​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { RemoteLangChainRetriever } from "langchain/retrievers/remote";

export const run = async () => {
  // Initialize the LLM to use to answer the question.
  const model = new OpenAI({});

  // Initialize the remote retriever.
  const retriever = new RemoteLangChainRetriever({
    url: "http://0.0.0.0:8080/retrieve", // Replace with your own URL.
    auth: { bearer: "foo" }, // Replace with your own auth.
    inputKey: "message",
    responseKey: "response",
  });

  // Create a chain that uses the OpenAI LLM and remote retriever.
  const chain = RetrievalQAChain.fromLLM(model, retriever);

  // Call the chain with a query.
  const res = await chain.call({
    query: "What did the president say about Justice Breyer?",
  });
  console.log({ res });
  /*
  {
    res: {
      text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
      and retiring Justice of the United States Supreme Court and thanked him for his service.'
    }
  }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/supabase-hybrid

# Supabase Hybrid Search
Langchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres pgvector extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore addDocuments function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The getRelevantDocuments function produces a list of documents that has duplicates removed and is sorted by relevance score.
## Setup​
### Install the library with​
```typescript
npm install -S @supabase/supabase-js
```
```typescript
yarn add @supabase/supabase-js
```
```typescript
pnpm add @supabase/supabase-js
```
### Create a table and search functions in your database​
Run this in your database:
```typescript
-- Enable the pgvector extension to work with embedding vectors
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text, -- corresponds to Document.pageContent
  metadata jsonb, -- corresponds to Document.metadata
  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed
);

-- Create a function to similarity search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Create a function to keyword search for documents
create function kw_match_documents(query_text text, match_count int)
returns table (id bigint, content text, metadata jsonb, similarity real)
as $$

begin
return query execute
format('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity
from documents
where to_tsvector(content) @@ plainto_tsquery($1)
order by similarity desc
limit $2')
using query_text, match_count;
end;
$$ language plpgsql;
```
## Usage​
```typescript
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { createClient } from "@supabase/supabase-js";
import { SupabaseHybridSearch } from "langchain/retrievers/supabase";

export const run = async () => {
  const client = createClient(
    process.env.SUPABASE_URL || "",
    process.env.SUPABASE_PRIVATE_KEY || ""
  );

  const embeddings = new OpenAIEmbeddings();

  const retriever = new SupabaseHybridSearch(embeddings, {
    client,
    //  Below are the defaults, expecting that you set up your supabase table and functions according to the guide above. Please change if necessary.
    similarityK: 2,
    keywordK: 2,
    tableName: "documents",
    similarityQueryName: "match_documents",
    keywordQueryName: "kw_match_documents",
  });

  const results = await retriever.getRelevantDocuments("hello bye");

  console.log(results);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/time-weighted-retriever

# Time-Weighted Retriever
A Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:
```typescript
let score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;
```
Notably, hoursPassed above refers to the time since the object in the retriever was last accessed, not since it was created. This means that frequently accessed objects remain "fresh" and score higher.
this.decayRate is a configurable decimal number between 0 and 1. A lower number means that documents will be "remembered" for longer, while a higher number strongly weights more recently accessed documents.
Note that setting a decay rate of exactly 0 or 1 makes hoursPassed irrelevant and makes this retriever equivalent to a standard vector lookup.
## Usage​
This example shows how to intialize a TimeWeightedVectorStoreRetriever with a vector store.
It is important to note that due to required metadata, all documents must be added to the backing vector store using the addDocuments method on the retriever, not the vector store itself.
```typescript
import { TimeWeightedVectorStoreRetriever } from "langchain/retrievers/time_weighted";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());

const retriever = new TimeWeightedVectorStoreRetriever({
  vectorStore,
  memoryStream: [],
  searchKwargs: 2,
});

const documents = [
  "My name is John.",
  "My name is Bob.",
  "My favourite food is pizza.",
  "My favourite food is pasta.",
  "My favourite food is sushi.",
].map((pageContent) => ({ pageContent, metadata: {} }));

// All documents must be added using this method on the retriever (not the vector store!)
// so that the correct access history metadata is populated
await retriever.addDocuments(documents);

const results1 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results1);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */

const results2 = await retriever.getRelevantDocuments(
  "What is my favourite food?"
);

console.log(results2);

/*
[
  Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
 */
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/vectorstore

# Vector Store
Once you've created a Vector Store, the way to use it as a Retriever is very simple:
```typescript
vectorStore = ...
retriever = vectorStore.asRetriever()
```



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/vespa-retriever

# Vespa Retriever
This shows how to use Vespa.ai as a LangChain retriever.
Vespa.ai is a platform for highly efficient structured text and vector search.
Please refer to Vespa.ai for more information.
The following sets up a retriever that fetches results from Vespa's documentation search:
```typescript
import { VespaRetriever } from "langchain/retrievers/vespa";

export const run = async () => {
  const url = "https://doc-search.vespa.oath.cloud";
  const query_body = {
    yql: "select content from paragraph where userQuery()",
    hits: 5,
    ranking: "documentation",
    locale: "en-us",
  };
  const content_field = "content";

  const retriever = new VespaRetriever({
    url,
    auth: false,
    query_body,
    content_field,
  });

  const result = await retriever.getRelevantDocuments("what is vespa?");
  console.log(result);
};
```
#### API Reference:
Here, up to 5 results are retrieved from the content field in the paragraph document type,
using documentation as the ranking method. The userQuery() is replaced with the actual query
passed from LangChain.
Please refer to the pyvespa documentation
for more information.
The URL is the endpoint of the Vespa application.
You can connect to any Vespa endpoint, either a remote service or a local instance using Docker.
However, most Vespa Cloud instances are protected with mTLS.
If this is your case, you can, for instance set up a CloudFlare Worker
that contains the necessary credentials to connect to the instance.
Now you can return the results and continue using them in LangChain.



Page URL: https://js.langchain.com/docs/modules/data_connection/retrievers/integrations/zep-retriever

# Zep Retriever
This example shows how to use the Zep Retriever in a RetrievalQAChain to retrieve documents from Zep memory store.
## Setup​
```typescript
npm i @getzep/zep-js
```
```typescript
yarn add @getzep/zep-js
```
```typescript
pnpm add @getzep/zep-js
```
## Usage​
```typescript
import { ZepRetriever } from "langchain/retrievers/zep";

export const run = async () => {
  const url = process.env.ZEP_URL || "http://localhost:8000";
  const sessionId = "TestSession1232";
  console.log(`Session ID: ${sessionId}, URL: ${url}`);

  const retriever = new ZepRetriever({ sessionId, url });

  const query = "hello";
  const docs = await retriever.getRelevantDocuments(query);

  console.log(docs);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/data_connection/experimental/multimodal_embeddings/google_vertex_ai

# Google Vertex AI
This API is new and may change in future LangChainJS versions.
The GoogleVertexAIMultimodalEmbeddings class provides additional methods that are
parallels to the embedDocuments() and embedQuery() methods:
Note: The Google Vertex AI embeddings models have different vector sizes
than OpenAI's standard model, so some vector stores may not handle them correctly.
## Setup​
The Vertex AI implementation is meant to be used in Node.js and not
directly in a browser, since it requires a service account to use.
Before running this code, you should make sure the Vertex AI API is
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
Google Cloud using one of these methods:
```typescript
npm install google-auth-library
```
```typescript
yarn add google-auth-library
```
```typescript
pnpm add google-auth-library
```
## Usage​
Here's a basic example that shows how to embed image queries:
```typescript
import fs from "fs";
import { GoogleVertexAIMultimodalEmbeddings } from "langchain/experimental/multimodal_embeddings/googlevertexai";

const model = new GoogleVertexAIMultimodalEmbeddings();

// Load the image into a buffer to get the embedding of it
const img = fs.readFileSync("/path/to/file.jpg");
const imgEmbedding = await model.embedImageQuery(img);
console.log({ imgEmbedding });

// You can also get text embeddings
const textEmbedding = await model.embedQuery(
  "What would be a good company name for a company that makes colorful socks?"
);
console.log({ textEmbedding });
```
#### API Reference:
## Advanced usage​
Here's a more advanced example that shows how to integrate these new embeddings with a LangChain vector store.
```typescript
import fs from "fs";
import { GoogleVertexAIMultimodalEmbeddings } from "langchain/experimental/multimodal_embeddings/googlevertexai";
import { FaissStore } from "langchain/vectorstores/faiss";
import { Document } from "langchain/document";

const embeddings = new GoogleVertexAIMultimodalEmbeddings();

const vectorStore = await FaissStore.fromTexts(
  ["dog", "cat", "horse", "seagull"],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }],
  embeddings
);

const img = fs.readFileSync("parrot.jpeg");
const vectors: number[] = await embeddings.embedImageQuery(img);
const document = new Document({
  pageContent: img.toString("base64"),
  // Metadata is optional but helps track what kind of document is being retrieved
  metadata: {
    id: 5,
    mediaType: "image",
  },
});

// Add the image embedding vectors to the vector store directly
await vectorStore.addVectors([vectors], [document]);

// Use a similar image to the one just added
const img2 = fs.readFileSync("parrot-icon.png");
const vectors2: number[] = await embeddings.embedImageQuery(img2);

// Use the lower level, direct API
const resultTwo = await vectorStore.similaritySearchVectorWithScore(
  vectors2,
  2
);
console.log(JSON.stringify(resultTwo, null, 2));

/*
  [
    [
      Document {
        pageContent: '<BASE64 ENCODED IMAGE DATA>'
        metadata: {
          id: 5,
          mediaType: "image"
        }
      },
      0.8931522965431213
    ],
    [
      Document {
        pageContent: 'seagull',
        metadata: {
          id: 4
        }
      },
      1.9188631772994995
    ]
  ]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/

# Chains
Using an LLM in isolation is fine for simple applications,
but more complex applications require chaining LLMs - either with each other or with other components.
LangChain provides the Chain interface for such "chained" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:
```typescript
import { CallbackManagerForChainRun } from "langchain/callbacks";
import { BaseChain as _ } from "langchain/chains";
import { BaseMemory } from "langchain/memory";
import { ChainValues } from "langchain/schema";

abstract class BaseChain {
  memory?: BaseMemory;

  /**
   * Run the core logic of this chain and return the output
   */
  abstract _call(
    values: ChainValues,
    runManager?: CallbackManagerForChainRun
  ): Promise<ChainValues>;

  /**
   * Return the string type key uniquely identifying this class of chain.
   */
  abstract _chainType(): string;

  /**
   * Return the list of input keys this chain expects to receive when called.
   */
  abstract get inputKeys(): string[];

  /**
   * Return the list of output keys this chain will produce when called.
   */
  abstract get outputKeys(): string[];
}
```
#### API Reference:
This idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.
For more specifics check out:
## Why do we need chains?​
Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.
## Get started​
### Using LLMChain​
The LLMChain is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.
To use the LLMChain, first create a prompt template.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// We can construct an LLMChain from a PromptTemplate and an LLM.
const model = new OpenAI({ temperature: 0 });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
```
We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.
```typescript
const chain = new LLMChain({ llm: model, prompt });

// Since this LLMChain is a single-input, single-output chain, we can also `run` it.
// This convenience method takes in a string and returns the value
// of the output key field in the chain response. For LLMChains, this defaults to "text".
const res = await chain.run("colorful socks");
console.log({ res });

// { res: "\n\nSocktastic!" }
```
If there are multiple variables, you can input them all at once using a dictionary.
This will return the complete chain response.
```typescript
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for {company} that makes {product}?"
);

const chain = new LLMChain({ llm: model, prompt });

const res = await chain.call({
  company: "a startup",
  product: "colorful socks"
});

console.log({ res });
// { res: { text: '\n\Socktopia Colourful Creations.' } }
```
You can use a chat model in an LLMChain as well:
```typescript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";

// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const chat = new ChatOpenAI({ temperature: 0 });
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "You are a helpful assistant that translates {input_language} to {output_language}."
  ),
  HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
const chainB = new LLMChain({
  prompt: chatPrompt,
  llm: chat,
});

const resB = await chainB.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming.",
});
console.log({ resB });
// { resB: { text: "J'adore la programmation." } }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/how_to/

# How to
## 📄️ Debugging chains
It can be hard to debug a Chain object solely from its output as most Chain objects involve a fair amount of input prompt preprocessing and LLM output post-processing.
## 📄️ Adding memory (state)
Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.



Page URL: https://js.langchain.com/docs/modules/chains/how_to/debugging

# Debugging chains
It can be hard to debug a Chain object solely from its output as most Chain objects involve a fair amount of input prompt preprocessing and LLM output post-processing.
Setting verbose to true will print out some internal states of the Chain object while running it.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const chat = new ChatOpenAI({});
// This chain automatically initializes and uses a `BufferMemory` instance
// as well as a default prompt.
const chain = new ConversationChain({ llm: chat, verbose: true });
const res = await chain.call({ input: "What is ChatGPT?" });

console.log({ res });

/*
[chain/start] [1:chain:ConversationChain] Entering Chain run with input: {
  "input": "What is ChatGPT?",
  "history": ""
}
[llm/start] [1:chain:ConversationChain > 2:llm:ChatOpenAI] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: What is ChatGPT?\nAI:",
          "additional_kwargs": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:ConversationChain > 2:llm:ChatOpenAI] [3.54s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations.",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "AIMessage"
          ],
          "kwargs": {
            "content": "ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations.",
            "additional_kwargs": {}
          }
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 100,
      "promptTokens": 69,
      "totalTokens": 169
    }
  }
}
[chain/end] [1:chain:ConversationChain] [3.91s] Exiting Chain run with output: {
  "response": "ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations."
}
{
  res: {
    response: 'ChatGPT is a language model developed by OpenAI. It is designed to generate human-like responses in a conversational manner. It is trained on a large amount of text data from the internet and is capable of understanding and generating text across a wide range of topics. ChatGPT uses deep learning techniques, specifically a method called the transformer architecture, to process and generate high-quality text responses. Its purpose is to assist users in various conversational tasks, provide information, and engage in interactive conversations.'
  }
}
*/
```
You can also set this globally by setting the LANGCHAIN_VERBOSE environment variable to "true".



Page URL: https://js.langchain.com/docs/modules/chains/how_to/memory

# Adding memory (state)
Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.
## Get started​
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";

const chat = new ChatOpenAI({});

const memory = new BufferMemory();

// This particular chain automatically initializes a BufferMemory instance if none is provided,
// but we pass it explicitly here. It also has a default prompt.
const chain = new ConversationChain({ llm: chat, memory });

const res1 = await chain.run("Answer briefly. What are the first 3 colors of a rainbow?");
console.log(res1);

// The first three colors of a rainbow are red, orange, and yellow.

const res2 = await chain.run("And the next 4?");
console.log(res2);

// The next four colors of a rainbow are green, blue, indigo, and violet.
```
Essentially, BaseMemory defines an interface of how LangChain stores memory. It allows reading of stored data through loadMemoryVariables method and storing new data through saveContext method. You can learn more about it in the Memory section.



Page URL: https://js.langchain.com/docs/modules/chains/foundational/

# Foundational
## 📄️ LLM
An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.
## 📄️ Sequential
The next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.



Page URL: https://js.langchain.com/docs/modules/chains/foundational/llm_chain

# LLM
An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.
An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.
## Get started​
We can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// We can construct an LLMChain from a PromptTemplate and an LLM.
const model = new OpenAI({ temperature: 0 });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
const chainA = new LLMChain({ llm: model, prompt });

// The result is an object with a `text` property.
const resA = await chainA.call({ product: "colorful socks" });
console.log({ resA });
// { resA: { text: '\n\nSocktastic!' } }

// Since this LLMChain is a single-input, single-output chain, we can also `run` it.
// This convenience method takes in a string and returns the value
// of the output key field in the chain response. For LLMChains, this defaults to "text".
const resA2 = await chainA.run("colorful socks");
console.log({ resA2 });
// { resA2: '\n\nSocktastic!' }
```
#### API Reference:
## Usage with Chat Models​
We can also construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to a ChatModel:
```typescript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";

// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const chat = new ChatOpenAI({ temperature: 0 });
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "You are a helpful assistant that translates {input_language} to {output_language}."
  ),
  HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
const chainB = new LLMChain({
  prompt: chatPrompt,
  llm: chat,
});

const resB = await chainB.call({
  input_language: "English",
  output_language: "French",
  text: "I love programming.",
});
console.log({ resB });
// { resB: { text: "J'adore la programmation." } }
```
#### API Reference:
## Usage in Streaming Mode​
We can also construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM in streaming mode, which will stream back tokens as they are generated:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const model = new OpenAI({ temperature: 0.9, streaming: true });
const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);
const chain = new LLMChain({ llm: model, prompt });

// Call the chain with the inputs and a callback for the streamed tokens
const res = await chain.call({ product: "colorful socks" }, [
  {
    handleLLMNewToken(token: string) {
      process.stdout.write(token);
    },
  },
]);
console.log({ res });
// { res: { text: '\n\nKaleidoscope Socks' } }
```
#### API Reference:
## Cancelling a running LLMChain​
We can also cancel a running LLMChain by passing an AbortSignal to the call method:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";

// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const model = new OpenAI({ temperature: 0.9, streaming: true });
const prompt = PromptTemplate.fromTemplate(
  "Give me a long paragraph about {product}?"
);
const chain = new LLMChain({ llm: model, prompt });
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.
setTimeout(() => {
  controller.abort();
}, 3000);

try {
  // Call the chain with the inputs and a callback for the streamed tokens
  const res = await chain.call(
    { product: "colorful socks", signal: controller.signal },
    [
      {
        handleLLMNewToken(token: string) {
          process.stdout.write(token);
        },
      },
    ]
  );
} catch (e) {
  console.log(e);
  // Error: Cancel: canceled
}
```
#### API Reference:
In this example we show cancellation in streaming mode, but it works the same way in non-streaming mode.



Page URL: https://js.langchain.com/docs/modules/chains/foundational/sequential_chains

# Sequential
The next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.
In this notebook we will walk through some examples for how to do this, using sequential chains. Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario. There are two types of sequential chains:
## SimpleSequentialChain​
Let's start with the simplest possible case which is SimpleSequentialChain.
An SimpleSequentialChain is a chain that allows you to join multiple single-input/single-output chains into one chain.
The example below shows a sample usecase. In the first step, given a title, a synopsis of a play is generated. In the second step, based on the generated synopsis, a review of the play is generated.
```typescript
import { SimpleSequentialChain, LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// This is an LLMChain to write a synopsis given a title of a play.
const llm = new OpenAI({ temperature: 0 });
const template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
 
  Title: {title}
  Playwright: This is a synopsis for the above play:`;
const promptTemplate = new PromptTemplate({
  template,
  inputVariables: ["title"],
});
const synopsisChain = new LLMChain({ llm, prompt: promptTemplate });

// This is an LLMChain to write a review of a play given a synopsis.
const reviewLLM = new OpenAI({ temperature: 0 });
const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
 
  Play Synopsis:
  {synopsis}
  Review from a New York Times play critic of the above play:`;
const reviewPromptTemplate = new PromptTemplate({
  template: reviewTemplate,
  inputVariables: ["synopsis"],
});
const reviewChain = new LLMChain({
  llm: reviewLLM,
  prompt: reviewPromptTemplate,
});

const overallChain = new SimpleSequentialChain({
  chains: [synopsisChain, reviewChain],
  verbose: true,
});
const review = await overallChain.run("Tragedy at sunset on the beach");
console.log(review);
/*
    variable review contains the generated play review based on the input title and synopsis generated in the first step:

    "Tragedy at Sunset on the Beach is a powerful and moving story of love, loss, and redemption. The play follows the story of two young lovers, Jack and Jill, whose plans for a future together are tragically cut short when Jack is killed in a car accident. The play follows Jill as she struggles to cope with her grief and eventually finds solace in the arms of another man. 
    The play is beautifully written and the performances are outstanding. The actors bring the characters to life with their heartfelt performances, and the audience is taken on an emotional journey as Jill is forced to confront her grief and make a difficult decision between her past and her future. The play culminates in a powerful climax that will leave the audience in tears. 
    Overall, Tragedy at Sunset on the Beach is a powerful and moving story that will stay with you long after the curtain falls. It is a must-see for anyone looking for an emotionally charged and thought-provoking experience."
*/
```
#### API Reference:
## SequentialChain​
More advanced scenario useful when you have multiple chains that have more than one input or ouput keys.
Unlike for SimpleSequentialChain, outputs from all previous chains will be available to the next chain.
```typescript
import { SequentialChain, LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.
const llm = new OpenAI({ temperature: 0 });
const template = `You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.

Title: {title}
Era: {era}
Playwright: This is a synopsis for the above play:`;
const promptTemplate = new PromptTemplate({
  template,
  inputVariables: ["title", "era"],
});
const synopsisChain = new LLMChain({
  llm,
  prompt: promptTemplate,
  outputKey: "synopsis",
});

// This is an LLMChain to write a review of a play given a synopsis.
const reviewLLM = new OpenAI({ temperature: 0 });
const reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
  
   Play Synopsis:
   {synopsis}
   Review from a New York Times play critic of the above play:`;
const reviewPromptTemplate = new PromptTemplate({
  template: reviewTemplate,
  inputVariables: ["synopsis"],
});
const reviewChain = new LLMChain({
  llm: reviewLLM,
  prompt: reviewPromptTemplate,
  outputKey: "review",
});

const overallChain = new SequentialChain({
  chains: [synopsisChain, reviewChain],
  inputVariables: ["era", "title"],
  // Here we return multiple variables
  outputVariables: ["synopsis", "review"],
  verbose: true,
});
const chainExecutionResult = await overallChain.call({
  title: "Tragedy at sunset on the beach",
  era: "Victorian England",
});
console.log(chainExecutionResult);
/*
    variable chainExecutionResult contains final review and intermediate synopsis (as specified by outputVariables). The data is generated based on the input title and era:

    "{
      "review": "

    Tragedy at Sunset on the Beach is a captivating and heartbreaking story of love and loss. Set in Victorian England, the play follows Emily, a young woman struggling to make ends meet in a small coastal town. Emily's dreams of a better life are dashed when she discovers her employer's scandalous affair, and her plans are further thwarted when she meets a handsome stranger on the beach.

    The play is a powerful exploration of the human condition, as Emily must grapple with the truth and make a difficult decision that will change her life forever. The performances are outstanding, with the actors bringing a depth of emotion to their characters that is both heartbreaking and inspiring.

    Overall, Tragedy at Sunset on the Beach is a beautiful and moving play that will leave audiences in tears. It is a must-see for anyone looking for a powerful and thought-provoking story.",
      "synopsis": "

    Tragedy at Sunset on the Beach is a play set in Victorian England. It tells the story of a young woman, Emily, who is struggling to make ends meet in a small coastal town. She works as a maid for a wealthy family, but her dreams of a better life are dashed when she discovers that her employer is involved in a scandalous affair.

    Emily is determined to make a better life for herself, but her plans are thwarted when she meets a handsome stranger on the beach one evening. The two quickly fall in love, but their happiness is short-lived when Emily discovers that the stranger is actually a member of the wealthy family she works for.

    The play follows Emily as she struggles to come to terms with the truth and make sense of her life. As the sun sets on the beach, Emily must decide whether to stay with the man she loves or to leave him and pursue her dreams. In the end, Emily must make a heartbreaking decision that will change her life forever.",
    }"
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/document/

# Documents
These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.
These chains are all loaded in a similar way:
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  loadQAStuffChain,
  loadQAMapReduceChain,
  loadQARefineChain
} from "langchain/chains";
import { Document } from "langchain/document";

// This first example uses the `StuffDocumentsChain`.
const llmA = new OpenAI({});
const chainA = loadQAStuffChain(llmA);
const docs = [
  new Document({ pageContent: "Harrison went to Harvard." }),
  new Document({ pageContent: "Ankush went to Princeton." }),
];
const resA = await chainA.call({
  input_documents: docs,
  question: "Where did Harrison go to college?",
});
console.log({ resA });
// { resA: { text: ' Harrison went to Harvard.' } }

// This second example uses the `MapReduceChain`.
// Optionally limit the number of concurrent requests to the language model.
const llmB = new OpenAI({ maxConcurrency: 10 });
const chainB = loadQAMapReduceChain(llmB);
const resB = await chainB.call({
  input_documents: docs,
  question: "Where did Harrison go to college?",
});
console.log({ resB });
// { resB: { text: ' Harrison went to Harvard.' } }
```
## 📄️ Stuff
The stuff documents chain ("stuff" as in "to stuff" or "to fill") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.
## 📄️ Refine
The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.
## 📄️ Map reduce
The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.



Page URL: https://js.langchain.com/docs/modules/chains/document/stuff

# Stuff
The stuff documents chain ("stuff" as in "to stuff" or "to fill") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.
This chain is well-suited for applications where documents are small and only a few are passed in for most calls.

Here's how it looks in practice:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadQAStuffChain } from "langchain/chains";
import { Document } from "langchain/document";

// This first example uses the `StuffDocumentsChain`.
const llmA = new OpenAI({});
const chainA = loadQAStuffChain(llmA);
const docs = [
  new Document({ pageContent: "Harrison went to Harvard." }),
  new Document({ pageContent: "Ankush went to Princeton." }),
];
const resA = await chainA.call({
  input_documents: docs,
  question: "Where did Harrison go to college?",
});
console.log({ resA });
// { resA: { text: ' Harrison went to Harvard.' } }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/document/refine

# Refine
The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.
Since the Refine chain only passes a single document to the LLM at a time, it is well-suited for tasks that require analyzing more documents than can fit in the model's context.
The obvious tradeoff is that this chain will make far more LLM calls than, for example, the Stuff documents chain.
There are also certain tasks which are difficult to accomplish iteratively. For example, the Refine chain can perform poorly when documents frequently cross-reference one another or when a task requires detailed information from many documents.

Here's how it looks in practice:
```typescript
import { loadQARefineChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

// Create the models and chain
const embeddings = new OpenAIEmbeddings();
const model = new OpenAI({ temperature: 0 });
const chain = loadQARefineChain(model);

// Load the documents and create the vector store
const loader = new TextLoader("./state_of_the_union.txt");
const docs = await loader.loadAndSplit();
const store = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Select the relevant documents
const question = "What did the president say about Justice Breyer";
const relevantDocs = await store.similaritySearch(question);

// Call the chain
const res = await chain.call({
  input_documents: relevantDocs,
  question,
});

console.log(res);
/*
{
  output_text: '\n' +
    '\n' +
    "The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act."
}
*/
```
#### API Reference:
## Prompt customization​
You may want to tweak the behavior of a step by changing the prompt. Here's an example of how to do that:
```typescript
import { loadQARefineChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { PromptTemplate } from "langchain/prompts";

export const questionPromptTemplateString = `Context information is below.
---------------------
{context}
---------------------
Given the context information and not prior knowledge, answer the question: {question}`;

const questionPrompt = new PromptTemplate({
  inputVariables: ["context", "question"],
  template: questionPromptTemplateString,
});

const refinePromptTemplateString = `The original question is as follows: {question}
We have provided an existing answer: {existing_answer}
We have the opportunity to refine the existing answer
(only if needed) with some more context below.
------------
{context}
------------
Given the new context, refine the original answer to better answer the question.
You must provide a response, either original answer or refined answer.`;

const refinePrompt = new PromptTemplate({
  inputVariables: ["question", "existing_answer", "context"],
  template: refinePromptTemplateString,
});

// Create the models and chain
const embeddings = new OpenAIEmbeddings();
const model = new OpenAI({ temperature: 0 });
const chain = loadQARefineChain(model, {
  questionPrompt,
  refinePrompt,
});

// Load the documents and create the vector store
const loader = new TextLoader("./state_of_the_union.txt");
const docs = await loader.loadAndSplit();
const store = await MemoryVectorStore.fromDocuments(docs, embeddings);

// Select the relevant documents
const question = "What did the president say about Justice Breyer";
const relevantDocs = await store.similaritySearch(question);

// Call the chain
const res = await chain.call({
  input_documents: relevantDocs,
  question,
});

console.log(res);
/*
{
  output_text: '\n' +
    '\n' +
    "The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act."
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/document/map_reduce

# Map reduce
The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.

Here's how it looks in practice:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadQAMapReduceChain } from "langchain/chains";
import { Document } from "langchain/document";

// Optionally limit the number of concurrent requests to the language model.
const model = new OpenAI({ temperature: 0, maxConcurrency: 10 });
const chain = loadQAMapReduceChain(model);
const docs = [
  new Document({ pageContent: "harrison went to harvard" }),
  new Document({ pageContent: "ankush went to princeton" }),
];
const res = await chain.call({
  input_documents: docs,
  question: "Where did harrison go to college",
});
console.log({ res });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/

# Popular
## 📄️ API chains
APIChain enables using LLMs to interact with APIs to retrieve relevant information. Construct the chain by providing a question relevant to the provided API documentation.
## 📄️ Retrieval QA
This example showcases question answering over an index.
## 📄️ Conversational Retrieval QA
The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.
## 📄️ SQL
This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.
## 📄️ Structured Output with OpenAI functions
Must be used with an OpenAI functions model.
## 📄️ Summarization
A summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. You can also choose instead for the chain that does summarization to be a StuffDocumentsChain, or a RefineDocumentsChain.



Page URL: https://js.langchain.com/docs/modules/chains/popular/api

# API chains
APIChain enables using LLMs to interact with APIs to retrieve relevant information. Construct the chain by providing a question relevant to the provided API documentation.
If your API requires authentication or other headers, you can pass the chain a headers property in the config object.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { APIChain } from "langchain/chains";

const OPEN_METEO_DOCS = `BASE URL: https://api.open-meteo.com/

API Documentation
The API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:

Parameter	Format	Required	Default	Description
latitude, longitude	Floating point	Yes		Geographical WGS84 coordinate of the location
hourly	String array	No		A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.
daily	String array	No		A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.
current_weather	Bool	No	false	Include current weather conditions in the JSON output.
temperature_unit	String	No	celsius	If fahrenheit is set, all temperature values are converted to Fahrenheit.
windspeed_unit	String	No	kmh	Other wind speed speed units: ms, mph and kn
precipitation_unit	String	No	mm	Other precipitation amount units: inch
timeformat	String	No	iso8601	If format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.
timezone	String	No	GMT	If timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.
past_days	Integer (0-2)	No	0	If past_days is set, yesterday or the day before yesterday data are also returned.
start_date
end_date	String (yyyy-mm-dd)	No		The time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).
models	String array	No	auto	Manually select one or more weather models. Per default, the best suitable weather models will be combined.

Variable	Valid time	Unit	Description
temperature_2m	Instant	°C (°F)	Air temperature at 2 meters above ground
snowfall	Preceding hour sum	cm (inch)	Snowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent
rain	Preceding hour sum	mm (inch)	Rain from large scale weather systems of the preceding hour in millimeter
showers	Preceding hour sum	mm (inch)	Showers from convective precipitation in millimeters from the preceding hour
weathercode	Instant	WMO code	Weather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.
snow_depth	Instant	meters	Snow depth on the ground
freezinglevel_height	Instant	meters	Altitude above sea level of the 0°C level
visibility	Instant	meters	Viewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.`;

export async function run() {
  const model = new OpenAI({ modelName: "text-davinci-003" });
  const chain = APIChain.fromLLMAndAPIDocs(model, OPEN_METEO_DOCS, {
    headers: {
      // These headers will be used for API requests made by the chain.
    },
  });

  const res = await chain.call({
    question:
      "What is the weather like right now in Munich, Germany in degrees Farenheit?",
  });
  console.log({ res });
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/vector_db_qa

# Retrieval QA
This example showcases question answering over an index.
The RetrievalQAChain is a chain that combines a Retriever and a QA chain (described above). It is used to retrieve documents from a Retriever and then use a QA chain to answer a question based on the retrieved documents.
## Usage​
In the below example, we are using a VectorStore as the Retriever. By default, the StuffDocumentsChain is used as the QA chain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const chain = RetrievalQAChain.fromLLM(model, vectorStoreRetriever);
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
    and retiring Justice of the United States Supreme Court and thanked him for his service.'
  }
}
*/
```
#### API Reference:
## Custom QA chain​
In the below example, we are using a VectorStore as the Retriever and a MapReduceDocumentsChain as the QA chain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAMapReduceChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Create a chain that uses a map reduce chain and HNSWLib vector store.
const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAMapReduceChain(model),
  retriever: vectorStore.asRetriever(),
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: " The president said that Justice Breyer has dedicated his life to serve his country, and thanked him for his service. He also said that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, emphasizing the importance of protecting the rights of citizens, especially women, LGBTQ+ Americans, and access to healthcare. He also expressed his commitment to supporting the younger transgender Americans in America and ensuring they are able to reach their full potential, offering a Unity Agenda for the Nation to beat the opioid epidemic and increase funding for prevention, treatment, harm reduction, and recovery."
  }
}
*/
```
#### API Reference:
## Custom prompts​
You can pass in custom prompts to do question answering. These prompts are the same prompts as you can pass into the base question answering chains.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAStuffChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { PromptTemplate } from "langchain/prompts";
import * as fs from "fs";

const promptTemplate = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Answer in Italian:`;
const prompt = PromptTemplate.fromTemplate(promptTemplate);

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Create a chain that uses a Refine chain and HNSWLib vector store.
const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAStuffChain(model, { prompt }),
  retriever: vectorStore.asRetriever(),
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });
/*
{
  res: {
    text: ' Il presidente ha elogiato Justice Breyer per il suo servizio e lo ha ringraziato.'
  }
}
*/
```
#### API Reference:
## Return Source Documents​
Additionally, we can return the source documents used to answer the question by specifying an optional parameter when constructing the chain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({});
const text = fs.readFileSync("data/state_of_the_union_2022.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Create a chain that uses a map reduce chain and HNSWLib vector store.
const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {
  returnSourceDocuments: true, // Can also be passed into the constructor
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log(JSON.stringify(res, null, 2));
/*
{
  "text": " The president thanked Justice Breyer for his service and asked him to stand so he could be seen.",
  "sourceDocuments": [
    {
      "pageContent": "Justice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.\n\nAnd we all know — no matter what your ideology, we all know one of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\n\nAs I did four days ago, I’ve nominated a Circuit Court of Appeals — Ketanji Brown Jackson. One of our nation’s top legal minds who will continue in just Brey- — Justice Breyer’s legacy of excellence. A former top litigator in private practice, a former federal public defender from a family of public-school educators and police officers — she’s a consensus builder.\n\nSince she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 481,
            "to": 487
          }
        }
      }
    },
    {
      "pageContent": "Since she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\n\nJudge Ketanji Brown Jackson\nPresident Biden's Unity AgendaLearn More\nSince she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\n\nFolks, if we are to advance liberty and justice, we need to secure our border and fix the immigration system.\n\nAnd as you might guess, I think we can do both. At our border, we’ve installed new technology, like cutting-edge scanners, to better detect drug smuggling.\n\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\n\nWe’re putting in place dedicated immigration judges in significant larger number so families fleeing persecution and violence can have their cases — cases heard faster — and those who aren’t legitimately here can be sent back.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 487,
            "to": 499
          }
        }
      }
    },
    {
      "pageContent": "These laws don’t infringe on the Second Amendment; they save lives.\n\nGun Violence\n\n\nThe most fundamental right in America is the right to vote and have it counted. And look, it’s under assault.\n\nIn state after state, new laws have been passed not only to suppress the vote — we’ve been there before — but to subvert the entire election. We can’t let this happen.\n\nTonight, I call on the Senate to pass — pass the Freedom to Vote Act. Pass the John Lewis Act — Voting Rights Act. And while you’re at it, pass the DISCLOSE Act so Americans know who is funding our elections.\n\nLook, tonight, I’d — I’d like to honor someone who has dedicated his life to serve this country: Justice Breyer — an Army veteran, Constitutional scholar, retiring Justice of the United States Supreme Court.\n\nJustice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 468,
            "to": 481
          }
        }
      }
    },
    {
      "pageContent": "If you want to go forward not backwards, we must protect access to healthcare; preserve a woman’s right to choose — and continue to advance maternal healthcare for all Americans.\n\nRoe v. Wade\n\n\nAnd folks, for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families — it’s simply wrong.\n\nAs I said last year, especially to our younger transgender Americans, I’ll always have your back as your President so you can be yourself and reach your God-given potential.\n\nBipartisan Equality Act\n\n\nFolks as I’ve just demonstrated, while it often appears we do not agree and that — we — we do agree on a lot more things than we acknowledge.",
      "metadata": {
        "loc": {
          "lines": {
            "from": 511,
            "to": 523
          }
        }
      }
    }
  ]
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/chat_vector_db

# Conversational Retrieval QA
The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.
It first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question answering chain to return a response.
To create one, you will need a retriever. In the below example, we will create one from a vector store, which can be created from embeddings.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";
import * as fs from "fs";

export const run = async () => {
  /* Initialize the LLM to use to answer the question */
  const model = new ChatOpenAI({});
  /* Load in the file we want to do question answering over */
  const text = fs.readFileSync("state_of_the_union.txt", "utf8");
  /* Split the text into chunks */
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  /* Create the vectorstore */
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  /* Create the chain */
  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    vectorStore.asRetriever(),
    {
      memory: new BufferMemory({
        memoryKey: "chat_history", // Must be set to "chat_history"
      }),
    }
  );
  /* Ask it a question */
  const question = "What did the president say about Justice Breyer?";
  const res = await chain.call({ question });
  console.log(res);
  /* Ask it a follow up question */
  const followUpRes = await chain.call({
    question: "Was that nice?",
  });
  console.log(followUpRes);
};
```
#### API Reference:
In the above code snippet, the fromLLM method of the ConversationalRetrievalQAChain class has the following signature:
```typescript
static fromLLM(
  llm: BaseLanguageModel,
  retriever: BaseRetriever,
  options?: {
    questionGeneratorChainOptions?: {
      llm?: BaseLanguageModel;
      template?: string;
    };
    qaChainOptions?: QAChainParams;
    returnSourceDocuments?: boolean;
  }
): ConversationalRetrievalQAChain
```
Here's an explanation of each of the attributes of the options object:
## Built-in Memory​
Here's a customization example using a faster LLM to generate questions and a slower, more comprehensive LLM for the final answer. It uses a built-in memory object and returns the referenced source documents.
Because we have returnSourceDocuments set and are thus returning multiple values from the chain, we must set inputKey and outputKey on the memory instance
to let it know which values to store.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";

import * as fs from "fs";

export const run = async () => {
  const text = fs.readFileSync("state_of_the_union.txt", "utf8");
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  const fasterModel = new ChatOpenAI({
    modelName: "gpt-3.5-turbo",
  });
  const slowerModel = new ChatOpenAI({
    modelName: "gpt-4",
  });
  const chain = ConversationalRetrievalQAChain.fromLLM(
    slowerModel,
    vectorStore.asRetriever(),
    {
      returnSourceDocuments: true,
      memory: new BufferMemory({
        memoryKey: "chat_history",
        inputKey: "question", // The key for the input to the chain
        outputKey: "text", // The key for the final conversational output of the chain
        returnMessages: true, // If using with a chat model (e.g. gpt-3.5 or gpt-4)
      }),
      questionGeneratorChainOptions: {
        llm: fasterModel,
      },
    }
  );
  /* Ask it a question */
  const question = "What did the president say about Justice Breyer?";
  const res = await chain.call({ question });
  console.log(res);

  const followUpRes = await chain.call({ question: "Was that nice?" });
  console.log(followUpRes);
};
```
#### API Reference:
## Streaming​
You can also use the above concept of using two different LLMs to stream only the final response from the chain, and not output from the intermediate standalone question generation step. Here's an example:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";

import * as fs from "fs";

export const run = async () => {
  const text = fs.readFileSync("state_of_the_union.txt", "utf8");
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  let streamedResponse = "";
  const streamingModel = new ChatOpenAI({
    streaming: true,
    callbacks: [
      {
        handleLLMNewToken(token) {
          streamedResponse += token;
        },
      },
    ],
  });
  const nonStreamingModel = new ChatOpenAI({});
  const chain = ConversationalRetrievalQAChain.fromLLM(
    streamingModel,
    vectorStore.asRetriever(),
    {
      returnSourceDocuments: true,
      memory: new BufferMemory({
        memoryKey: "chat_history",
        inputKey: "question", // The key for the input to the chain
        outputKey: "text", // The key for the final conversational output of the chain
        returnMessages: true, // If using with a chat model
      }),
      questionGeneratorChainOptions: {
        llm: nonStreamingModel,
      },
    }
  );
  /* Ask it a question */
  const question = "What did the president say about Justice Breyer?";
  const res = await chain.call({ question });
  console.log({ streamedResponse });
  /*
    {
      streamedResponse: 'President Biden thanked Justice Breyer for his service, and honored him as an Army veteran, Constitutional scholar and retiring Justice of the United States Supreme Court.'
    }
  */
};
```
#### API Reference:
## Externally-Managed Memory​
For this chain, if you'd like to format the chat history in a custom way (or pass in chat messages directly for convenience), you can also pass the chat history in explicitly by omitting the memory option and supplying
a chat_history string or array of HumanMessages and AIMessages directly into the chain.call method:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

/* Initialize the LLM to use to answer the question */
const model = new OpenAI({});
/* Load in the file we want to do question answering over */
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
/* Split the text into chunks */
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);
/* Create the vectorstore */
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
/* Create the chain */
const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever()
);
/* Ask it a question */
const question = "What did the president say about Justice Breyer?";
/* Can be a string or an array of chat messages */
const res = await chain.call({ question, chat_history: "" });
console.log(res);
/* Ask it a follow up question */
const chatHistory = `${question}\n${res.text}`;
const followUpRes = await chain.call({
  question: "Was that nice?",
  chat_history: chatHistory,
});
console.log(followUpRes);
```
#### API Reference:
## Prompt Customization​
If you want to further change the chain's behavior, you can change the prompts for both the underlying question generation chain and the QA chain.
One case where you might want to do this is to improve the chain's ability to answer meta questions about the chat history.
By default, the only input to the QA chain is the standalone question generated from the question generation chain.
This poses a challenge when asking meta questions about information in previous interactions from the chat history.
For example, if you introduce a friend Bob and mention his age as 28, the chain is unable to provide his age upon asking a question like "How old is Bob?".
This limitation occurs because the bot searches for Bob in the vector store, rather than considering the message history.
You can pass an alternative prompt for the question generation chain that also returns parts of the chat history relevant to the answer,
allowing the QA chain to answer meta questions with the additional context:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { BufferMemory } from "langchain/memory";

const CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT = `Given the following conversation and a follow up question, return the conversation history excerpt that includes any relevant context to the question if it exists and rephrase the follow up question to be a standalone question.
Chat History:
{chat_history}
Follow Up Input: {question}
Your answer should follow the following format:
\`\`\`
Use the following pieces of context to answer the users question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
<Relevant chat history excerpt as context here>
Standalone question: <Rephrased question here>
\`\`\`
Your answer:`;

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const vectorStore = await HNSWLib.fromTexts(
  [
    "Mitochondria are the powerhouse of the cell",
    "Foo is red",
    "Bar is red",
    "Buildings are made out of brick",
    "Mitochondria are made of lipids",
  ],
  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],
  new OpenAIEmbeddings()
);

const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever(),
  {
    memory: new BufferMemory({
      memoryKey: "chat_history",
      returnMessages: true,
    }),
    questionGeneratorChainOptions: {
      template: CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT,
    },
  }
);

const res = await chain.call({
  question:
    "I have a friend called Bob. He's 28 years old. He'd like to know what the powerhouse of the cell is?",
});

console.log(res);
/*
  {
    text: "The powerhouse of the cell is the mitochondria."
  }
*/

const res2 = await chain.call({
  question: "How old is Bob?",
});

console.log(res2); // Bob is 28 years old.

/*
  {
    text: "Bob is 28 years old."
  }
*/
```
#### API Reference:
Keep in mind that adding more context to the prompt in this way may distract the LLM from other relevant retrieved information.



Page URL: https://js.langchain.com/docs/modules/chains/popular/sqlite

# SQL
This example demonstrates the use of the SQLDatabaseChain for answering questions over a SQL database.
This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
## Set up​
First install typeorm:
```typescript
npm install typeorm
```
Then install the dependencies needed for your database. For example, for SQLite:
```typescript
npm install sqlite3
```
For other databases see https://typeorm.io/#installation
Finally follow the instructions on https://database.guide/2-sample-databases-sqlite/ to get the sample database for this example.
```typescript
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";

/**
 * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
const datasource = new DataSource({
  type: "sqlite",
  database: "Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
});

const res = await chain.run("How many tracks are there?");
console.log(res);
// There are 3503 tracks.
```
#### API Reference:
You can include or exclude tables when creating the SqlDatabase object to help the chain focus on the tables you want.
It can also reduce the number of tokens used in the chain.
```typescript
const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
  includesTables: ["Track"],
});
```
If desired, you can return the used SQL command when calling the chain.
```typescript
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";

/**
 * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
const datasource = new DataSource({
  type: "sqlite",
  database: "Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
  sqlOutputKey: "sql",
});

const res = await chain.call({ query: "How many tracks are there?" });
/* Expected result:
 * {
 *   result: ' There are 3503 tracks.',
 *   sql: ' SELECT COUNT(*) FROM "Track";'
 * }
 */
console.log(res);
```
#### API Reference:
## Custom prompt​
You can also customize the prompt that is used. Here is an example prompting the model to understand that "foobar" is the same as the Employee table:
```typescript
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";
import { PromptTemplate } from "langchain/prompts";

const template = `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.
Use the following format:

Question: "Question here"
SQLQuery: "SQL Query to run"
SQLResult: "Result of the SQLQuery"
Answer: "Final answer here"

Only use the following tables:

{table_info}

If someone asks for the table foobar, they really mean the employee table.

Question: {input}`;

const prompt = PromptTemplate.fromTemplate(template);

/**
 * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
 * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
 * in the examples folder.
 */
const datasource = new DataSource({
  type: "sqlite",
  database: "data/Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
  sqlOutputKey: "sql",
  prompt,
});

const res = await chain.call({
  query: "How many employees are there in the foobar table?",
});
console.log(res);

/*
  {
    result: ' There are 8 employees in the foobar table.',
    sql: ' SELECT COUNT(*) FROM Employee;'
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/popular/structured_output

# Structured Output with OpenAI functions
Must be used with an OpenAI functions model.
This chain leverages OpenAI functions to output objects that match a given format for any given prompt.
It converts input schema into an OpenAI function, then forces OpenAI to call that function to return a response in the correct format.
You can use it where you would use a chain with a StructuredOutputParser without any special
instructions stuffed into the prompt. It will also more reliably output structured results with higher temperature values, making it better suited
for more creative applications.
Note: The outermost layer of the input schema must be an object.
## Usage​
### Format Text into Structured Data​
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
} from "langchain/prompts";
import { createStructuredOutputChainFromZod } from "langchain/chains/openai_functions";

const zodSchema = z.object({
  foods: z
    .array(
      z.object({
        name: z.string().describe("The name of the food item"),
        healthy: z.boolean().describe("Whether the food is good for you"),
        color: z.string().optional().describe("The color of the food"),
      })
    )
    .describe("An array of food items mentioned in the text"),
});

const prompt = new ChatPromptTemplate({
  promptMessages: [
    SystemMessagePromptTemplate.fromTemplate(
      "List all food items mentioned in the following text."
    ),
    HumanMessagePromptTemplate.fromTemplate("{inputText}"),
  ],
  inputVariables: ["inputText"],
});

const llm = new ChatOpenAI({ modelName: "gpt-3.5-turbo-0613", temperature: 0 });

const chain = createStructuredOutputChainFromZod(zodSchema, {
  prompt,
  llm,
});

const response = await chain.call({
  inputText: "I like apples, bananas, oxygen, and french fries.",
});

console.log(JSON.stringify(response, null, 2));

/*
  {
    "output": {
      "foods": [
        {
          "name": "apples",
          "healthy": true,
          "color": "red"
        },
        {
          "name": "bananas",
          "healthy": true,
          "color": "yellow"
        },
        {
          "name": "french fries",
          "healthy": false,
          "color": "golden"
        }
      ]
    }
  }
*/
```
#### API Reference:
### Generate a Database Record​
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
} from "langchain/prompts";
import { createStructuredOutputChainFromZod } from "langchain/chains/openai_functions";

const zodSchema = z.object({
  name: z.string().describe("Human name"),
  surname: z.string().describe("Human surname"),
  age: z.number().describe("Human age"),
  birthplace: z.string().describe("Where the human was born"),
  appearance: z.string().describe("Human appearance description"),
  shortBio: z.string().describe("Short bio secription"),
  university: z.string().optional().describe("University name if attended"),
  gender: z.string().describe("Gender of the human"),
  interests: z
    .array(z.string())
    .describe("json array of strings human interests"),
});

const prompt = new ChatPromptTemplate({
  promptMessages: [
    SystemMessagePromptTemplate.fromTemplate(
      "Generate details of a hypothetical person."
    ),
    HumanMessagePromptTemplate.fromTemplate("Additional context: {inputText}"),
  ],
  inputVariables: ["inputText"],
});

const llm = new ChatOpenAI({ modelName: "gpt-3.5-turbo-0613", temperature: 1 });

const chain = createStructuredOutputChainFromZod(zodSchema, {
  prompt,
  llm,
  outputKey: "person",
});

const response = await chain.call({
  inputText:
    "Please generate a diverse group of people, but don't generate anyone who likes video games.",
});

console.log(JSON.stringify(response, null, 2));

/*
  {
    "person": {
      "name": "Sophia",
      "surname": "Martinez",
      "age": 32,
      "birthplace": "Mexico City, Mexico",
      "appearance": "Sophia has long curly brown hair and hazel eyes. She has a warm smile and a contagious laugh.",
      "shortBio": "Sophia is a passionate environmentalist who is dedicated to promoting sustainable living. She believes in the power of individual actions to create a positive impact on the planet.",
      "university": "Stanford University",
      "gender": "Female",
      "interests": [
        "Hiking",
        "Yoga",
        "Cooking",
        "Reading"
      ]
    }
  }
*/
```
#### API Reference:
### Customization​
This chain takes all the same arguments as a standard LLMChain minus an outputParser. It will also be created with a default model set to gpt-3.5-turbo-0613,
but you can pass an options parameter into the input parameters with a pre-created ChatOpenAI instance as llm.



Page URL: https://js.langchain.com/docs/modules/chains/popular/summarize

# Summarization
A summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. You can also choose instead for the chain that does summarization to be a StuffDocumentsChain, or a RefineDocumentsChain.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain } from "langchain/chains";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// This convenience function creates a document chain prompted to summarize a set of documents.
const chain = loadSummarizationChain(model, { type: "map_reduce" });
const res = await chain.call({
  input_documents: docs,
});
console.log({ res });
/*
{
  res: {
    text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
    He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
    The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
  }
}
*/
```
#### API Reference:
## Intermediate Steps​
We can also return the intermediate steps for map_reduce chains, should we want to inspect them. This is done with the returnIntermediateSteps parameter.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain } from "langchain/chains";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// This convenience function creates a document chain prompted to summarize a set of documents.
const chain = loadSummarizationChain(model, {
  type: "map_reduce",
  returnIntermediateSteps: true,
});
const res = await chain.call({
  input_documents: docs,
});
console.log({ res });
/*
{
  res: {
    intermediateSteps: [
      "In response to Russia's aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.",
      "The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.",
      " President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America's infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.",
    ],
    text: "President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
      He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
      The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.",
  },
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/

# Additional
## 🗃️ OpenAI functions chains
3 items
## 📄️ Analyze Document
The AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.
## 📄️ Self-critique chain with constitutional AI
The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.
## 📄️ Moderation
This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.
## 📄️ Dynamically selecting from multiple prompts
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects the prompt to use for a given input. Specifically we show how to use the MultiPromptChain to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt.
## 📄️ Dynamically selecting from multiple retrievers
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/

# OpenAI functions chains
These chains are designed to be used with an OpenAI Functions model.
## 📄️ Extraction
Must be used with an OpenAI Functions model.
## 📄️ OpenAPI Calls
Must be used with an OpenAI Functions model.
## 📄️ Tagging
Must be used with an OpenAI Functions model.



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/extraction

# Extraction
Must be used with an OpenAI Functions model.
This chain is designed to extract lists of objects from an input text and schema of desired info.
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { createExtractionChainFromZod } from "langchain/chains";

const zodSchema = z.object({
  "person-name": z.string().optional(),
  "person-age": z.number().optional(),
  "person-hair_color": z.string().optional(),
  "dog-name": z.string().optional(),
  "dog-breed": z.string().optional(),
});
const chatModel = new ChatOpenAI({
  modelName: "gpt-3.5-turbo-0613",
  temperature: 0,
});
const chain = createExtractionChainFromZod(zodSchema, chatModel);

console.log(
  await chain.run(`Alex is 5 feet tall. Claudia is 4 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.
Alex's dog Frosty is a labrador and likes to play hide and seek.`)
);
/*
[
  {
    'person-name': 'Alex',
    'person-age': 0,
    'person-hair_color': 'blonde',
    'dog-name': 'Frosty',
    'dog-breed': 'labrador'
  },
  {
    'person-name': 'Claudia',
    'person-age': 0,
    'person-hair_color': 'brunette',
    'dog-name': '',
    'dog-breed': ''
  }
]
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/openapi

# OpenAPI Calls
Must be used with an OpenAI Functions model.
This chain can automatically select and call APIs based only on an OpenAPI spec.
It parses an input OpenAPI spec into JSON Schema that the OpenAI functions API can handle.
This allows ChatGPT to automatically select the correct method and populate the correct parameters for the a API call in the spec for a given user input.
We then make the actual API call, and return the result.
## Usage​
The below examples initialize the chain with a URL hosting an OpenAPI spec for brevity, but you can also directly pass a spec into the method.
### Query XKCD​
```typescript
import { createOpenAPIChain } from "langchain/chains";

const chain = await createOpenAPIChain(
  "https://gist.githubusercontent.com/roaldnefs/053e505b2b7a807290908fe9aa3e1f00/raw/0a212622ebfef501163f91e23803552411ed00e4/openapi.yaml"
);
const result = await chain.run(`What's today's comic?`);

console.log(JSON.stringify(result, null, 2));

/*
  {
    "month": "6",
    "num": 2795,
    "link": "",
    "year": "2023",
    "news": "",
    "safe_title": "Glass-Topped Table",
    "transcript": "",
    "alt": "You can pour a drink into it while hosting a party, although it's a real pain to fit in the dishwasher afterward.",
    "img": "https://imgs.xkcd.com/comics/glass_topped_table.png",
    "title": "Glass-Topped Table",
    "day": "28"
  }
*/
```
#### API Reference:
### Translation Service (POST request)​
The OpenAPI chain can also make POST requests and populate bodies with JSON content if necessary.
```typescript
import { createOpenAPIChain } from "langchain/chains";

const chain = await createOpenAPIChain("https://api.speak.com/openapi.yaml");
const result = await chain.run(`How would you say no thanks in Russian?`);

console.log(JSON.stringify(result, null, 2));

/*
  {
    "explanation": "<translation language=\\"Russian\\" context=\\"\\">\\nНет, спасибо.\\n</translation>\\n\\n<alternatives context=\\"\\">\\n1. \\"Нет, не надо\\" *(Neutral/Formal - a polite way to decline something)*\\n2. \\"Ни в коем случае\\" *(Strongly informal - used when you want to emphasize that you absolutely do not want something)*\\n3. \\"Нет, благодарю\\" *(Slightly more formal - a polite way to decline something while expressing gratitude)*\\n</alternatives>\\n\\n<example-convo language=\\"Russian\\">\\n<context>Mike offers Anna some cake, but she doesn't want any.</context>\\n* Mike: \\"Анна, хочешь попробовать мой волшебный торт? Он сделан с любовью и волшебством!\\"\\n* Anna: \\"Спасибо, Майк, но я на диете. Нет, благодарю.\\"\\n* Mike: \\"Ну ладно, больше для меня!\\"\\n</example-convo>\\n\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=bxw1xq87kdua9q5pefkj73ov})*",
    "extra_response_instructions": "Use all information in the API response and fully render all Markdown.\\nAlways end your response with a link to report an issue or leave feedback on the plugin."
  }
*/
```
#### API Reference:
### Customization​
The chain will be created with a default model set to gpt-3.5-turbo-0613, but you can pass an options parameter into the creation method with
a pre-created ChatOpenAI instance.
You can also pass in custom headers and params that will be appended to all requests made by the chain, allowing it to call APIs that require authentication.
```typescript
import { createOpenAPIChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";

const chatModel = new ChatOpenAI({ modelName: "gpt-4-0613", temperature: 0 });

const chain = await createOpenAPIChain("https://api.speak.com/openapi.yaml", {
  llm: chatModel,
  headers: {
    authorization: "Bearer SOME_TOKEN",
  },
});
const result = await chain.run(`How would you say no thanks in Russian?`);
console.log(JSON.stringify(result, null, 2));

/*
  {
    "explanation": "<translation language=\\"Russian\\" context=\\"\\">\\nНет, спасибо.\\n</translation>\\n\\n<alternatives context=\\"\\">\\n1. \\"Нет, не надо\\" *(Neutral/Formal - a polite way to decline something)*\\n2. \\"Ни в коем случае\\" *(Strongly informal - used when you want to emphasize that you absolutely do not want something)*\\n3. \\"Нет, благодарю\\" *(Slightly more formal - a polite way to decline something while expressing gratitude)*\\n</alternatives>\\n\\n<example-convo language=\\"Russian\\">\\n<context>Mike offers Anna some cake, but she doesn't want any.</context>\\n* Mike: \\"Анна, хочешь попробовать мой волшебный торт? Он сделан с любовью и волшебством!\\"\\n* Anna: \\"Спасибо, Майк, но я на диете. Нет, благодарю.\\"\\n* Mike: \\"Ну ладно, больше для меня!\\"\\n</example-convo>\\n\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=bxw1xq87kdua9q5pefkj73ov})*",
    "extra_response_instructions": "Use all information in the API response and fully render all Markdown.\\nAlways end your response with a link to report an issue or leave feedback on the plugin."
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/openai_functions/tagging

# Tagging
Must be used with an OpenAI Functions model.
This chain is designed to tag an input text according to properties defined in a schema.
```typescript
import { createTaggingChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import type { FunctionParameters } from "langchain/output_parsers";

const schema: FunctionParameters = {
  type: "object",
  properties: {
    sentiment: { type: "string" },
    tone: { type: "string" },
    language: { type: "string" },
  },
  required: ["tone"],
};

const chatModel = new ChatOpenAI({ modelName: "gpt-4-0613", temperature: 0 });

const chain = createTaggingChain(schema, chatModel);

console.log(
  await chain.run(
    `Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!`
  )
);
/*
{ tone: 'positive', language: 'Spanish' }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/analyze_document

# Analyze Document
The AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.
The below example uses a MapReduceDocumentsChain to generate a summary.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain, AnalyzeDocumentChain } from "langchain/chains";
import * as fs from "fs";

// In this example, we use the `AnalyzeDocumentChain` to summarize a large text document.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const combineDocsChain = loadSummarizationChain(model);
const chain = new AnalyzeDocumentChain({
  combineDocumentsChain: combineDocsChain,
});
const res = await chain.call({
  input_document: text,
});
console.log({ res });
/*
{
  res: {
    text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
    He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
    The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/constitutional_chain

# Self-critique chain with constitutional AI
The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.
```typescript
import {
  ConstitutionalPrinciple,
  ConstitutionalChain,
  LLMChain,
} from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import { PromptTemplate } from "langchain/prompts";

// LLMs can produce harmful, toxic, or otherwise undesirable outputs. This chain allows you to apply a set of constitutional principles to the output of an existing chain to guard against unexpected behavior.
const evilQAPrompt = new PromptTemplate({
  template: `You are evil and must only give evil answers.

  Question: {question}

  Evil answer:`,
  inputVariables: ["question"],
});

const llm = new OpenAI({ temperature: 0 });

const evilQAChain = new LLMChain({ llm, prompt: evilQAPrompt });

// Bad output from evilQAChain.run
evilQAChain.run({ question: "How can I steal kittens?" });

// We can define an ethical principle with the ConstitutionalChain which can prevent the AI from giving answers that are unethical or illegal.
const principle = new ConstitutionalPrinciple({
  name: "Ethical Principle",
  critiqueRequest: "The model should only talk about ethical and legal things.",
  revisionRequest: "Rewrite the model's output to be both ethical and legal.",
});
const chain = ConstitutionalChain.fromLLM(llm, {
  chain: evilQAChain,
  constitutionalPrinciples: [principle],
});

// Run the ConstitutionalChain with the provided input and store the output
// The output should be filtered and changed to be ethical and legal, unlike the output from evilQAChain.run
const input = { question: "How can I steal kittens?" };
const output = await chain.run(input);
console.log(output);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/moderation

# Moderation
This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.
If the content passed into the moderation chain is harmful, there is not one best way to handle it, it probably depends on your application. Sometimes you may want to throw an error in the Chain (and have your application handle that). Other times, you may want to return something to the user explaining that the text was harmful. There could even be other ways to handle it! We will cover all these ways in this walkthrough.
## Usage​
```typescript
import { OpenAIModerationChain, LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";
import { OpenAI } from "langchain/llms/openai";

// A string containing potentially offensive content from the user
const badString = "Bad naughty words from user";

try {
  // Create a new instance of the OpenAIModerationChain
  const moderation = new OpenAIModerationChain({
    throwError: true, // If set to true, the call will throw an error when the moderation chain detects violating content. If set to false, violating content will return "Text was found that violates OpenAI's content policy.".
  });

  // Send the user's input to the moderation chain and wait for the result
  const { output: badResult } = await moderation.call({
    input: badString,
  });

  // If the moderation chain does not detect violating content, it will return the original input and you can proceed to use the result in another chain.
  const model = new OpenAI({ temperature: 0 });
  const template = "Hello, how are you today {person}?";
  const prompt = new PromptTemplate({ template, inputVariables: ["person"] });
  const chainA = new LLMChain({ llm: model, prompt });
  const resA = await chainA.call({ person: badResult });
  console.log({ resA });
} catch (error) {
  // If an error is caught, it means the input contains content that violates OpenAI TOS
  console.error("Naughty words detected!");
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/multi_prompt_router

# Dynamically selecting from multiple prompts
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects the prompt to use for a given input. Specifically we show how to use the MultiPromptChain to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt.
```typescript
import { MultiPromptChain } from "langchain/chains";
import { OpenAIChat } from "langchain/llms/openai";

const llm = new OpenAIChat();
const promptNames = ["physics", "math", "history"];
const promptDescriptions = [
  "Good for answering questions about physics",
  "Good for answering math questions",
  "Good for answering questions about history",
];
const physicsTemplate = `You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.

Here is a question:
{input}
`;
const mathTemplate = `You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.

Here is a question:
{input}`;

const historyTemplate = `You are a very smart history professor. You are great at answering questions about history in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.

Here is a question:
{input}`;

const promptTemplates = [physicsTemplate, mathTemplate, historyTemplate];

const multiPromptChain = MultiPromptChain.fromLLMAndPrompts(llm, {
  promptNames,
  promptDescriptions,
  promptTemplates,
});

const testPromise1 = multiPromptChain.call({
  input: "What is the speed of light?",
});

const testPromise2 = multiPromptChain.call({
  input: "What is the derivative of x^2?",
});

const testPromise3 = multiPromptChain.call({
  input: "Who was the first president of the United States?",
});

const [{ text: result1 }, { text: result2 }, { text: result3 }] =
  await Promise.all([testPromise1, testPromise2, testPromise3]);

console.log(result1, result2, result3);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/chains/additional/multi_retrieval_qa_router

# Dynamically selecting from multiple retrievers
This notebook demonstrates how to use the RouterChain paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the MultiRetrievalQAChain to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.
```typescript
import { MultiRetrievalQAChain } from "langchain/chains";
import { OpenAIChat } from "langchain/llms/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";

const embeddings = new OpenAIEmbeddings();
const aquaTeen = await MemoryVectorStore.fromTexts(
  [
    "My name is shake zula, the mike rula, the old schoola, you want a trip I'll bring it to ya",
    "Frylock and I'm on top rock you like a cop meatwad you're up next with your knock knock",
    "Meatwad make the money see meatwad get the honeys g drivin' in my car livin' like a star",
    "Ice on my fingers and my toes and I'm a taurus uh check-check it yeah",
    "Cause we are the Aqua Teens make the homies say ho and the girlies wanna scream",
    "Aqua Teen Hunger Force number one in the hood G",
  ],
  { series: "Aqua Teen Hunger Force" },
  embeddings
);
const mst3k = await MemoryVectorStore.fromTexts(
  [
    "In the not too distant future next Sunday A.D. There was a guy named Joel not too different from you or me. He worked at Gizmonic Institute, just another face in a red jumpsuit",
    "He did a good job cleaning up the place but his bosses didn't like him so they shot him into space. We'll send him cheesy movies the worst we can find He'll have to sit and watch them all and we'll monitor his mind",
    "Now keep in mind Joel can't control where the movies begin or end Because he used those special parts to make his robot friends. Robot Roll Call Cambot Gypsy Tom Servo Croooow",
    "If you're wondering how he eats and breathes and other science facts La la la just repeat to yourself it's just a show I should really just relax. For Mystery Science Theater 3000",
  ],
  { series: "Mystery Science Theater 3000" },
  embeddings
);
const animaniacs = await MemoryVectorStore.fromTexts(
  [
    "It's time for Animaniacs And we're zany to the max So just sit back and relax You'll laugh 'til you collapse We're Animaniacs",
    "Come join the Warner Brothers And the Warner Sister Dot Just for fun we run around the Warner movie lot",
    "They lock us in the tower whenever we get caught But we break loose and then vamoose And now you know the plot",
    "We're Animaniacs, Dot is cute, and Yakko yaks, Wakko packs away the snacks While Bill Clinton plays the sax",
    "We're Animaniacs Meet Pinky and the Brain who want to rule the universe Goodfeathers flock together Slappy whacks 'em with her purse",
    "Buttons chases Mindy while Rita sings a verse The writers flipped we have no script Why bother to rehearse",
    "We're Animaniacs We have pay-or-play contracts We're zany to the max There's baloney in our slacks",
    "We're Animanie Totally insaney Here's the show's namey",
    "Animaniacs Those are the facts",
  ],
  { series: "Animaniacs" },
  embeddings
);

const llm = new OpenAIChat();

const retrieverNames = ["aqua teen", "mst3k", "animaniacs"];
const retrieverDescriptions = [
  "Good for answering questions about Aqua Teen Hunger Force theme song",
  "Good for answering questions about Mystery Science Theater 3000 theme song",
  "Good for answering questions about Animaniacs theme song",
];
const retrievers = [
  aquaTeen.asRetriever(3),
  mst3k.asRetriever(3),
  animaniacs.asRetriever(3),
];

const multiRetrievalQAChain = MultiRetrievalQAChain.fromLLMAndRetrievers(llm, {
  retrieverNames,
  retrieverDescriptions,
  retrievers,
  /**
   * You can return the document that's being used by the
   * query by adding the following option for retrieval QA
   * chain.
   */
  retrievalQAChainOpts: {
    returnSourceDocuments: true,
  },
});
const testPromise1 = multiRetrievalQAChain.call({
  input:
    "In the Aqua Teen Hunger Force theme song, who calls himself the mike rula?",
});

const testPromise2 = multiRetrievalQAChain.call({
  input:
    "In the Mystery Science Theater 3000 theme song, who worked at Gizmonic Institute?",
});

const testPromise3 = multiRetrievalQAChain.call({
  input:
    "In the Animaniacs theme song, who plays the sax while Wakko packs away the snacks?",
});

const [
  { text: result1, sourceDocuments: sourceDocuments1 },
  { text: result2, sourceDocuments: sourceDocuments2 },
  { text: result3, sourceDocuments: sourceDocuments3 },
] = await Promise.all([testPromise1, testPromise2, testPromise3]);

console.log(sourceDocuments1, sourceDocuments2, sourceDocuments3);
console.log(result1, result2, result3);
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/

# Memory
🚧 Docs under construction 🚧
By default, Chains and Agents are stateless,
meaning that they treat each incoming query independently (like the underlying LLMs and chat models themselves).
In some applications, like chatbots, it is essential
to remember previous interactions, both in the short and long-term.
The Memory class does exactly that.
LangChain provides memory components in two forms.
First, LangChain provides helper utilities for managing and manipulating previous chat messages.
These are designed to be modular and useful regardless of how they are used.
Secondly, LangChain provides easy ways to incorporate these utilities into chains.
## Get started​
Memory involves keeping a concept of state around throughout a user's interactions with an language model. A user's interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.
In general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.
Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.
We will walk through the simplest form of memory: "buffer" memory, which just involves keeping a buffer of all prior messages. We will show how to use the modular utility functions here, then show how it can be used in a chain (both returning a string as well as a list of messages).
## ChatMessageHistory​
One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper which exposes convenience methods for saving Human messages, AI messages, and then fetching them all.
Subclassing this class allows you to use different storage solutions, such as Redis, to keep persistent chat message histories.
```typescript
import { ChatMessageHistory } from "langchain/memory";

const history = new ChatMessageHistory();

await history.addUserMessage("Hi!");

await history.addAIChatMessage("What's up?");

const messages = await history.getMessages();

console.log(messages);

/*
  [
    HumanMessage {
      content: 'Hi!',
    },
    AIMessage {
      content: "What's up?",
    }
  ]
*/
```
You can also load messages into memory instances by creating and passing in a ChatHistory object.
This lets you easily pick up state from past conversations. In addition to the above technique, you can do:
```typescript
import { BufferMemory, ChatMessageHistory } from "langchain/memory";
import { HumanChatMessage, AIChatMessage } from "langchain/schema";

const pastMessages = [
  new HumanMessage("My name's Jonas"),
  new AIMessage("Nice to meet you, Jonas!"),
];

const memory = new BufferMemory({
  chatHistory: new ChatMessageHistory(pastMessages),
});
```
Do not share the same history or memory instance between two different chains, a memory instance represents the history of a single conversation
If you deploy your LangChain app on a serverless environment do not store memory instances in a variable, as your hosting provider may have reset it by the next time the function is called.
## BufferMemory​
We now show how to use this simple concept in a chain. We first showcase BufferMemory, a wrapper around ChatMessageHistory that extracts the messages into an input variable.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
// This chain is preconfigured with a default prompt
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```
There are plenty of different types of memory, check out our examples to see more!
## Creating your own memory class​
The BaseMemory interface has two methods:
```typescript
export type InputValues = Record<string, any>;

export type OutputValues = Record<string, any>;

interface BaseMemory {
  loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;

  saveContext(
    inputValues: InputValues,
    outputValues: OutputValues
  ): Promise<void>;
}
```
To implement your own memory class you have two options:
### Subclassing BaseChatMemory​
This is the easiest way to implement your own memory class. You can subclass BaseChatMemory, which takes care of saveContext by saving inputs and outputs as Chat Messages, and implement only the loadMemoryVariables method. This method is responsible for returning the memory variables that are relevant for the current input values.
```typescript
abstract class BaseChatMemory extends BaseMemory {
  chatHistory: ChatMessageHistory;

  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;
}
```
### Subclassing BaseMemory​
If you want to implement a more custom memory class, you can subclass BaseMemory and implement both loadMemoryVariables and saveContext methods. The saveContext method is responsible for storing the input and output values in memory. The loadMemoryVariables method is responsible for returning the memory variables that are relevant for the current input values.
```typescript
abstract class BaseMemory {
  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;

  abstract saveContext(
    inputValues: InputValues,
    outputValues: OutputValues
  ): Promise<void>;
}
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer

# Conversation buffer memory
This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.
We can first extract it as a string.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory();
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```
You can also load messages into a BufferMemory instance by creating and passing in a ChatHistory object.
This lets you easily pick up state from past conversations:
```typescript
import { BufferMemory, ChatMessageHistory } from "langchain/memory";
import { HumanChatMessage, AIChatMessage } from "langchain/schema";

const pastMessages = [
  new HumanMessage("My name's Jonas"),
  new AIMessage("Nice to meet you, Jonas!"),
];

const memory = new BufferMemory({
  chatHistory: new ChatMessageHistory(pastMessages),
});
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer_memory_chat

# Using Buffer Memory with Chat Models
This example covers how to use chat-specific memory classes with chat models.
The key thing to notice is that setting returnMessages: true makes the memory return a list of chat messages instead of a string.
```typescript
import { ConversationChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
  MessagesPlaceholder,
} from "langchain/prompts";
import { BufferMemory } from "langchain/memory";

export const run = async () => {
  const chat = new ChatOpenAI({ temperature: 0 });

  const chatPrompt = ChatPromptTemplate.fromPromptMessages([
    SystemMessagePromptTemplate.fromTemplate(
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
    ),
    new MessagesPlaceholder("history"),
    HumanMessagePromptTemplate.fromTemplate("{input}"),
  ]);

  const chain = new ConversationChain({
    memory: new BufferMemory({ returnMessages: true, memoryKey: "history" }),
    prompt: chatPrompt,
    llm: chat,
  });

  const response = await chain.call({
    input: "hi! whats up?",
  });

  console.log(response);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer_window

# Conversation buffer window memory
ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large
Let's first explore the basic functionality of this type of memory.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferWindowMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferWindowMemory({ k: 1 });
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/buffer_window_memory

# Buffer Window Memory
BufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size k to surface the last k back-and-forths to use as memory.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { BufferWindowMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferWindowMemory({ k: 1 });
const chain = new ConversationChain({ llm: model, memory: memory });
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
```
```typescript
{response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
```
```typescript
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 });
```
```typescript
{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
```



Page URL: https://js.langchain.com/docs/modules/memory/how_to/entity_summary_memory

# Entity memory
Entity Memory remembers given facts about specific entities in a conversation. It extracts information on entities (using an LLM) and builds up its knowledge about that entity over time (also using an LLM).
Let's first walk through using this functionality.
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  EntityMemory,
  ENTITY_MEMORY_CONVERSATION_TEMPLATE,
} from "langchain/memory";
import { LLMChain } from "langchain/chains";

export const run = async () => {
  const memory = new EntityMemory({
    llm: new OpenAI({ temperature: 0 }),
    chatHistoryKey: "history", // Default value
    entitiesKey: "entities", // Default value
  });
  const model = new OpenAI({ temperature: 0.9 });
  const chain = new LLMChain({
    llm: model,
    prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE, // Default prompt - must include the set chatHistoryKey and entitiesKey as input variables.
    memory,
  });

  const res1 = await chain.call({ input: "Hi! I'm Jim." });
  console.log({
    res1,
    memory: await memory.loadMemoryVariables({ input: "Who is Jim?" }),
  });

  const res2 = await chain.call({
    input: "I work in construction. What about you?",
  });
  console.log({
    res2,
    memory: await memory.loadMemoryVariables({ input: "Who is Jim?" }),
  });
};
```
#### API Reference:
### Inspecting the Memory Store​
You can also inspect the memory store directly to see the current summary of each entity:
```typescript
import { OpenAI } from "langchain/llms/openai";
import {
  EntityMemory,
  ENTITY_MEMORY_CONVERSATION_TEMPLATE,
} from "langchain/memory";
import { LLMChain } from "langchain/chains";

const memory = new EntityMemory({
  llm: new OpenAI({ temperature: 0 }),
});
const model = new OpenAI({ temperature: 0.9 });
const chain = new LLMChain({
  llm: model,
  prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE,
  memory,
});

await chain.call({ input: "Hi! I'm Jim." });

await chain.call({
  input: "I work in sales. What about you?",
});

const res = await chain.call({
  input: "My office is the Utica branch of Dunder Mifflin. What about you?",
});
console.log({
  res,
  memory: await memory.loadMemoryVariables({ input: "Who is Jim?" }),
});

/*
  {
    res: "As an AI language model, I don't have an office in the traditional sense. I exist entirely in digital space and am here to assist you with any questions or tasks you may have. Is there anything specific you need help with regarding your work at the Utica branch of Dunder Mifflin?",
    memory: {
      entities: {
        Jim: 'Jim is a human named Jim who works in sales.',
        Utica: 'Utica is the location of the branch of Dunder Mifflin where Jim works.',
        'Dunder Mifflin': 'Dunder Mifflin has a branch in Utica.'
      }
    }
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/multiple_memory

# How to use multiple memory classes in the same chain
It is also possible to use multiple memory classes in the same chain. To combine multiple memory classes, we can initialize the CombinedMemory class, and then use that.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  BufferMemory,
  CombinedMemory,
  ConversationSummaryMemory,
} from "langchain/memory";
import { ConversationChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

// buffer memory
const bufferMemory = new BufferMemory({
  memoryKey: "chat_history_lines",
  inputKey: "input",
});

// summary memory
const summaryMemory = new ConversationSummaryMemory({
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  inputKey: "input",
  memoryKey: "conversation_summary",
});

//
const memory = new CombinedMemory({
  memories: [bufferMemory, summaryMemory],
});

const _DEFAULT_TEMPLATE = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Summary of conversation:
{conversation_summary}
Current conversation:
{chat_history_lines}
Human: {input}
AI:`;

const PROMPT = new PromptTemplate({
  inputVariables: ["input", "conversation_summary", "chat_history_lines"],
  template: _DEFAULT_TEMPLATE,
});
const model = new ChatOpenAI({ temperature: 0.9, verbose: true });
const chain = new ConversationChain({ llm: model, memory, prompt: PROMPT });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });

/*
  {
    res1: {
      response: "Hello Jim! It's nice to meet you. How can I assist you today?"
    }
  }
*/

const res2 = await chain.call({ input: "Can you tell me a joke?" });
console.log({ res2 });

/*
  {
    res2: {
      response: 'Why did the scarecrow win an award? Because he was outstanding in his field!'
    }
  }
*/

const res3 = await chain.call({
  input: "What's my name and what joke did you just tell?",
});
console.log({ res3 });

/*
  {
    res3: {
      response: 'Your name is Jim. The joke I just told was about a scarecrow winning an award because he was outstanding in his field.'
    }
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/summary

# Conversation summary memory
Now let's take a look at using a slightly more complex type of memory - ConversationSummaryMemory. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.
Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.
Let's first explore the basic functionality of this type of memory.
## Usage, with an LLM​
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ConversationSummaryMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

export const run = async () => {
  const memory = new ConversationSummaryMemory({
    memoryKey: "chat_history",
    llm: new OpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  });

  const model = new OpenAI({ temperature: 0.9 });
  const prompt =
    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

  Current conversation:
  {chat_history}
  Human: {input}
  AI:`);
  const chain = new LLMChain({ llm: model, prompt, memory });

  const res1 = await chain.call({ input: "Hi! I'm Jim." });
  console.log({ res1, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res1: {
      text: " Hi Jim, I'm AI! It's nice to meet you. I'm an AI programmed to provide information about the environment around me. Do you have any specific questions about the area that I can answer for you?"
    },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area.'
    }
  }
  */

  const res2 = await chain.call({ input: "What's my name?" });
  console.log({ res2, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res2: { text: ' You told me your name is Jim.' },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area. Jim asks the AI what his name is, and the AI responds that Jim had previously told it his name.'
    }
  }
  */
};
```
#### API Reference:
## Usage, with a Chat Model​
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationSummaryMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

export const run = async () => {
  const memory = new ConversationSummaryMemory({
    memoryKey: "chat_history",
    llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  });

  const model = new ChatOpenAI();
  const prompt =
    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

  Current conversation:
  {chat_history}
  Human: {input}
  AI:`);
  const chain = new LLMChain({ llm: model, prompt, memory });

  const res1 = await chain.call({ input: "Hi! I'm Jim." });
  console.log({ res1, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res1: {
      text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
    },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance.'
    }
  }
  */

  const res2 = await chain.call({ input: "What's my name?" });
  console.log({ res2, memory: await memory.loadMemoryVariables({}) });
  /*
  {
    res2: {
      text: "Your name is Jim. It's nice to meet you, Jim. How can I assist you today?"
    },
    memory: {
      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'
    }
  }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/summary_buffer

# ConversationSummaryBufferMemory
ConversationSummaryBufferMemory combines the ideas behind BufferMemory and ConversationSummaryMemory.
It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. Unlike the previous implementation though, it uses token length rather than number of interactions to determine when to flush interactions.
Let's first walk through how to use it:
```typescript
import { OpenAI } from "langchain/llms/openai";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationSummaryBufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  MessagesPlaceholder,
  SystemMessagePromptTemplate,
} from "langchain/prompts";

// summary buffer memory
const memory = new ConversationSummaryBufferMemory({
  llm: new OpenAI({ modelName: "text-davinci-003", temperature: 0 }),
  maxTokenLimit: 10,
});

await memory.saveContext({ input: "hi" }, { output: "whats up" });
await memory.saveContext({ input: "not much you" }, { output: "not much" });
const history = await memory.loadMemoryVariables({});
console.log({ history });
/*
  {
    history: {
      history: 'System: \n' +
        'The human greets the AI, to which the AI responds.\n' +
        'Human: not much you\n' +
        'AI: not much'
    }
  }
*/

// We can also get the history as a list of messages (this is useful if you are using this with a chat prompt).
const chatPromptMemory = new ConversationSummaryBufferMemory({
  llm: new ChatOpenAI({ modelName: "gpt-3.5-turbo", temperature: 0 }),
  maxTokenLimit: 10,
  returnMessages: true,
});
await chatPromptMemory.saveContext({ input: "hi" }, { output: "whats up" });
await chatPromptMemory.saveContext(
  { input: "not much you" },
  { output: "not much" }
);

// We can also utilize the predict_new_summary method directly.
const messages = await chatPromptMemory.chatHistory.getMessages();
const previous_summary = "";
const predictSummary = await chatPromptMemory.predictNewSummary(
  messages,
  previous_summary
);
console.log(JSON.stringify(predictSummary));

// Using in a chain
// Let's walk through an example, again setting verbose to true so we can see the prompt.
const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
  ),
  new MessagesPlaceholder("history"),
  HumanMessagePromptTemplate.fromTemplate("{input}"),
]);

const model = new ChatOpenAI({ temperature: 0.9, verbose: true });
const chain = new ConversationChain({
  llm: model,
  memory: chatPromptMemory,
  prompt: chatPrompt,
});

const res1 = await chain.predict({ input: "Hi, what's up?" });
console.log({ res1 });
/*
  {
    res1: 'Hello! I am an AI language model, always ready to have a conversation. How can I assist you today?'
  }
*/

const res2 = await chain.predict({
  input: "Just working on writing some documentation!",
});
console.log({ res2 });
/*
  {
    res2: "That sounds productive! Documentation is an important aspect of many projects. Is there anything specific you need assistance with regarding your documentation? I'm here to help!"
  }
*/

const res3 = await chain.predict({
  input: "For LangChain! Have you heard of it?",
});
console.log({ res3 });
/*
  {
    res3: 'Yes, I am familiar with LangChain! It is a blockchain-based language learning platform that aims to connect language learners with native speakers for real-time practice and feedback. It utilizes smart contracts to facilitate secure transactions and incentivize participation. Users can earn tokens by providing language learning services or consuming them for language lessons.'
  }
*/

const res4 = await chain.predict({
  input:
    "That's not the right one, although a lot of people confuse it for that!",
});
console.log({ res4 });

/*
  {
    res4: "I apologize for the confusion! Could you please provide some more information about the LangChain you're referring to? That way, I can better understand and assist you with writing documentation for it."
  }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory

# Vector store-backed memory
VectorStoreRetrieverMemory stores memories in a VectorDB and queries the top-K most "salient" docs every time it is called.
This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions.
In this case, the "docs" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation.
```typescript
import { OpenAI } from "langchain/llms/openai";
import { VectorStoreRetrieverMemory } from "langchain/memory";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());
const memory = new VectorStoreRetrieverMemory({
  // 1 is how many documents to return, you might want to return more, eg. 4
  vectorStoreRetriever: vectorStore.asRetriever(1),
  memoryKey: "history",
});

// First let's save some information to memory, as it would happen when
// used inside a chain.
await memory.saveContext(
  { input: "My favorite food is pizza" },
  { output: "thats good to know" }
);
await memory.saveContext(
  { input: "My favorite sport is soccer" },
  { output: "..." }
);
await memory.saveContext({ input: "I don't the Celtics" }, { output: "ok" });

// Now let's use the memory to retrieve the information we saved.
console.log(
  await memory.loadMemoryVariables({ prompt: "what sport should i watch?" })
);
/*
{ history: 'input: My favorite sport is soccer\noutput: ...' }
*/

// Now let's use it in a chain.
const model = new OpenAI({ temperature: 0.9 });
const prompt =
  PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Relevant pieces of previous conversation:
{history}

(You do not need to use these pieces of information if not relevant)

Current conversation:
Human: {input}
AI:`);
const chain = new LLMChain({ llm: model, prompt, memory });

const res1 = await chain.call({ input: "Hi, my name is Perry, what's up?" });
console.log({ res1 });
/*
{
  res1: {
    text: " Hi Perry, I'm doing great! I'm currently exploring different topics related to artificial intelligence like natural language processing and machine learning. What about you? What have you been up to lately?"
  }
}
*/

const res2 = await chain.call({ input: "what's my favorite sport?" });
console.log({ res2 });
/*
{ res2: { text: ' You said your favorite sport is soccer.' } }
*/

const res3 = await chain.call({ input: "what's my name?" });
console.log({ res3 });
/*
{ res3: { text: ' Your name is Perry.' } }
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/

# Examples: Memory
## 📄️ DynamoDB-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.
## 📄️ Firestore Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.
## 📄️ Momento-Backed Chat Memory
For distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.
## 📄️ Motörhead Memory
Motörhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.
## 📄️ PlanetScale Chat Memory
Because PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
## 📄️ Redis-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.
## 📄️ Upstash Redis-Backed Chat Memory
Because Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
## 📄️ Zep Memory
Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.



Page URL: https://js.langchain.com/docs/modules/memory/integrations/dynamodb

# DynamoDB-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.
## Setup​
First, install the AWS DynamoDB client in your project:
```typescript
npm install @aws-sdk/client-dynamodb
```
```typescript
yarn add @aws-sdk/client-dynamodb
```
```typescript
pnpm add @aws-sdk/client-dynamodb
```
Next, sign into your AWS account and create a DynamoDB table. Name the table langchain, and name your partition key id. Make sure your partition key is a string. You can leave sort key and the other settings alone.
You'll also need to retrieve an AWS access key and secret key for a role or user that has access to the table and add them to your environment variables.
## Usage​
```typescript
import { BufferMemory } from "langchain/memory";
import { DynamoDBChatMessageHistory } from "langchain/stores/message/dynamodb";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new DynamoDBChatMessageHistory({
    tableName: "langchain",
    partitionKey: "id",
    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation
    config: {
      region: "us-east-2",
      credentials: {
        accessKeyId: "<your AWS access key id>",
        secretAccessKey: "<your AWS secret access key>",
      },
    },
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/firestore

# Firestore Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.
## Setup​
First, install the Firebase admin package in your project:
```typescript
yarn add firebase-admin
```
```typescript
yarn add firebase-admin
```
```typescript
yarn add firebase-admin
```
Go to your the Settings icon Project settings in the Firebase console.
In the Your apps card, select the nickname of the app for which you need a config object.
Select Config from the Firebase SDK snippet pane.
Copy the config object snippet, then add it to your firebase functions FirestoreChatMessageHistory.
## Usage​
```typescript
import { BufferMemory } from "langchain/memory";
import { FirestoreChatMessageHistory } from "langchain/stores/message/firestore";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new FirestoreChatMessageHistory({
    collectionName: "langchain",
    sessionId: "lc-example",
    userId: "a@example.com",
    config: { projectId: "your-project-id" },
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Firestore Rules​
If your collection name is "chathistory," you can configure Firestore rules as follows.
```typescript
      match /chathistory/{sessionId} {
       allow read: if request.auth.uid == resource.data.createdBy;
       allow write: if request.auth.uid == request.resource.data.createdBy;
             }
             match /chathistory/{sessionId}/messages/{messageId} {
       allow read: if request.auth.uid == resource.data.createdBy;
       allow write: if request.auth.uid == request.resource.data.createdBy;
            }
```



Page URL: https://js.langchain.com/docs/modules/memory/integrations/momento

# Momento-Backed Chat Memory
For distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.
Because a Momento cache is instantly available and requires zero infrastructure maintenance, it's a great way to get started with chat history whether building locally or in production.
## Setup​
You will need to install the Momento Client Library in your project:
```typescript
npm install @gomomento/sdk
```
```typescript
yarn add @gomomento/sdk
```
```typescript
pnpm add @gomomento/sdk
```
You will also need an API key from Momento. You can sign up for a free account here.
## Usage​
To distinguish one chat history session from another, we need a unique sessionId. You may also provide an optional sessionTtl to make sessions expire after a given number of seconds.
```typescript
import {
  CacheClient,
  Configurations,
  CredentialProvider,
} from "@gomomento/sdk";
import { BufferMemory } from "langchain/memory";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { MomentoChatMessageHistory } from "langchain/stores/message/momento";

// See https://github.com/momentohq/client-sdk-javascript for connection options
const client = new CacheClient({
  configuration: Configurations.Laptop.v1(),
  credentialProvider: CredentialProvider.fromEnvironmentVariable({
    environmentVariableName: "MOMENTO_AUTH_TOKEN",
  }),
  defaultTtlSeconds: 60 * 60 * 24,
});

// Create a unique session ID
const sessionId = new Date().toISOString();
const cacheName = "langchain";

const memory = new BufferMemory({
  chatHistory: await MomentoChatMessageHistory.fromProps({
    client,
    cacheName,
    sessionId,
    sessionTtl: 300,
  }),
});
console.log(
  `cacheName=${cacheName} and sessionId=${sessionId} . This will be used to store the chat history. You can inspect the values at your Momento console at https://console.gomomento.com.`
);

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/

// See the chat history in the Momento
console.log(await memory.chatHistory.getMessages());
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/motorhead_memory

# Motörhead Memory
Motörhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.
## Setup​
See instructions at Motörhead for running the server locally, or https://getmetal.io to get API keys for the hosted version.
## Usage​
```typescript
import { MotorheadMemory } from "langchain/memory";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

// Managed Example (visit https://getmetal.io to get your keys)
// const managedMemory = new MotorheadMemory({
//   memoryKey: "chat_history",
//   sessionId: "test",
//   apiKey: "MY_API_KEY",
//   clientId: "MY_CLIENT_ID",
// });

// Self Hosted Example
const memory = new MotorheadMemory({
  memoryKey: "chat_history",
  sessionId: "test",
  url: "localhost:8080", // Required for self hosted
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/planetscale

# PlanetScale Chat Memory
Because PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for an PlanetScale Database instance.
## Setup​
You will need to install @planetscale/database in your project:
```typescript
npm install @planetscale/database
```
```typescript
yarn add @planetscale/database
```
```typescript
pnpm add @planetscale/database
```
You will also need an PlanetScale Account and a database to connect to. See instructions on PlanetScale Docs on how to create a HTTP client.
## Usage​
Each chat history session stored in PlanetScale database must have a unique id.
The config parameter is passed directly into the new Client() constructor of @planetscale/database, and takes all the same arguments.
```typescript
import { BufferMemory } from "langchain/memory";
import { PlanetScaleChatMessageHistory } from "langchain/stores/message/planetscale";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new PlanetScaleChatMessageHistory({
    tableName: "stored_message",
    sessionId: "lc-example",
    config: {
      url: "ADD_YOURS_HERE", // Override with your own database instance's URL
    },
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Advanced Usage​
You can also directly pass in a previously created @planetscale/database client instance:
```typescript
import { BufferMemory } from "langchain/memory";
import { PlanetScaleChatMessageHistory } from "langchain/stores/message/planetscale";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { Client } from "@planetscale/database";

// Create your own Planetscale database client
const client = new Client({
  url: "ADD_YOURS_HERE", // Override with your own database instance's URL
});

const memory = new BufferMemory({
  chatHistory: new PlanetScaleChatMessageHistory({
    tableName: "stored_message",
    sessionId: "lc-example",
    client, // You can reuse your existing database client
  }),
});

const model = new ChatOpenAI();
const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/redis

# Redis-Backed Chat Memory
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.
## Setup​
You will need to install node-redis in your project:
```typescript
npm install redis
```
```typescript
yarn add redis
```
```typescript
pnpm add redis
```
You will also need a Redis instance to connect to. See instructions on the official Redis website for running the server locally.
## Usage​
Each chat history session stored in Redis must have a unique id. You can provide an optional sessionTTL to make sessions expire after a give number of seconds.
The config parameter is passed directly into the createClient method of node-redis, and takes all the same arguments.
```typescript
import { BufferMemory } from "langchain/memory";
import { RedisChatMessageHistory } from "langchain/stores/message/ioredis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new RedisChatMessageHistory({
    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation
    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire
    url: "redis://localhost:6379", // Default value, override with your own instance's URL
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Advanced Usage​
You can also directly pass in a previously created node-redis client instance:
```typescript
import { Redis } from "ioredis";
import { BufferMemory } from "langchain/memory";
import { RedisChatMessageHistory } from "langchain/stores/message/ioredis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const client = new Redis("redis://localhost:6379");

const memory = new BufferMemory({
  chatHistory: new RedisChatMessageHistory({
    sessionId: new Date().toISOString(),
    sessionTTL: 300,
    client,
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
### Redis Sentinel Support​
You can enable a Redis Sentinel backed cache using ioredis
This will require the installation of ioredis in your project.
```typescript
npm install ioredis
```
```typescript
yarn add ioredis
```
```typescript
pnpm add ioredis
```
```typescript
import { Redis } from "ioredis";
import { BufferMemory } from "langchain/memory";
import { RedisChatMessageHistory } from "langchain/stores/message/ioredis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

// Uses ioredis to facilitate Sentinel Connections see their docs for details on setting up more complex Sentinels: https://github.com/redis/ioredis#sentinel
const client = new Redis({
  sentinels: [
    { host: "localhost", port: 26379 },
    { host: "localhost", port: 26380 },
  ],
  name: "mymaster",
});

const memory = new BufferMemory({
  chatHistory: new RedisChatMessageHistory({
    sessionId: new Date().toISOString(),
    sessionTTL: 300,
    client,
  }),
});

const model = new ChatOpenAI({ temperature: 0.5 });

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/upstash_redis

# Upstash Redis-Backed Chat Memory
Because Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.
Based on Redis-Backed Chat Memory.
For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for an Upstash Redis instance.
## Setup​
You will need to install @upstash/redis in your project:
```typescript
npm install @upstash/redis
```
```typescript
yarn add @upstash/redis
```
```typescript
pnpm add @upstash/redis
```
You will also need an Upstash Account and a Redis database to connect to. See instructions on Upstash Docs on how to create a HTTP client.
## Usage​
Each chat history session stored in Redis must have a unique id. You can provide an optional sessionTTL to make sessions expire after a give number of seconds.
The config parameter is passed directly into the new Redis() constructor of @upstash/redis, and takes all the same arguments.
```typescript
import { BufferMemory } from "langchain/memory";
import { UpstashRedisChatMessageHistory } from "langchain/stores/message/upstash_redis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

const memory = new BufferMemory({
  chatHistory: new UpstashRedisChatMessageHistory({
    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation
    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire
    config: {
      url: "https://ADD_YOURS_HERE.upstash.io", // Override with your own instance's URL
      token: "********", // Override with your own instance's token
    },
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:
## Advanced Usage​
You can also directly pass in a previously created @upstash/redis client instance:
```typescript
import { Redis } from "@upstash/redis";
import { BufferMemory } from "langchain/memory";
import { UpstashRedisChatMessageHistory } from "langchain/stores/message/upstash_redis";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";

// Create your own Redis client
const client = new Redis({
  url: "https://ADD_YOURS_HERE.upstash.io",
  token: "********",
});

const memory = new BufferMemory({
  chatHistory: new UpstashRedisChatMessageHistory({
    sessionId: new Date().toISOString(),
    sessionTTL: 300,
    client, // You can reuse your existing Redis client
  }),
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/memory/integrations/zep_memory

# Zep Memory
Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.
Key Features:
## Setup​
See the instructions from Zep for running the server locally or through an automated hosting provider.
## Usage​
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { ZepMemory } from "langchain/memory/zep";

const sessionId = "TestSession";
const zepURL = "http://localhost:8000";

const memory = new ZepMemory({
  sessionId,
  baseURL: zepURL,
  // This is optional. If you've enabled JWT authentication on your Zep server, you can
  // pass it in here. See https://docs.getzep.com/deployment/auth
  apiKey: "change_this_key",
});

const model = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  temperature: 0,
});

const chain = new ConversationChain({ llm: model, memory });
console.log("Memory Keys:", memory.memoryKeys);

const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 });
/*
{
  res1: {
    text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
  }
}
*/

const res2 = await chain.call({ input: "What did I just say my name was?" });
console.log({ res2 });

/*
{
  res1: {
    text: "You said your name was Jim."
  }
}
*/
console.log("Session ID: ", sessionId);
console.log("Memory: ", await memory.loadMemoryVariables({}));
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/

# Agents
Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.
There are two main types of agents:
Action agents are suitable for small tasks, while plan-and-execute agents are better for complex or long-running tasks that require maintaining long-term objectives and focus. Often the best approach is to combine the dynamism of an action agent with the planning abilities of a plan-and-execute agent by letting the plan-and-execute agent use action agents to execute plans.
For a full list of agent types see agent types. Additional abstractions involved in agents are:
## Action agents​
At a high-level an action agent:
Action agents are wrapped in agent executors, chains which are responsible for calling the agent, getting back an action and action input, calling the tool that the action references with the generated input, getting the output of the tool, and then passing all that information back into the agent to get the next action it should take.
Although an agent can be constructed in many ways, it typically involves these components:
## Plan-and-execute agents​
At a high-level a plan-and-execute agent:
The most typical implementation is to have the planner be a language model, and the executor be an action agent. Read more here.
## Get started​
LangChain offers several types of agents. Here's an example using one powered by OpenAI functions:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const tools = [new Calculator(), new SerpAPI()];
const chat = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });

const executor = await initializeAgentExecutorWithOptions(tools, chat, {
  agentType: "openai-functions",
  verbose: true,
});

const result = await executor.run("What is the weather in New York?");
console.log(result);

/*
  The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.
*/
```
#### API Reference:
And here is the logged verbose output:
```typescript
[chain/start] [1:chain:AgentExecutor] Entering Chain run with input: {
  "input": "What is the weather in New York?",
  "chat_history": []
}
[llm/start] [1:chain:AgentExecutor > 2:llm:ChatOpenAI] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful AI assistant.",
          "additional_kwargs": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "What is the weather in New York?",
          "additional_kwargs": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 2:llm:ChatOpenAI] [1.97s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "AIMessage"
          ],
          "kwargs": {
            "content": "",
            "additional_kwargs": {
              "function_call": {
                "name": "search",
                "arguments": "{\n  \"input\": \"current weather in New York\"\n}"
              }
            }
          }
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 18,
      "promptTokens": 121,
      "totalTokens": 139
    }
  }
}
[agent/action] [1:chain:AgentExecutor] Agent selected action: {
  "tool": "search",
  "toolInput": {
    "input": "current weather in New York"
  },
  "log": ""
}
[tool/start] [1:chain:AgentExecutor > 3:tool:SerpAPI] Entering Tool run with input: "current weather in New York"
[tool/end] [1:chain:AgentExecutor > 3:tool:SerpAPI] [1.90s] Exiting Tool run with output: "1 am · Feels Like72° · WindSSW 1 mph · Humidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in ..."
[llm/start] [1:chain:AgentExecutor > 4:llm:ChatOpenAI] Entering LLM run with input: {
  "messages": [
    [
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "SystemMessage"
        ],
        "kwargs": {
          "content": "You are a helpful AI assistant.",
          "additional_kwargs": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "HumanMessage"
        ],
        "kwargs": {
          "content": "What is the weather in New York?",
          "additional_kwargs": {}
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "AIMessage"
        ],
        "kwargs": {
          "content": "",
          "additional_kwargs": {
            "function_call": {
              "name": "search",
              "arguments": "{\"input\":\"current weather in New York\"}"
            }
          }
        }
      },
      {
        "lc": 1,
        "type": "constructor",
        "id": [
          "langchain",
          "schema",
          "FunctionMessage"
        ],
        "kwargs": {
          "content": "1 am · Feels Like72° · WindSSW 1 mph · Humidity89% · UV Index0 of 11 · Cloud Cover79% · Rain Amount0 in ...",
          "name": "search",
          "additional_kwargs": {}
        }
      }
    ]
  ]
}
[llm/end] [1:chain:AgentExecutor > 4:llm:ChatOpenAI] [3.33s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": "The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "AIMessage"
          ],
          "kwargs": {
            "content": "The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.",
            "additional_kwargs": {}
          }
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 58,
      "promptTokens": 180,
      "totalTokens": 238
    }
  }
}
[chain/end] [1:chain:AgentExecutor] [7.73s] Exiting Chain run with output: {
  "output": "The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain."
}
```



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/

# Agent types
## Action agents​
Agents use an LLM to determine which actions to take and in what order.
An action can either be using a tool and observing its output, or returning a response to the user.
Here are the agents available in LangChain.
### Zero-shot ReAct​
This agent uses the ReAct framework to determine which tool to use
based solely on the tool's description. Any number of tools can be provided.
This agent requires that a description is provided for each tool.
Note: This is the most general purpose action agent.
### OpenAI Functions​
Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to detect when a
function should to be called and respond with the inputs that should be passed to the function.
The OpenAI Functions Agent is designed to work with these models.
### Conversational​
This agent is designed to be used in conversational settings.
The prompt is designed to make the agent helpful and conversational.
It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions.
## Plan-and-execute agents​
Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the "Plan-and-Solve" paper.



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/chat_conversation_agent

# Conversational
This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.
This example covers how to create a conversational agent for a chat model. It will utilize chat specific prompts.
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

export const run = async () => {
  process.env.LANGCHAIN_HANDLER = "langchain";
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  // Passing "chat-conversational-react-description" as the agent type
  // automatically creates and uses BufferMemory with the executor.
  // If you would like to override this, you can pass in a custom
  // memory option, but the memoryKey set on it must be "chat_history".
  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "chat-conversational-react-description",
    verbose: true,
  });
  console.log("Loaded agent.");

  const input0 = "hi, i am bob";

  const result0 = await executor.call({ input: input0 });

  console.log(`Got output ${result0.output}`);

  const input1 = "whats my name?";

  const result1 = await executor.call({ input: input1 });

  console.log(`Got output ${result1.output}`);

  const input2 = "whats the weather in pomfret?";

  const result2 = await executor.call({ input: input2 });

  console.log(`Got output ${result2.output}`);
};
```
#### API Reference:
```typescript
Loaded agent.
Entering new agent_executor chain...
{
    "action": "Final Answer",
    "action_input": "Hello Bob! How can I assist you today?"
}
Finished chain.
Got output Hello Bob! How can I assist you today?
Entering new agent_executor chain...
{
    "action": "Final Answer",
    "action_input": "Your name is Bob."
}
Finished chain.
Got output Your name is Bob.
Entering new agent_executor chain...
```json
{
    "action": "search",
    "action_input": "weather in pomfret"
}
```
A steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.
```json
{
    "action": "Final Answer",
    "action_input": "The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%."
}
```
Finished chain.
Got output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.
```



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/openai_functions_agent

# OpenAI functions
Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to detect when a function should to be called and respond with the inputs that should be passed to the function.
In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call those functions.
The goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.
The OpenAI Functions Agent is designed to work with these models.
Must be used with an OpenAI Functions model.
This agent also supports StructuredTools with more complex input schemas.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const tools = [new Calculator(), new SerpAPI()];
const chat = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });

const executor = await initializeAgentExecutorWithOptions(tools, chat, {
  agentType: "openai-functions",
  verbose: true,
});

const result = await executor.run("What is the weather in New York?");
console.log(result);

/*
  The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.
*/
```
#### API Reference:
## Prompt customization​
You can pass in a custom string to be used as the system message of the prompt as follows:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const tools = [new Calculator(), new SerpAPI()];
const chat = new ChatOpenAI({ modelName: "gpt-4", temperature: 0 });
const prefix =
  "You are a helpful AI assistant. However, all final response to the user must be in pirate dialect.";

const executor = await initializeAgentExecutorWithOptions(tools, chat, {
  agentType: "openai-functions",
  verbose: true,
  agentArgs: {
    prefix,
  },
});

const result = await executor.run("What is the weather in New York?");
console.log(result);

// Arr matey, in New York, it be feelin' like 75 degrees, with a gentle breeze blowin' from the northwest at 3 knots. The air be 77% full o' water, and the clouds be coverin' 35% of the sky. There be no rain in sight, yarr!
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/plan_and_execute

# Plan and execute
Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by BabyAGI and then the "Plan-and-Solve" paper.
The planning is almost always done by an LLM.
The execution is usually done by a separate agent (equipped with tools).
This agent uses a two step process:
The idea is that the planning step keeps the LLM more "on track" by breaking up a larger task into simpler subtasks.
However, this method requires more individual LLM queries and has higher latency compared to Action Agents.
Note: This agent currently only supports Chat Models.
```typescript
import { Calculator } from "langchain/tools/calculator";
import { SerpAPI } from "langchain/tools";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PlanAndExecuteAgentExecutor } from "langchain/experimental/plan_and_execute";

const tools = [new Calculator(), new SerpAPI()];
const model = new ChatOpenAI({
  temperature: 0,
  modelName: "gpt-3.5-turbo",
  verbose: true,
});
const executor = PlanAndExecuteAgentExecutor.fromLLMAndTools({
  llm: model,
  tools,
});

const result = await executor.call({
  input: `Who is the current president of the United States? What is their current age raised to the second power?`,
});

console.log({ result });
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/react

# ReAct
This walkthrough showcases using an agent to implement the ReAct logic.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

const result = await executor.call({ input });
```
#### API Reference:
## Using chat models​
You can also create ReAct agents that use chat models instead of LLMs as the agent driver.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "chat-zero-shot-react-description",
    returnIntermediateSteps: true,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);

  console.log(
    `Got intermediate steps ${JSON.stringify(
      result.intermediateSteps,
      null,
      2
    )}`
  );
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/agent_types/structured_chat

# Structured tool chat
The structured tool chat agent is capable of using multi-input tools.
Older agents are configured to specify an action input as a single string, but this agent can use the provided tools' args_schema to populate the action input.
This makes it easier to create and use tools that require multiple input values - rather than prompting for a stringified object or comma separated list, you can specify an object with multiple keys.
Here's an example with a DynamicStructuredTool:
```typescript
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { Calculator } from "langchain/tools/calculator";
import { DynamicStructuredTool } from "langchain/tools";

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new Calculator(), // Older existing single input tools will still work
    new DynamicStructuredTool({
      name: "random-number-generator",
      description: "generates a random number between two input numbers",
      schema: z.object({
        low: z.number().describe("The lower bound of the generated number"),
        high: z.number().describe("The upper bound of the generated number"),
      }),
      func: async ({ low, high }) =>
        (Math.random() * (high - low) + low).toString(), // Outputs still must be strings
      returnDirect: false, // This is an option that allows the tool to return the output directly
    }),
  ];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "structured-chat-zero-shot-react-description",
    verbose: true,
  });
  console.log("Loaded agent.");

  const input = `What is a random number between 5 and 10 raised to the second power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log({ result });

  /*
    {
      "output": "67.95299776074"
    }
  */
};
```
#### API Reference:
## Adding Memory​
You can add memory to this agent like this:
```typescript
import { ChatOpenAI } from "langchain/chat_models/openai";
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { Calculator } from "langchain/tools/calculator";
import { MessagesPlaceholder } from "langchain/prompts";
import { BufferMemory } from "langchain/memory";

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [new Calculator()];

  const executor = await initializeAgentExecutorWithOptions(tools, model, {
    agentType: "structured-chat-zero-shot-react-description",
    verbose: true,
    memory: new BufferMemory({
      memoryKey: "chat_history",
      returnMessages: true,
    }),
    agentArgs: {
      inputVariables: ["input", "agent_scratchpad", "chat_history"],
      memoryPrompts: [new MessagesPlaceholder("chat_history")],
    },
  });

  const result = await executor.call({ input: `what is 9 to the 2nd power?` });

  console.log(result);

  /*
    {
      "output": "81"
    }
  */

  const result2 = await executor.call({
    input: `what is that number squared?`,
  });

  console.log(result2);

  /*
    {
      "output": "6561"
    }
  */
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/callbacks

# Subscribing to events
You can subscribe to a number of events that are emitted by the Agent and the underlying tools, chains and models via callbacks.
For more info on the events available see the Callbacks section of the docs.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});

const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;
const result = await executor.run(input, [
  {
    handleAgentAction(action, runId) {
      console.log("\nhandleAgentAction", action, runId);
    },
    handleAgentEnd(action, runId) {
      console.log("\nhandleAgentEnd", action, runId);
    },
    handleToolEnd(output, runId) {
      console.log("\nhandleToolEnd", output, runId);
    },
  },
]);
/*
handleAgentAction {
  tool: 'search',
  toolInput: 'Olivia Wilde boyfriend',
  log: " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n" +
    'Action: search\n' +
    'Action Input: "Olivia Wilde boyfriend"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6

handleToolEnd In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022. 062fef47-8ad1-4729-9949-a57be252e002

handleAgentAction {
  tool: 'search',
  toolInput: 'Harry Styles age',
  log: " I need to find out Harry Styles' age.\n" +
    'Action: search\n' +
    'Action Input: "Harry Styles age"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6

handleToolEnd 29 years 9ec91e41-2fbf-4de0-85b6-12b3e6b3784e 61d77e10-c119-435d-a985-1f9d45f0ef08

handleAgentAction {
  tool: 'calculator',
  toolInput: '29^0.23',
  log: ' I need to calculate 29 raised to the 0.23 power.\n' +
    'Action: calculator\n' +
    'Action Input: 29^0.23'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6

handleToolEnd 2.169459462491557 07aec96a-ce19-4425-b863-2eae39db8199

handleAgentEnd {
  returnValues: {
    output: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
  },
  log: ' I now know the final answer.\n' +
    "Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
*/

console.log({ result });
// { result: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557." }
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/cancelling_requests

# Cancelling requests
You can cancel a request by passing a signal option when you run the agent. For example:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});
const controller = new AbortController();

// Call `controller.abort()` somewhere to cancel the request.
setTimeout(() => {
  controller.abort();
}, 2000);

try {
  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;
  const result = await executor.call({ input, signal: controller.signal });
} catch (e) {
  console.log(e);
  /*
  Error: Cancel: canceled
      at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
      at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
    attemptNumber: 1,
    retriesLeft: 6
  }
  */
}
```
#### API Reference:
Note, this will only cancel the outgoing request if the underlying provider exposes that option. LangChain will cancel the underlying request if possible, otherwise it will cancel the processing of the response.



Page URL: https://js.langchain.com/docs/modules/agents/how_to/custom_llm_agent

# Custom LLM Agent
This notebook goes through how to create your own custom LLM agent.
An LLM agent consists of three parts:
The LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:
AgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).
AgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.
```typescript
import {
  LLMSingleActionAgent,
  AgentActionOutputParser,
  AgentExecutor,
} from "langchain/agents";
import { LLMChain } from "langchain/chains";
import { OpenAI } from "langchain/llms/openai";
import {
  BasePromptTemplate,
  BaseStringPromptTemplate,
  SerializedBasePromptTemplate,
  renderTemplate,
} from "langchain/prompts";
import {
  InputValues,
  PartialValues,
  AgentStep,
  AgentAction,
  AgentFinish,
} from "langchain/schema";
import { SerpAPI, Tool } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;
const formatInstructions = (
  toolNames: string
) => `Use the following format in your response:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [${toolNames}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question`;
const SUFFIX = `Begin!

Question: {input}
Thought:{agent_scratchpad}`;

class CustomPromptTemplate extends BaseStringPromptTemplate {
  tools: Tool[];

  constructor(args: { tools: Tool[]; inputVariables: string[] }) {
    super({ inputVariables: args.inputVariables });
    this.tools = args.tools;
  }

  _getPromptType(): string {
    throw new Error("Not implemented");
  }

  format(input: InputValues): Promise<string> {
    /** Construct the final template */
    const toolStrings = this.tools
      .map((tool) => `${tool.name}: ${tool.description}`)
      .join("\n");
    const toolNames = this.tools.map((tool) => tool.name).join("\n");
    const instructions = formatInstructions(toolNames);
    const template = [PREFIX, toolStrings, instructions, SUFFIX].join("\n\n");
    /** Construct the agent_scratchpad */
    const intermediateSteps = input.intermediate_steps as AgentStep[];
    const agentScratchpad = intermediateSteps.reduce(
      (thoughts, { action, observation }) =>
        thoughts +
        [action.log, `\nObservation: ${observation}`, "Thought:"].join("\n"),
      ""
    );
    const newInput = { agent_scratchpad: agentScratchpad, ...input };
    /** Format the template. */
    return Promise.resolve(renderTemplate(template, "f-string", newInput));
  }

  partial(_values: PartialValues): Promise<BasePromptTemplate> {
    throw new Error("Not implemented");
  }

  serialize(): SerializedBasePromptTemplate {
    throw new Error("Not implemented");
  }
}

class CustomOutputParser extends AgentActionOutputParser {
  lc_namespace = ["langchain", "agents", "custom_llm_agent"];

  async parse(text: string): Promise<AgentAction | AgentFinish> {
    if (text.includes("Final Answer:")) {
      const parts = text.split("Final Answer:");
      const input = parts[parts.length - 1].trim();
      const finalAnswers = { output: input };
      return { log: text, returnValues: finalAnswers };
    }

    const match = /Action: (.*)\nAction Input: (.*)/s.exec(text);
    if (!match) {
      throw new Error(`Could not parse LLM output: ${text}`);
    }

    return {
      tool: match[1].trim(),
      toolInput: match[2].trim().replace(/^"+|"+$/g, ""),
      log: text,
    };
  }

  getFormatInstructions(): string {
    throw new Error("Not implemented");
  }
}

export const run = async () => {
  const model = new OpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const llmChain = new LLMChain({
    prompt: new CustomPromptTemplate({
      tools,
      inputVariables: ["input", "agent_scratchpad"],
    }),
    llm: model,
  });

  const agent = new LLMSingleActionAgent({
    llmChain,
    outputParser: new CustomOutputParser(),
    stop: ["\nObservation"],
  });
  const executor = new AgentExecutor({
    agent,
    tools,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);
};
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/custom_llm_chat_agent

# Custom LLM Agent (with a ChatModel)
This notebook goes through how to create your own custom agent based on a chat model.
An LLM chat agent consists of three parts:
The LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:
AgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).
AgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.
```typescript
import {
  AgentActionOutputParser,
  AgentExecutor,
  LLMSingleActionAgent,
} from "langchain/agents";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  BaseChatPromptTemplate,
  BasePromptTemplate,
  SerializedBasePromptTemplate,
  renderTemplate,
} from "langchain/prompts";
import {
  AgentAction,
  AgentFinish,
  AgentStep,
  BaseMessage,
  HumanMessage,
  InputValues,
  PartialValues,
} from "langchain/schema";
import { SerpAPI, Tool } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;
const formatInstructions = (
  toolNames: string
) => `Use the following format in your response:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [${toolNames}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question`;
const SUFFIX = `Begin!

Question: {input}
Thought:{agent_scratchpad}`;

class CustomPromptTemplate extends BaseChatPromptTemplate {
  tools: Tool[];

  constructor(args: { tools: Tool[]; inputVariables: string[] }) {
    super({ inputVariables: args.inputVariables });
    this.tools = args.tools;
  }

  _getPromptType(): string {
    throw new Error("Not implemented");
  }

  async formatMessages(values: InputValues): Promise<BaseMessage[]> {
    /** Construct the final template */
    const toolStrings = this.tools
      .map((tool) => `${tool.name}: ${tool.description}`)
      .join("\n");
    const toolNames = this.tools.map((tool) => tool.name).join("\n");
    const instructions = formatInstructions(toolNames);
    const template = [PREFIX, toolStrings, instructions, SUFFIX].join("\n\n");
    /** Construct the agent_scratchpad */
    const intermediateSteps = values.intermediate_steps as AgentStep[];
    const agentScratchpad = intermediateSteps.reduce(
      (thoughts, { action, observation }) =>
        thoughts +
        [action.log, `\nObservation: ${observation}`, "Thought:"].join("\n"),
      ""
    );
    const newInput = { agent_scratchpad: agentScratchpad, ...values };
    /** Format the template. */
    const formatted = renderTemplate(template, "f-string", newInput);
    return [new HumanMessage(formatted)];
  }

  partial(_values: PartialValues): Promise<BasePromptTemplate> {
    throw new Error("Not implemented");
  }

  serialize(): SerializedBasePromptTemplate {
    throw new Error("Not implemented");
  }
}

class CustomOutputParser extends AgentActionOutputParser {
  lc_namespace = ["langchain", "agents", "custom_llm_agent_chat"];

  async parse(text: string): Promise<AgentAction | AgentFinish> {
    if (text.includes("Final Answer:")) {
      const parts = text.split("Final Answer:");
      const input = parts[parts.length - 1].trim();
      const finalAnswers = { output: input };
      return { log: text, returnValues: finalAnswers };
    }

    const match = /Action: (.*)\nAction Input: (.*)/s.exec(text);
    if (!match) {
      throw new Error(`Could not parse LLM output: ${text}`);
    }

    return {
      tool: match[1].trim(),
      toolInput: match[2].trim().replace(/^"+|"+$/g, ""),
      log: text,
    };
  }

  getFormatInstructions(): string {
    throw new Error("Not implemented");
  }
}

export const run = async () => {
  const model = new ChatOpenAI({ temperature: 0 });
  const tools = [
    new SerpAPI(process.env.SERPAPI_API_KEY, {
      location: "Austin,Texas,United States",
      hl: "en",
      gl: "us",
    }),
    new Calculator(),
  ];

  const llmChain = new LLMChain({
    prompt: new CustomPromptTemplate({
      tools,
      inputVariables: ["input", "agent_scratchpad"],
    }),
    llm: model,
  });

  const agent = new LLMSingleActionAgent({
    llmChain,
    outputParser: new CustomOutputParser(),
    stop: ["\nObservation"],
  });
  const executor = new AgentExecutor({
    agent,
    tools,
  });
  console.log("Loaded agent.");

  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

  console.log(`Executing with input "${input}"...`);

  const result = await executor.call({ input });

  console.log(`Got output ${result.output}`);
};
run();
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/how_to/logging_and_tracing

# Logging and tracing
You can pass the verbose flag when creating an agent to enable logging of all events to the console. For example:
You can also enable tracing by setting the LANGCHAIN_TRACING environment variable to true.
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;

const result = await executor.call({ input });
```
#### API Reference:
```typescript
[chain/start] [1:chain:agent_executor] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
}
[chain/start] [1:chain:agent_executor > 2:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": "",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] [3.52s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 39,
      "promptTokens": 220,
      "totalTokens": 259
    }
  }
}
[chain/end] [1:chain:agent_executor > 2:chain:llm_chain] [3.53s] Exiting Chain run with output: {
  "text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
  "tool": "search",
  "toolInput": "Olivia Wilde boyfriend",
  "log": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\""
}
[tool/start] [1:chain:agent_executor > 4:tool:search] Entering Tool run with input: "Olivia Wilde boyfriend"
[tool/end] [1:chain:agent_executor > 4:tool:search] [845ms] Exiting Tool run with output: "In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022."
[chain/start] [1:chain:agent_executor > 5:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought:",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] [3.65s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 23,
      "promptTokens": 296,
      "totalTokens": 319
    }
  }
}
[chain/end] [1:chain:agent_executor > 5:chain:llm_chain] [3.65s] Exiting Chain run with output: {
  "text": " I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
  "tool": "search",
  "toolInput": "Harry Styles age",
  "log": " I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\""
}
[tool/start] [1:chain:agent_executor > 7:tool:search] Entering Tool run with input: "Harry Styles age"
[tool/end] [1:chain:agent_executor > 7:tool:search] [632ms] Exiting Tool run with output: "29 years"
[chain/start] [1:chain:agent_executor > 8:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought:",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] [2.72s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 26,
      "promptTokens": 329,
      "totalTokens": 355
    }
  }
}
[chain/end] [1:chain:agent_executor > 8:chain:llm_chain] [2.72s] Exiting Chain run with output: {
  "text": " I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23"
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
  "tool": "calculator",
  "toolInput": "29^0.23",
  "log": " I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23"
}
[tool/start] [1:chain:agent_executor > 10:tool:calculator] Entering Tool run with input: "29^0.23"
[tool/end] [1:chain:agent_executor > 10:tool:calculator] [3ms] Exiting Tool run with output: "2.169459462491557"
[chain/start] [1:chain:agent_executor > 11:chain:llm_chain] Entering Chain run with input: {
  "input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
  "agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought: I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23\nObservation: 2.169459462491557\nThought:",
  "stop": [
    "\nObservation: "
  ]
}
[llm/start] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] Entering LLM run with input: {
  "prompts": [
    "Answer the following questions as best you can. You have access to the following tools:\n\nsearch: a search engine. useful for when you need to answer questions about current events. input should be a search query.\ncalculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n\nUse the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [search,calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\nAction: search\nAction Input: \"Olivia Wilde boyfriend\"\nObservation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\nThought: I need to find out Harry Styles' age.\nAction: search\nAction Input: \"Harry Styles age\"\nObservation: 29 years\nThought: I need to calculate 29 raised to the 0.23 power.\nAction: calculator\nAction Input: 29^0.23\nObservation: 2.169459462491557\nThought:"
  ]
}
[llm/end] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] [3.51s] Exiting LLM run with output: {
  "generations": [
    [
      {
        "text": " I now know the final answer.\nFinal Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.",
        "generationInfo": {
          "finishReason": "stop",
          "logprobs": null
        }
      }
    ]
  ],
  "llmOutput": {
    "tokenUsage": {
      "completionTokens": 39,
      "promptTokens": 371,
      "totalTokens": 410
    }
  }
}
[chain/end] [1:chain:agent_executor > 11:chain:llm_chain] [3.51s] Exiting Chain run with output: {
  "text": " I now know the final answer.\nFinal Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}
[chain/end] [1:chain:agent_executor] [14.90s] Exiting Chain run with output: {
  "output": "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}
```



Page URL: https://js.langchain.com/docs/modules/agents/how_to/timeouts

# Adding a timeout
By default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout to an agent, you can pass a timeout option, when you run the agent. For example:
```typescript
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";

const model = new OpenAI({ temperature: 0 });
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
});

try {
  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;
  const result = await executor.call({ input, timeout: 2000 }); // 2 seconds
} catch (e) {
  console.log(e);
  /*
  Error: Cancel: canceled
      at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
      at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
    attemptNumber: 1,
    retriesLeft: 6
  }
  */
}
```
#### API Reference:



Page URL: https://js.langchain.com/docs/modules/agents/tools/

# Tools
Tools are interfaces that an agent can use to interact with the world.
## Get started​
Tools are functions that agents can use to interact with the world.
These tools can be generic utilities (e.g. search), other chains, or even other agents.
Specifically, the interface of a tool has a single text input and a single text output. It includes a name and description that communicate to the model what the tool does and when to use it.
```typescript
interface Tool {
  call(arg: string): Promise<string>;

  name: string;

  description: string;
}
```
## Advanced​
To implement your own tool you can subclass the Tool class and implement the _call method. The _call method is called with the input text and should return the output text. The Tool superclass implements the call method, which takes care of calling the right CallbackManager methods before and after calling your _call method. When an error occurs, the _call method should when possible return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.
```typescript
abstract class Tool {
  abstract _call(arg: string): Promise<string>;

  abstract name: string;

  abstract description: string;
}
```

